{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7847261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmpc95ievpx_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated | Statements of Comprehensive | Income (Loss) | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 20 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 4. | Controls and Procedures | 32 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n",
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 99 chars\n",
      "\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "\n",
      "     Type: part, Length: 4,191 chars\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 52 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Business...\n",
      "INFO:__main__:  ‚Ä¢ 10: ...\n",
      "INFO:__main__:  ‚Ä¢ 10: Directors, Executive Officers, and Corporate Gover...\n",
      "INFO:__main__:  ‚Ä¢ 11: ...\n",
      "INFO:__main__:  ‚Ä¢ 11: Executive Compensation...\n",
      "INFO:__main__:  ‚Ä¢ 12: ...\n",
      "INFO:__main__:  ‚Ä¢ 12: Security Ownership of Certain Beneficial Owners an...\n",
      "INFO:__main__:  ‚Ä¢ 13: ...\n",
      "INFO:__main__:  ‚Ä¢ 13: Certain Relationships and Related Transactions, an...\n",
      "INFO:__main__:TOC analysis found 52 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Business'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Directors, Executive Officers, and Corporate Governance'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Executive Compensation'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Certain Relationships and Related Transactions, and Director Independence'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Principal Accountant Fees and Services'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits, Financial Statement Schedules'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form 10-K Summary'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unresolved Staff Comments'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Properties'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Reserved'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements and Supplementary Data'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Changes in and Disagreements with Accountants on Accounting and Financial Disclosure'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Disclosure Regarding Foreign Jurisdictions that Prevent Inspections'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 1. | Business | 3 | Item 1A. | Risk Factors | 6 | Item 1B. | Unresolved Staff Comments | 16 | Item 2. | Properties | 17 | Item 3. | Legal Proceedings | 17 | Item 4. | Mine Safety Disclosures | 17 | PART II | Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 10.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 15.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part II - . OTHER INFORMATION...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 13,293 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 55,960 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 106 chars\n",
      "\n",
      "  4. Properties\n",
      "\n",
      "     Type: item, Length: 1,437 chars\n",
      "\n",
      "  5. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 185 chars\n",
      "\n",
      "  6. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 113 chars\n",
      "\n",
      "  7. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 516 chars\n",
      "\n",
      "  8. Reserved\n",
      "\n",
      "     Type: item, Length: 50,497 chars\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,524 chars\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 86,346 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. . FINANCIAL INFORMATION\n",
      "\n",
      "     Type: part, Length: 34,985 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,101 chars\n",
      "\n",
      "  3. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 45,106 chars\n",
      "\n",
      "  4. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 4,404 chars\n",
      "\n",
      "  5. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 2,075 chars\n",
      "\n",
      "  6. . OTHER INFORMATION\n",
      "\n",
      "     Type: part, Length: 189 chars\n",
      "\n",
      "  7. Risk Factors\n",
      "\n",
      "     Type: item, Length: 59,432 chars\n",
      "\n",
      "  8. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 102 chars\n",
      "\n",
      "  9. Defaults Upon Senior Securities\n",
      "\n",
      "     Type: item, Length: 152 chars\n",
      "\n",
      "  10. Other Information\n",
      "\n",
      "     Type: item, Length: 3,030 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive | Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 22 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 33 | Item 4. | Controls and Procedures | 34 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . Financial Information...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part II - . Other Information...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 21 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: Certificate of Incorporation of the Company, inclu...\n",
      "INFO:__main__:  ‚Ä¢ 1: Intentionally omitted....\n",
      "INFO:__main__:  ‚Ä¢ 10: Form of Note for 3.200% Notes due 2023 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 11: Form of Note for 1.875% Notes due 2026 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 12: Form of Note for 1.125% Notes due 2022 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 13: Form of Note for 0.75% Notes due 2023 ‚Äî incorporat...\n",
      "INFO:__main__:  ‚Ä¢ 14: Form of Note for 1.125% Notes due 2027 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 15: Form of Note for 1.625% Notes due 2035 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 16: Form of Note for 1.875% Notes due 2020 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 17: Form of Note for 2.875% Notes due 2025 ‚Äî incorpora...\n",
      "INFO:__main__:TOC analysis found 21 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Certificate of Incorporation of the Company, including Amendment of Certificate of Incorporation, dated July 27, 2012 ‚Äî incorporated herein by reference to Exhibit 3.1 to the Company's Quarterly Report on Form 10-Q for the quarter ended September 28, 2012.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Intentionally omitted.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.200% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.8 to the Company's Current Report on Form 8-K filed on November 1, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.875% Notes due 2026 ‚Äî incorporated herein by reference to Exhibit 4.4 to the Company's Registration Statement on Form 8-A filed on September 19, 2014.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.125% Notes due 2022 ‚Äî incorporated herein by reference to Exhibit 4.5 to the Company's Registration Statement on Form 8-A filed on September 19, 2014.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 0.75% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.125% Notes due 2027 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.625% Notes due 2035 ‚Äî incorporated herein by reference to Exhibit 4.8 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.875% Notes due 2020 ‚Äî incorporated herein by reference to Exhibit 4.5 to the Company's Current Report on Form 8-K filed on October 27, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.875% Notes due 2025 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on October 27, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.55% Notes due 2026 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on May 31, 2016.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.550% Notes due 2021 ‚Äî incorporated herein by reference to Exhibit 4.4 to the Company's Current Report on Form 8-K filed on September 1, 2016.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'As permitted by the rules of the SEC, the Company has not filed certain instruments defining the rights of holders of long-term debt of the Company or consolidated subsidiaries under which the total amount of securities authorized does not exceed 10 percent of the total assets of the Company and its consolidated subsidiaries. The Company agrees to furnish to the SEC, upon request, a copy of any omitted instrument.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'By-Laws of the Company, as amended and restated through April 22, 2020 ‚Äî incorporated herein by reference to Exhibit 3.2 to the Company's Quarterly Report on Form 10-Q for the quarter ended March 27, 2020.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Amended and Restated Indenture, dated as of April 26, 1988, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.1 to the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'First Supplemental Indenture, dated as of February 24, 1992, to Amended and Restated Indenture, dated as of April 26, 1988, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.2 to the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Second Supplemental Indenture, dated as of November 1, 2007, to Amended and Restated Indenture, dated as of April 26, 1988, as amended, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.3 of the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.150% Notes due November 15, 2020 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Current Report on Form 8-K filed on November 18, 2010.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.30% Notes due September 1, 2021 ‚Äî incorporated herein by reference to Exhibit 4.14 to the Company's Quarterly Report on Form 10-Q for the quarter ended September 30, 2011.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.500% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on March 5, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.450% Notes due 2020 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Current Report on Form 8-K filed on November 1, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. . Financial Information\n",
      "\n",
      "     Type: part, Length: 115,924 chars\n",
      "\n",
      "  2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 1,004 chars\n",
      "\n",
      "  5. . Other Information\n",
      "\n",
      "     Type: part, Length: 248 chars\n",
      "\n",
      "  6. Risk Factors\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Exhibits\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +0 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "  11. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  12. Controls and Procedures\n",
      "\n",
      "  13. Other Information\n",
      "\n",
      "  14. Executive Compensation\n",
      "\n",
      "  15. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  16. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  17. Principal Accountant Fees and Services\n",
      "\n",
      "  18. Item 15.    Exhibit and Financial Statement Schedules\n",
      "\n",
      "  19. Form 10-K Summary\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n",
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmpzovylgxp_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n",
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated | Statements of Comprehensive | Income (Loss) | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 20 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 4. | Controls and Procedures | 32 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 99 chars\n",
      "\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "\n",
      "     Type: part, Length: 4,191 chars\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 52 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Business...\n",
      "INFO:__main__:  ‚Ä¢ 10: ...\n",
      "INFO:__main__:  ‚Ä¢ 10: Directors, Executive Officers, and Corporate Gover...\n",
      "INFO:__main__:  ‚Ä¢ 11: ...\n",
      "INFO:__main__:  ‚Ä¢ 11: Executive Compensation...\n",
      "INFO:__main__:  ‚Ä¢ 12: ...\n",
      "INFO:__main__:  ‚Ä¢ 12: Security Ownership of Certain Beneficial Owners an...\n",
      "INFO:__main__:  ‚Ä¢ 13: ...\n",
      "INFO:__main__:  ‚Ä¢ 13: Certain Relationships and Related Transactions, an...\n",
      "INFO:__main__:TOC analysis found 52 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Business'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Directors, Executive Officers, and Corporate Governance'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Executive Compensation'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Certain Relationships and Related Transactions, and Director Independence'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Principal Accountant Fees and Services'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits, Financial Statement Schedules'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form 10-K Summary'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unresolved Staff Comments'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Properties'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Reserved'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements and Supplementary Data'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Changes in and Disagreements with Accountants on Accounting and Financial Disclosure'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Disclosure Regarding Foreign Jurisdictions that Prevent Inspections'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 1. | Business | 3 | Item 1A. | Risk Factors | 6 | Item 1B. | Unresolved Staff Comments | 16 | Item 2. | Properties | 17 | Item 3. | Legal Proceedings | 17 | Item 4. | Mine Safety Disclosures | 17 | PART II | Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 10.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 15.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part II - . OTHER INFORMATION...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 13,293 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 55,960 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 106 chars\n",
      "\n",
      "  4. Properties\n",
      "\n",
      "     Type: item, Length: 1,437 chars\n",
      "\n",
      "  5. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 185 chars\n",
      "\n",
      "  6. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 113 chars\n",
      "\n",
      "  7. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 516 chars\n",
      "\n",
      "  8. Reserved\n",
      "\n",
      "     Type: item, Length: 50,497 chars\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,524 chars\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 86,346 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. . FINANCIAL INFORMATION\n",
      "\n",
      "     Type: part, Length: 34,985 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,101 chars\n",
      "\n",
      "  3. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 45,106 chars\n",
      "\n",
      "  4. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 4,404 chars\n",
      "\n",
      "  5. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 2,075 chars\n",
      "\n",
      "  6. . OTHER INFORMATION\n",
      "\n",
      "     Type: part, Length: 189 chars\n",
      "\n",
      "  7. Risk Factors\n",
      "\n",
      "     Type: item, Length: 59,432 chars\n",
      "\n",
      "  8. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 102 chars\n",
      "\n",
      "  9. Defaults Upon Senior Securities\n",
      "\n",
      "     Type: item, Length: 152 chars\n",
      "\n",
      "  10. Other Information\n",
      "\n",
      "     Type: item, Length: 3,030 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive | Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 22 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 33 | Item 4. | Controls and Procedures | 34 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . Financial Information...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part II - . Other Information...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 21 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: Certificate of Incorporation of the Company, inclu...\n",
      "INFO:__main__:  ‚Ä¢ 1: Intentionally omitted....\n",
      "INFO:__main__:  ‚Ä¢ 10: Form of Note for 3.200% Notes due 2023 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 11: Form of Note for 1.875% Notes due 2026 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 12: Form of Note for 1.125% Notes due 2022 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 13: Form of Note for 0.75% Notes due 2023 ‚Äî incorporat...\n",
      "INFO:__main__:  ‚Ä¢ 14: Form of Note for 1.125% Notes due 2027 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 15: Form of Note for 1.625% Notes due 2035 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 16: Form of Note for 1.875% Notes due 2020 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 17: Form of Note for 2.875% Notes due 2025 ‚Äî incorpora...\n",
      "INFO:__main__:TOC analysis found 21 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Certificate of Incorporation of the Company, including Amendment of Certificate of Incorporation, dated July 27, 2012 ‚Äî incorporated herein by reference to Exhibit 3.1 to the Company's Quarterly Report on Form 10-Q for the quarter ended September 28, 2012.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Intentionally omitted.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.200% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.8 to the Company's Current Report on Form 8-K filed on November 1, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.875% Notes due 2026 ‚Äî incorporated herein by reference to Exhibit 4.4 to the Company's Registration Statement on Form 8-A filed on September 19, 2014.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.125% Notes due 2022 ‚Äî incorporated herein by reference to Exhibit 4.5 to the Company's Registration Statement on Form 8-A filed on September 19, 2014.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 0.75% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.125% Notes due 2027 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.625% Notes due 2035 ‚Äî incorporated herein by reference to Exhibit 4.8 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.875% Notes due 2020 ‚Äî incorporated herein by reference to Exhibit 4.5 to the Company's Current Report on Form 8-K filed on October 27, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.875% Notes due 2025 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on October 27, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.55% Notes due 2026 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on May 31, 2016.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.550% Notes due 2021 ‚Äî incorporated herein by reference to Exhibit 4.4 to the Company's Current Report on Form 8-K filed on September 1, 2016.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'As permitted by the rules of the SEC, the Company has not filed certain instruments defining the rights of holders of long-term debt of the Company or consolidated subsidiaries under which the total amount of securities authorized does not exceed 10 percent of the total assets of the Company and its consolidated subsidiaries. The Company agrees to furnish to the SEC, upon request, a copy of any omitted instrument.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'By-Laws of the Company, as amended and restated through April 22, 2020 ‚Äî incorporated herein by reference to Exhibit 3.2 to the Company's Quarterly Report on Form 10-Q for the quarter ended March 27, 2020.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Amended and Restated Indenture, dated as of April 26, 1988, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.1 to the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'First Supplemental Indenture, dated as of February 24, 1992, to Amended and Restated Indenture, dated as of April 26, 1988, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.2 to the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Second Supplemental Indenture, dated as of November 1, 2007, to Amended and Restated Indenture, dated as of April 26, 1988, as amended, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.3 of the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.150% Notes due November 15, 2020 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Current Report on Form 8-K filed on November 18, 2010.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.30% Notes due September 1, 2021 ‚Äî incorporated herein by reference to Exhibit 4.14 to the Company's Quarterly Report on Form 10-Q for the quarter ended September 30, 2011.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.500% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on March 5, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.450% Notes due 2020 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Current Report on Form 8-K filed on November 1, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. . Financial Information\n",
      "\n",
      "     Type: part, Length: 115,924 chars\n",
      "\n",
      "  2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 1,004 chars\n",
      "\n",
      "  5. . Other Information\n",
      "\n",
      "     Type: part, Length: 248 chars\n",
      "\n",
      "  6. Risk Factors\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Exhibits\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +0 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "  11. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  12. Controls and Procedures\n",
      "\n",
      "  13. Other Information\n",
      "\n",
      "  14. Executive Compensation\n",
      "\n",
      "  15. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  16. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  17. Principal Accountant Fees and Services\n",
      "\n",
      "  18. Item 15.    Exhibit and Financial Statement Schedules\n",
      "\n",
      "  19. Form 10-K Summary\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start, # Changed from match.start() for consistency with line-based detection\n",
    "                'end_pos': line_end,     # Changed from match.end()\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part # Get part from DocumentSection, e.g., \"PART I\", \"PART II\"\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Use part_number from DocumentSection to decide which 10Q map to use\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                # Fallback if part not clearly identified, try both maps\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        # If it's a PART section, check if it also includes an item title, as some PARTs have \"PART I. FINANCIAL INFORMATION\"\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            # This handles cases like \"PART I - Item 1. Financial Statements\" if detect_sections_universal_sec captures it that way\n",
    "            return f\"{part_number} - {section.title.replace(part_number, '').strip(' -.')}\"\n",
    "        return part_number # Just return \"PART I\", \"PART II\" etc.\n",
    "\n",
    "    # Fallback for named_section, content, or document type sections\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns with variable whitespace and optional period after item number\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns with variable whitespace\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line)\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped)\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line)\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated)\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\")\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g. \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., \"1.\")\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            # Extract section identifier and title more carefully\n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if len(groups) > 0:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Check if it's an Item/Part ID based on common patterns (e.g., \"1\", \"1A\", \"I\", \"II\")\n",
    "                if re.match(r'^\\d+[A-C]?$', potential_id, re.I) or re.match(r'^[IVX]+$', potential_id, re.I):\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(re.escape('[TABLE_END]') + r'.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    elif 'Item' in full_line or 'PART' in full_line:\n",
    "                        # Extract title after \"Item X.\" or \"PART X\"\n",
    "                        clean_line = re.sub(r'^\\s*(Item|PART)\\s*\\d*[A-C]*[IVX]*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', full_line, flags=re.I).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else: # Fallback if clean_line is too long or empty\n",
    "                             section_title = full_line # Still use full line as title if too complex\n",
    "                else: # If the first group was not an ID, treat as generic title\n",
    "                    section_title = full_line\n",
    "                    # Attempt to extract ID if it's a known named section (e.g., \"BUSINESS\")\n",
    "                    if 'BUSINESS' in full_line.upper(): section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper(): section_id = '1A'\n",
    "                    # Add other named section mappings if needed. These will typically be caught by the direct regex anyway.\n",
    "\n",
    "            # Store the original start/end of the line for correct content extraction\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown', # Default to 'unknown' if no ID found\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start() # Keep for internal sorting preference\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or 'content'\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If same ID (e.g., multiple \"Item 1\" mentions), keep the earliest one unless a stronger pattern comes up\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match # Replace with higher priority pattern\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # End position is the start of the next matched section, or end of content if it's the last one\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title to be just the part if it's a generic capture\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item\n",
    "            # Refine title to be just the item if it's a generic capture\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                title = f\"Item {item_number}\"\n",
    "        # For named_section, title is already the full_line or specific keyword match\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # CORRECTED: Relaxed whitespace and optional period for item numbers.\n",
    "    # Also made \"Item\" and \"PART\" literal words, not regex metacharacters.\n",
    "    item_patterns = [\n",
    "        # Example: \"Item 1. Financial Statements | 3\"\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+', re.DOTALL),\n",
    "        # Example: \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        # Example: \"Item 1A. Risk Factors\" (not in table, without page number)\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        # Example: \"1. | Financial Statements | 3\" (starting with number, in table)\n",
    "        re.compile(r'(?i)(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+', re.DOTALL),\n",
    "        # Example: \"PART II\" (simple part declaration)\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)', re.M)\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    # Only try to find items if TOC content was found\n",
    "    if toc_content:\n",
    "        for pattern in item_patterns:\n",
    "            for match in pattern.finditer(toc_content):\n",
    "                groups = match.groups()\n",
    "                item_id = None\n",
    "                item_title = \"\"\n",
    "\n",
    "                if len(groups) >= 2: # Pattern captured both ID and Title\n",
    "                    item_id = groups[0].strip()\n",
    "                    item_title = groups[1].strip()\n",
    "                elif len(groups) == 1: # Pattern only captured ID\n",
    "                    item_id = groups[0].strip()\n",
    "                    # Attempt to get text immediately following the ID match in the TOC line\n",
    "                    line_remainder_start = match.end()\n",
    "                    line_end_of_match = toc_content.find('\\n', line_remainder_start)\n",
    "                    if line_end_of_match == -1:\n",
    "                        line_end_of_match = len(toc_content)\n",
    "                    \n",
    "                    potential_title_from_line = toc_content[line_remainder_start:line_end_of_match].strip()\n",
    "                    if potential_title_from_line:\n",
    "                        item_title = potential_title_from_line\n",
    "                    else:\n",
    "                        item_title = f\"Section {item_id}\" # Fallback generic title\n",
    "\n",
    "                if item_id: # Ensure an ID was captured\n",
    "                    item_title = re.sub(r'\\s+', ' ', item_title).strip() # Normalize whitespace\n",
    "                    found_items.append((item_id, item_title))\n",
    "\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    # Sort found items by their ID for more consistent processing, then by title for tie-breaking\n",
    "    found_items.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    for item_id, title in found_items:\n",
    "        # Create a unique key for deduplication, focusing on ID and a portion of title\n",
    "        key = f\"{item_id}_{title[:50]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    current_part = None # Track current part for items found in TOC\n",
    "\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None # Initial value\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "            part_num = current_part # Assign the last seen part to this item\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = f\"PART {item_id}\"\n",
    "            current_part = part_num # Update the current part context\n",
    "        else:\n",
    "            section_type = 'content' # Treat as generic content section\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is intentionally empty here; will be filled by main sectioning if this strategy is chosen.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num # Store the identified part (either detected or inherited)\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # Sort toc_entries by their expected appearance in the document if they don't have start_pos\n",
    "        # This is crucial for iterating and finding them correctly in the content.\n",
    "        # If TOC parsing doesn't give start_pos, rely on the sequence.\n",
    "        # If TOC parsing gives parts/items, sort by those.\n",
    "        toc_entries_sorted = sorted(toc_entries, key=lambda x: (x.part if x.part else '', x.item_number if x.item_number else '', x.title))\n",
    "\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries_sorted): # Iterate through sorted TOC entries\n",
    "            # Create flexible regex for the title/item number to find it in the main content\n",
    "            pattern_parts = []\n",
    "            if toc_entry.item_number:\n",
    "                # Be flexible about \"Item\" prefix and trailing period\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part:\n",
    "                # Be flexible about \"PART\" prefix\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")))\n",
    "            \n",
    "            # Use the full title as a fallback if item/part number is not explicit or title is more descriptive\n",
    "            if toc_entry.title:\n",
    "                # Ensure escaped title allows for flexible whitespace in the content\n",
    "                pattern_parts.append(re.escape(toc_entry.title).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "            if not pattern_parts: # Skip if no pattern can be formed for this TOC entry\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            # Search from current_content_pos to ensure sequential parsing\n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                # The content for this section goes until the start of the next TOC entry, or end of document\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries_sorted): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries_sorted[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    if next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")))\n",
    "                    if next_toc_entry.title:\n",
    "                        next_pattern_parts.append(re.escape(next_toc_entry.title).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        # Search for the next section starting *after* the current match's end\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part, # Preserve the part info derived from TOC\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "                # If a TOC entry is not found, its content might be part of the previous section,\n",
    "                # or it's a false positive in the TOC. For simplicity, we just move on.\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0\n",
    "\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    sections_1 = detect_sections_strategy_1_improved(content)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    sections_2 = detect_sections_strategy_2(content)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "if chunks:\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison)\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    sections_dist = {}\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0 # Renamed variable to avoid conflict with function parameter 'overlap_tokens'\n",
    "\n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            # Ensure we don't go past the start of the chunk\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # If after trying to create overlap, we still don't have enough tokens for overlap\n",
    "            # (e.g., first few sentences are very long), just take some minimal content.\n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                # Fallback to last sentence if no other overlap possible and current chunk exists\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test functions adapted to _fixed suffix to avoid NameErrors from notebook re-runs\n",
    "# Ensure these are called after all function definitions.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\", # This file name is in the future based on current date\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b8f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n",
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmptvzb4wl9_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated | Statements of Comprehensive | Income (Loss) | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 20 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 4. | Controls and Procedures | 32 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 00: $...\n",
      "INFO:__main__:  ‚Ä¢ 04: $...\n",
      "INFO:__main__:  ‚Ä¢ 18: $...\n",
      "INFO:__main__:  ‚Ä¢ 26: $...\n",
      "INFO:__main__:  ‚Ä¢ 37: $...\n",
      "INFO:__main__:  ‚Ä¢ 40: $...\n",
      "INFO:__main__:  ‚Ä¢ 56: $...\n",
      "INFO:__main__:  ‚Ä¢ 58: $...\n",
      "INFO:__main__:  ‚Ä¢ 68: $...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '$'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 99 chars\n",
      "\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "\n",
      "     Type: part, Length: 4,191 chars\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 52 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Business...\n",
      "INFO:__main__:  ‚Ä¢ 10: ...\n",
      "INFO:__main__:  ‚Ä¢ 10: Directors, Executive Officers, and Corporate Gover...\n",
      "INFO:__main__:  ‚Ä¢ 11: ...\n",
      "INFO:__main__:  ‚Ä¢ 11: Executive Compensation...\n",
      "INFO:__main__:  ‚Ä¢ 12: ...\n",
      "INFO:__main__:  ‚Ä¢ 12: Security Ownership of Certain Beneficial Owners an...\n",
      "INFO:__main__:  ‚Ä¢ 13: ...\n",
      "INFO:__main__:  ‚Ä¢ 13: Certain Relationships and Related Transactions, an...\n",
      "INFO:__main__:TOC analysis found 52 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Business'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Directors, Executive Officers, and Corporate Governance'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Executive Compensation'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Certain Relationships and Related Transactions, and Director Independence'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Principal Accountant Fees and Services'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits, Financial Statement Schedules'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form 10-K Summary'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unresolved Staff Comments'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Properties'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Reserved'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements and Supplementary Data'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Changes in and Disagreements with Accountants on Accounting and Financial Disclosure'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Disclosure Regarding Foreign Jurisdictions that Prevent Inspections'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 1. | Business | 3 | Item 1A. | Risk Factors | 6 | Item 1B. | Unresolved Staff Comments | 16 | Item 2. | Properties | 17 | Item 3. | Legal Proceedings | 17 | Item 4. | Mine Safety Disclosures | 17 | PART II | Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 10.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 15.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part II - . OTHER INFORMATION...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 13,293 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 55,960 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 106 chars\n",
      "\n",
      "  4. Properties\n",
      "\n",
      "     Type: item, Length: 1,437 chars\n",
      "\n",
      "  5. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 185 chars\n",
      "\n",
      "  6. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 113 chars\n",
      "\n",
      "  7. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 516 chars\n",
      "\n",
      "  8. Reserved\n",
      "\n",
      "     Type: item, Length: 50,497 chars\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,524 chars\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 86,346 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. . FINANCIAL INFORMATION\n",
      "\n",
      "     Type: part, Length: 34,985 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,101 chars\n",
      "\n",
      "  3. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 45,106 chars\n",
      "\n",
      "  4. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 4,404 chars\n",
      "\n",
      "  5. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 2,075 chars\n",
      "\n",
      "  6. . OTHER INFORMATION\n",
      "\n",
      "     Type: part, Length: 189 chars\n",
      "\n",
      "  7. Risk Factors\n",
      "\n",
      "     Type: item, Length: 59,432 chars\n",
      "\n",
      "  8. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 102 chars\n",
      "\n",
      "  9. Defaults Upon Senior Securities\n",
      "\n",
      "     Type: item, Length: 152 chars\n",
      "\n",
      "  10. Other Information\n",
      "\n",
      "     Type: item, Length: 3,030 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 22 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1: Financial Statements...\n",
      "INFO:__main__:  ‚Ä¢ 1: Legal Proceedings...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Risk Factors...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Management‚Äôs Discussion and Analysis of Financial ...\n",
      "INFO:__main__:  ‚Ä¢ 2: Unregistered Sales of Equity Securities and Use of...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: Consolidated Statements of Operations...\n",
      "INFO:__main__:TOC analysis found 22 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Financial Statements'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Legal Proceedings'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Risk Factors'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Unregistered Sales of Equity Securities and Use of Proceeds'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Statements of Operations'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Defaults Upon Senior Securities'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Quantitative and Qualitative Disclosures About Market Risk'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Controls and Procedures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Mine Safety Disclosures'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Consolidated Balance Sheets'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Other Information'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Exhibits'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive | Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 22 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 33 | Item 4. | Controls and Procedures | 34 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . Financial Information...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part II - . Other Information...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 21 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: Certificate of Incorporation of the Company, inclu...\n",
      "INFO:__main__:  ‚Ä¢ 1: Intentionally omitted....\n",
      "INFO:__main__:  ‚Ä¢ 10: Form of Note for 3.200% Notes due 2023 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 11: Form of Note for 1.875% Notes due 2026 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 12: Form of Note for 1.125% Notes due 2022 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 13: Form of Note for 0.75% Notes due 2023 ‚Äî incorporat...\n",
      "INFO:__main__:  ‚Ä¢ 14: Form of Note for 1.125% Notes due 2027 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 15: Form of Note for 1.625% Notes due 2035 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 16: Form of Note for 1.875% Notes due 2020 ‚Äî incorpora...\n",
      "INFO:__main__:  ‚Ä¢ 17: Form of Note for 2.875% Notes due 2025 ‚Äî incorpora...\n",
      "INFO:__main__:TOC analysis found 21 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Certificate of Incorporation of the Company, including Amendment of Certificate of Incorporation, dated July 27, 2012 ‚Äî incorporated herein by reference to Exhibit 3.1 to the Company's Quarterly Report on Form 10-Q for the quarter ended September 28, 2012.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Intentionally omitted.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.200% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.8 to the Company's Current Report on Form 8-K filed on November 1, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.875% Notes due 2026 ‚Äî incorporated herein by reference to Exhibit 4.4 to the Company's Registration Statement on Form 8-A filed on September 19, 2014.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.125% Notes due 2022 ‚Äî incorporated herein by reference to Exhibit 4.5 to the Company's Registration Statement on Form 8-A filed on September 19, 2014.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 0.75% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.125% Notes due 2027 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.625% Notes due 2035 ‚Äî incorporated herein by reference to Exhibit 4.8 to the Company's Registration Statement on Form 8-A filed on March 6, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.875% Notes due 2020 ‚Äî incorporated herein by reference to Exhibit 4.5 to the Company's Current Report on Form 8-K filed on October 27, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.875% Notes due 2025 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on October 27, 2015.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.55% Notes due 2026 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on May 31, 2016.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 1.550% Notes due 2021 ‚Äî incorporated herein by reference to Exhibit 4.4 to the Company's Current Report on Form 8-K filed on September 1, 2016.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'As permitted by the rules of the SEC, the Company has not filed certain instruments defining the rights of holders of long-term debt of the Company or consolidated subsidiaries under which the total amount of securities authorized does not exceed 10 percent of the total assets of the Company and its consolidated subsidiaries. The Company agrees to furnish to the SEC, upon request, a copy of any omitted instrument.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'By-Laws of the Company, as amended and restated through April 22, 2020 ‚Äî incorporated herein by reference to Exhibit 3.2 to the Company's Quarterly Report on Form 10-Q for the quarter ended March 27, 2020.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Amended and Restated Indenture, dated as of April 26, 1988, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.1 to the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'First Supplemental Indenture, dated as of February 24, 1992, to Amended and Restated Indenture, dated as of April 26, 1988, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.2 to the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Second Supplemental Indenture, dated as of November 1, 2007, to Amended and Restated Indenture, dated as of April 26, 1988, as amended, between the Company and Deutsche Bank Trust Company Americas, as successor to Bankers Trust Company, as trustee ‚Äî incorporated herein by reference to Exhibit 4.3 of the Company's Current Report on Form 8-K filed on May 25, 2017.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.150% Notes due November 15, 2020 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Current Report on Form 8-K filed on November 18, 2010.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 3.30% Notes due September 1, 2021 ‚Äî incorporated herein by reference to Exhibit 4.14 to the Company's Quarterly Report on Form 10-Q for the quarter ended September 30, 2011.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.500% Notes due 2023 ‚Äî incorporated herein by reference to Exhibit 4.6 to the Company's Current Report on Form 8-K filed on March 5, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Form of Note for 2.450% Notes due 2020 ‚Äî incorporated herein by reference to Exhibit 4.7 to the Company's Current Report on Form 8-K filed on November 1, 2013.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. . Financial Information\n",
      "\n",
      "     Type: part, Length: 115,924 chars\n",
      "\n",
      "  2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 1,004 chars\n",
      "\n",
      "  5. . Other Information\n",
      "\n",
      "     Type: part, Length: 248 chars\n",
      "\n",
      "  6. Risk Factors\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Exhibits\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +0 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "  11. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  12. Controls and Procedures\n",
      "\n",
      "  13. Other Information\n",
      "\n",
      "  14. Executive Compensation\n",
      "\n",
      "  15. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  16. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  17. Principal Accountant Fees and Services\n",
      "\n",
      "  18. Item 15.    Exhibit and Financial Statement Schedules\n",
      "\n",
      "  19. Form 10-K Summary\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part # Get part from DocumentSection, e.g., \"PART I\", \"PART II\"\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Use part_number from DocumentSection to decide which 10Q map to use\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                # Fallback if part not clearly identified, try both maps\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        # If it's a PART section, check if it also includes an item title, as some PARTs have \"PART I. FINANCIAL INFORMATION\"\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            # This handles cases like \"PART I - Item 1. Financial Statements\" if detect_sections_universal_sec captures it that way\n",
    "            return f\"{part_number} - {section.title.replace(part_number, '').strip(' -.')}\"\n",
    "        return part_number # Just return \"PART I\", \"PART II\" etc.\n",
    "\n",
    "    # Fallback for named_section, content, or document type sections\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns with variable whitespace and optional period after item number\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns with variable whitespace\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line)\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped)\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line)\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated)\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\")\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g. \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., \"1.\")\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            # Extract section identifier and title more carefully\n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if len(groups) > 0:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Check if it's an Item/Part ID based on common patterns (e.g., \"1\", \"1A\", \"I\", \"II\")\n",
    "                if re.match(r'^\\d+[A-C]?$', potential_id, re.I) or re.match(r'^[IVX]+$', potential_id, re.I):\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(re.escape('[TABLE_END]') + r'.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    elif 'Item' in full_line or 'PART' in full_line:\n",
    "                        # Extract title after \"Item X.\" or \"PART X\"\n",
    "                        clean_line = re.sub(r'^\\s*(Item|PART)\\s*\\d*[A-C]*[IVX]*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', full_line, flags=re.I).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else: # Fallback if clean_line is too long or empty\n",
    "                             section_title = full_line # Still use full line as title if too complex\n",
    "                else: # If the first group was not an ID, treat as generic title\n",
    "                    section_title = full_line\n",
    "                    # Attempt to extract ID if it's a known named section (e.g., \"BUSINESS\")\n",
    "                    if 'BUSINESS' in full_line.upper(): section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper(): section_id = '1A'\n",
    "                    # Add other named section mappings if needed. These will typically be caught by the direct regex anyway.\n",
    "\n",
    "            # Store the original start/end of the line for correct content extraction\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown', # Default to 'unknown' if no ID found\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start() # Keep for internal sorting preference\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or 'content'\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If same ID (e.g., multiple \"Item 1\" mentions), keep the earliest one unless a stronger pattern comes up\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match # Replace with higher priority pattern\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # End position is the start of the next matched section, or end of content if it's the last one\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title to be just the part if it's a generic capture\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item\n",
    "            # Refine title to be just the item if it's a generic capture\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                title = f\"Item {item_number}\"\n",
    "        # For named_section, title is already the full_line or specific keyword match\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # CORRECTED: Relaxed whitespace and optional period for item numbers.\n",
    "    # Also made \"Item\" and \"PART\" literal words, not regex metacharacters.\n",
    "    item_patterns = [\n",
    "        # Example: \"Item 1. Financial Statements | 3\"\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+', re.DOTALL),\n",
    "        # Example: \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        # Example: \"Item 1A. Risk Factors\" (not in table, without page number)\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        # Example: \"1. | Financial Statements | 3\" (starting with number, in table)\n",
    "        re.compile(r'(?i)(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+', re.DOTALL),\n",
    "        # Example: \"PART II\" (simple part declaration)\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)', re.M)\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    # Only try to find items if TOC content was found\n",
    "    if toc_content:\n",
    "        for pattern in item_patterns:\n",
    "            for match in pattern.finditer(toc_content):\n",
    "                groups = match.groups()\n",
    "                item_id = None\n",
    "                item_title = \"\"\n",
    "\n",
    "                if len(groups) >= 2: # Pattern captured both ID and Title\n",
    "                    item_id = groups[0].strip()\n",
    "                    item_title = groups[1].strip()\n",
    "                elif len(groups) == 1: # Pattern only captured ID\n",
    "                    item_id = groups[0].strip()\n",
    "                    # Attempt to get text immediately following the ID match in the TOC line\n",
    "                    line_remainder_start = match.end()\n",
    "                    line_end_of_match = toc_content.find('\\n', line_remainder_start)\n",
    "                    if line_end_of_match == -1:\n",
    "                        line_end_of_match = len(toc_content)\n",
    "                    \n",
    "                    potential_title_from_line = toc_content[line_remainder_start:line_end_of_match].strip()\n",
    "                    if potential_title_from_line:\n",
    "                        item_title = potential_title_from_line\n",
    "                    else:\n",
    "                        item_title = f\"Section {item_id}\" # Fallback generic title\n",
    "\n",
    "                if item_id: # Ensure an ID was captured\n",
    "                    item_title = re.sub(r'\\s+', ' ', item_title).strip() # Normalize whitespace\n",
    "                    found_items.append((item_id, item_title))\n",
    "\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    # Sort found items by their ID for more consistent processing, then by title for tie-breaking\n",
    "    found_items.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    for item_id, title in found_items:\n",
    "        # Create a unique key for deduplication, focusing on ID and a portion of title\n",
    "        key = f\"{item_id}_{title[:50]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    current_part = None # Track current part for items found in TOC\n",
    "\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None # Initial value\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "            part_num = current_part # Assign the last seen part to this item\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = f\"PART {item_id}\"\n",
    "            current_part = part_num # Update the current part context\n",
    "        else:\n",
    "            section_type = 'content' # Treat as generic content section\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is intentionally empty here; will be filled by main sectioning if this strategy is chosen.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num # Store the identified part (either detected or inherited)\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # Sort toc_entries by their expected appearance in the document if they don't have start_pos\n",
    "        # This is crucial for iterating and finding them correctly in the content.\n",
    "        # If TOC parsing doesn't give start_pos, rely on the sequence.\n",
    "        # If TOC parsing gives parts/items, sort by those.\n",
    "        toc_entries_sorted = sorted(toc_entries, key=lambda x: (x.part if x.part else '', x.item_number if x.item_number else '', x.title))\n",
    "\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries_sorted): # Iterate through sorted TOC entries\n",
    "            # Create flexible regex for the title/item number to find it in the main content\n",
    "            pattern_parts = []\n",
    "            if toc_entry.item_number:\n",
    "                # Be flexible about \"Item\" prefix and trailing period\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part:\n",
    "                # Be flexible about \"PART\" prefix\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")))\n",
    "            \n",
    "            # Use the full title as a fallback if item/part number is not explicit or title is more descriptive\n",
    "            if toc_entry.title:\n",
    "                # Ensure escaped title allows for flexible whitespace in the content\n",
    "                pattern_parts.append(re.escape(toc_entry.title).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "            if not pattern_parts: # Skip if no pattern can be formed for this TOC entry\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            # Search from current_content_pos to ensure sequential parsing\n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                # The content for this section goes until the start of the next TOC entry, or end of document\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries_sorted): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries_sorted[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    if next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")))\n",
    "                    if next_toc_entry.title:\n",
    "                        next_pattern_parts.append(re.escape(next_toc_entry.title).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        # Search for the next section starting *after* the current match's end\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part, # Preserve the part info derived from TOC\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "                # If a TOC entry is not found, its content might be part of the previous section,\n",
    "                # or it's a false positive in the TOC. For simplicity, we just move on.\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0\n",
    "\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    sections_1 = detect_sections_strategy_1_improved(content)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    sections_2 = detect_sections_strategy_2(content)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "if chunks:\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison)\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    sections_dist = {}\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb6ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part # This will be like \"PART I\", \"PART II\" or None\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Prioritize using the part_number if available from section detection\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                # Fallback: if part is not explicitly set, try to infer from item maps\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        # If it's a PART section itself, format it. Includes cases where title might capture an item.\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            # Example: \"PART I - Item 1. Business\" if detected that way\n",
    "            # Strip \"PART X\" from the original title if it's already there to avoid duplication\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number # Just returns \"PART I\", \"PART II\" etc.\n",
    "\n",
    "    # Fallback for named_section (e.g., \"BUSINESS\" without Item number), 'content', or 'document' types\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns: e.g., \"[TABLE_START] Item 1. | Business...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns: e.g., \"[TABLE_START] PART I | FINANCIAL INFORMATION...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line): e.g., \"Item 1. Business\"\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped): e.g., \"Item 1. | Business\"\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line): e.g., \"PART I. FINANCIAL INFORMATION\"\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated): e.g., \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\", usually at start of line)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g., \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers (e.g., \"BUSINESS\", \"RISK FACTORS\")\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., \"1.\")\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or # Exclude table markers if not part of a valid section header\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Determine if the first captured group is a valid Item/Part ID\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        # Clean up title: remove trailing table markers like \"[TABLE_END]\" if they were captured\n",
    "                        section_title = re.sub(re.escape('[TABLE_END]') + r'.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip() # Remove pipe characters\n",
    "                    else: # No title captured explicitly by the group\n",
    "                        # Try to extract title from the rest of the line after the ID match\n",
    "                        clean_line = re.sub(r'^\\s*(Item|PART)\\s*\\d*[A-C]*[IVX]*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', full_line, flags=re.I).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line # Fallback to full line if title extraction is problematic\n",
    "                else: # First captured group was not a standard Item/Part ID, treat as part of title\n",
    "                    section_title = full_line\n",
    "                    # For generic named sections, use their fixed ID if applicable (e.g., BUSINESS -> 1)\n",
    "                    if 'BUSINESS' in full_line.upper() and 'ITEM' not in full_line.upper(): section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and 'ITEM' not in full_line.upper(): section_id = '1A'\n",
    "\n",
    "            # Store the original start/end of the line for correct content extraction\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns early in the list)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or less specific types\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If both are specific, prefer the one matched by a higher-priority pattern (lower index)\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                # If they have the same ID but the new match offers a cleaner/more robust title\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8: # Heuristic for \"cleaner\"\n",
    "                     final_matches[-1] = current_match\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title: remove \"PART X\" if it's already in the title to avoid redundancy.\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part # Fallback to just \"PART X\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item (inherited)\n",
    "            # Refine title: remove \"Item X\" if it's already in the title\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\" # Fallback to just \"Item X\"\n",
    "        # For named_section (e.g., \"BUSINESS\" when it's not explicitly an Item number)\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    # Using re.escape for literal brackets, and compiling patterns once.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # CORRECTED: Significant refinement here. Removed redundant escapes,\n",
    "    # and ensured capture groups correctly isolate ID and title without extra junk.\n",
    "    item_patterns = [\n",
    "        # Standard table format: \"Item X. Title | Page\" or \"Item X. Title\" or \"Item X | Title\"\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*(?:\\|\\s*|\\s*([^\\n|]+?)\\s*(?:\\|\\s*\\d+)?$|([^\\n|]+)$)', re.M),\n",
    "        # Standard table format: \"PART X | Title\"\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)\\s*\\|\\s*([^\\n|]+)', re.M),\n",
    "        # Standalone Item line (e.g., \"Item 1A. Risk Factors\")\n",
    "        re.compile(r'(?i)^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.M),\n",
    "        # Standalone Part line (e.g., \"PART II. Other Information\")\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.M),\n",
    "        # Number-dot format (e.g., \"1. Financial Statements\") often seen as main heading in TOC tables\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    if toc_content: # Only try to find items if TOC content was found\n",
    "        for pattern in item_patterns:\n",
    "            for match in pattern.finditer(toc_content):\n",
    "                groups = match.groups()\n",
    "                item_id = None\n",
    "                item_title = \"\"\n",
    "\n",
    "                # Extract ID (first non-None group)\n",
    "                if groups[0] is not None:\n",
    "                    item_id = groups[0].strip()\n",
    "                else: # This case is for patterns where the ID isn't the first group, or if pattern needs adjustment\n",
    "                    continue # Skip if no ID is clearly captured by the primary group\n",
    "\n",
    "                # Extract title (find the first non-None group after the ID)\n",
    "                if len(groups) > 1:\n",
    "                    for g in groups[1:]:\n",
    "                        if g is not None:\n",
    "                            item_title = g.strip()\n",
    "                            break\n",
    "                \n",
    "                # Further clean title to remove common TOC artifacts\n",
    "                item_title = re.sub(r'\\|\\s*\\d+', '', item_title).strip() # Remove \"| PageNumber\"\n",
    "                item_title = re.sub(r'\\s*\\.\\s*$', '', item_title).strip() # Remove trailing periods\n",
    "                item_title = re.sub(r'\\s+', ' ', item_title).strip() # Normalize whitespace\n",
    "\n",
    "                if item_id and item_title: # Ensure both ID and a meaningful title are present\n",
    "                    found_items.append((item_id, item_title))\n",
    "                elif item_id and not item_title: # If ID found but no title, use generic\n",
    "                    if re.match(r'^\\d+[A-C]?$', item_id, re.I):\n",
    "                        item_title = f\"Item {item_id}\"\n",
    "                    elif re.match(r'^[IVX]+$', item_id, re.I):\n",
    "                        item_title = f\"PART {item_id}\"\n",
    "                    if item_title:\n",
    "                        found_items.append((item_id, item_title))\n",
    "\n",
    "\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    # Sort found items by their ID for more consistent processing, then by title for tie-breaking\n",
    "    # Use tuple for sorting to maintain order when IDs are same\n",
    "    found_items.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    for item_id, title in found_items:\n",
    "        # Create a unique key for deduplication, focusing on ID and a portion of title\n",
    "        # Use a more robust key to distinguish similar titles or items.\n",
    "        key = (item_id, title) # Use full tuple for strict uniqueness for now\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    current_part = None # Track current part for items found in TOC\n",
    "\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None # Initial value\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "            part_num = current_part # Assign the last seen part to this item\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = f\"PART {item_id}\"\n",
    "            current_part = part_num # Update the current part context\n",
    "        else:\n",
    "            section_type = 'content' # Treat as generic content section if no item/part found\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is intentionally empty here; will be filled by main sectioning if this strategy is chosen.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num # Store the identified part (either detected or inherited)\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # Sort toc_entries by their estimated location, using Item/Part numbers for robust sorting\n",
    "        # This is crucial for iterating and finding them correctly in the content.\n",
    "        def get_sort_key(entry):\n",
    "            if entry.part and re.match(r'PART\\s*([IVX]+)', entry.part):\n",
    "                # Convert Roman numerals to int for sorting\n",
    "                roman_map = {'I':1, 'V':5, 'X':10, 'L':50, 'C':100, 'D':500, 'M':1000}\n",
    "                part_int = 0\n",
    "                for char in entry.part.replace(\"PART \", \"\"):\n",
    "                    part_int += roman_map.get(char, 0)\n",
    "                return (part_int, entry.item_number if entry.item_number else '')\n",
    "            elif entry.item_number:\n",
    "                # Convert item number (e.g., \"1A\") to a sortable tuple (1, 'A')\n",
    "                num_part = re.match(r'(\\d+)([A-C]?)', entry.item_number)\n",
    "                if num_part:\n",
    "                    base_num = int(num_part.group(1))\n",
    "                    alpha_part = num_part.group(2) if num_part.group(2) else ''\n",
    "                    return (base_num, alpha_part)\n",
    "                return (float('inf'), entry.title) # Fallback for malformed item_number\n",
    "            return (float('inf'), entry.title) # Fallback for entries with no part/item\n",
    "\n",
    "        toc_entries_sorted = sorted(toc_entries, key=get_sort_key)\n",
    "\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries_sorted):\n",
    "            # Create flexible regex for the title/item number to find it in the main content\n",
    "            pattern_parts = []\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers as they are more consistent\n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part:\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")))\n",
    "            \n",
    "            # Add the full cleaned title as a fallback/alternative match\n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip() # Remove page numbers like \"| 3\"\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip() # Remove trailing periods\n",
    "                pattern_parts.append(re.escape(cleaned_title_for_regex).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "            if not pattern_parts: # Should not happen if TOC parsing is good\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                # The content for this section goes until the start of the next TOC entry, or end of document\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries_sorted): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries_sorted[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    if next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")))\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_pattern_parts.append(re.escape(next_cleaned_title_for_regex).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part, # Preserve the part info derived from TOC\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b524b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 26 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: Item 1...\n",
      "INFO:__main__:  ‚Ä¢ 10: Item 10...\n",
      "INFO:__main__:  ‚Ä¢ 11: Item 11...\n",
      "INFO:__main__:  ‚Ä¢ 12: Item 12...\n",
      "INFO:__main__:  ‚Ä¢ 13: Item 13...\n",
      "INFO:__main__:  ‚Ä¢ 14: Item 14...\n",
      "INFO:__main__:  ‚Ä¢ 15: Item 15...\n",
      "INFO:__main__:  ‚Ä¢ 16: Item 16...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Item 1A...\n",
      "INFO:__main__:  ‚Ä¢ 1B: Item 1B...\n",
      "INFO:__main__:TOC analysis found 26 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1A'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1B'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 3'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 10'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 4'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 6'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 15'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 7'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 7A'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 8'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 9'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 9A'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 9B'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 9C'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 10'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 11'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 12'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 13'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 14'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 15'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 16'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 7 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: Item 1...\n",
      "INFO:__main__:  ‚Ä¢ 1A: Item 1A...\n",
      "INFO:__main__:  ‚Ä¢ 2: Item 2...\n",
      "INFO:__main__:  ‚Ä¢ 3: Item 3...\n",
      "INFO:__main__:  ‚Ä¢ 4: Item 4...\n",
      "INFO:__main__:  ‚Ä¢ 5: Item 5...\n",
      "INFO:__main__:  ‚Ä¢ 6: Item 6...\n",
      "INFO:__main__:TOC analysis found 7 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1A'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 3'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 4'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 6'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +0 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa56ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part # This will be like \"PART I\", \"PART II\" or None\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Prioritize using the part_number if available from section detection\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                # Fallback if part not clearly identified, try both maps\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        # If it's a PART section itself, format it. Includes cases where title might capture an item.\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            # Example: \"PART I - Item 1. Financial Statements\" if detected that way\n",
    "            # Strip \"PART X\" from the original title if it's already there to avoid duplication\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number # Just returns \"PART I\", \"PART II\" etc.\n",
    "\n",
    "    # Fallback for named_section (e.g., \"BUSINESS\" without Item number), 'content', or 'document' types\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns: e.g., \"[TABLE_START] Item 1. | Business...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns: e.g., \"[TABLE_START] PART I | FINANCIAL INFORMATION...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line): e.g., \"Item 1. Business\"\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped): e.g., \"Item 1. | Business\"\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line): e.g., \"PART I. FINANCIAL INFORMATION\"\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated): e.g., \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\", usually at start of line)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g., \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers (e.g., \"BUSINESS\", \"RISK FACTORS\")\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives (e.g., content that looks like a header but isn't)\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., just \"1.\")\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or # Exclude table markers if not part of a valid section header\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line if specific extraction fails\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Determine if the first captured group is a valid Item/Part ID\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        # Clean up title: remove trailing table markers like \"[TABLE_END]\" if they were captured\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip() # Remove pipe characters\n",
    "                    else: # No explicit title captured by a group\n",
    "                        # Try to extract a clean title from the remainder of the line after the ID\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line # Fallback to full line if cleaning is problematic\n",
    "                else: # First captured group was not a standard Item/Part ID, treat as part of title\n",
    "                    section_title = full_line\n",
    "                    # For generic named sections (e.g., \"BUSINESS\"), assign a canonical ID if not part of an Item/Part already\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "                    # Add other named section mappings if needed.\n",
    "\n",
    "            # Store the original start/end of the line for correct content extraction\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns early in the list)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or less specific types\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If both are specific, prefer the one matched by a higher-priority pattern (lower index means earlier in list)\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                # If they have the same ID but the new match offers a cleaner/more robust title\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8: # Heuristic for \"cleaner\"\n",
    "                     final_matches[-1] = current_match\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title: remove \"PART X\" if it's already in the title to avoid redundancy.\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part # Fallback to just \"PART X\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item (inherited)\n",
    "            # Refine title: remove \"Item X\" if it's already in the title\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\" # Fallback to just \"Item X\"\n",
    "        # For named_section (e.g., \"BUSINESS\" when it's not explicitly an Item number)\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # CORRECTED: Significant refinement here. Relaxed whitespace, optional periods,\n",
    "    # and ensured capture groups correctly isolate ID and title without extra junk.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Page | PART/ITEM | Item_ID. | Title | Page_Num\n",
    "        # e.g., \"Page | PART I. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3\"\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|)?\\s*(PART\\s*[IVX]+)\\s*\\.?\\s*([^|]+?)\\s*\\|\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        # Pattern 2: PART/ITEM | Title | Page_Num (Most common, like KO, simpler AMZN)\n",
    "        # e.g., \"Part I. Financial Information | 1 | Item 1. | Financial Statements (Unaudited) | 2\" (This is tough, let's try to get clean item/title)\n",
    "        # Or: \"Item 1. | Financial Statements | 3\" or \"PART I | FINANCIAL INFORMATION | 3\"\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        # Pattern 3: Simple Item/Part ID then Title, possibly on same line, no pipes to separate title\n",
    "        # e.g., \"Item 1A. Risk Factors\" or \"PART II. OTHER INFORMATION\"\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        # Pattern 4: TOC lines that are just titles, potentially indented, often sub-sections\n",
    "        # e.g., \"Consolidated Statements of Cash Flows | 3\" (these don't have Item #, but are important sections)\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z\\s-,\\']{10,})\\s*(?:\\|\\s*\\d+)?$', re.M), # Captures longer phrases as titles\n",
    "        # Pattern 5: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id = None # To associate items without explicit part with the last seen part\n",
    "    \n",
    "    if toc_content: # Only try to find items if TOC content was found\n",
    "        # Iterate over lines for better context and to handle multi-column TOCs more flexibly\n",
    "        toc_lines = toc_content.split('\\n')\n",
    "        for line in toc_lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line) # Search within the line\n",
    "                if match:\n",
    "                    groups = match.groups()\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "\n",
    "                    # Determine item_id and item_title based on pattern groups\n",
    "                    # This logic needs to be carefully adjusted per pattern.\n",
    "                    if pattern == item_patterns[0]: # Page | PART/ITEM | Item_ID. | Title | Page_Num\n",
    "                        # Group 1: PART ID, Group 2: PART Title, Group 3: Item ID, Group 4: Item Title\n",
    "                        part_candidate = groups[0].strip() if groups[0] else None\n",
    "                        part_title_candidate = groups[1].strip() if groups[1] else None\n",
    "                        item_id = groups[2].strip() if groups[2] else None\n",
    "                        item_title = groups[3].strip() if groups[3] else \"\"\n",
    "\n",
    "                        if part_candidate: # If a PART was captured first, record it\n",
    "                            current_part_id = part_candidate\n",
    "                            found_items.append((part_candidate, part_title_candidate, 'part')) # Record PART entry\n",
    "\n",
    "                        if item_id: # Then record the ITEM entry\n",
    "                            found_items.append((item_id, item_title, 'item'))\n",
    "                            break # Found a match for this line, move to next line\n",
    "                    \n",
    "                    elif pattern == item_patterns[1] or pattern == item_patterns[2] or pattern == item_patterns[4]: # Item/PART | Title | Page, or Item/PART. Title\n",
    "                        # For these patterns, group 1 is ID, group 2 is title (or part of it)\n",
    "                        item_id = groups[0].strip() if groups[0] else None\n",
    "                        item_title = groups[1].strip() if len(groups) > 1 and groups[1] else \"\" # Title captured by second group\n",
    "\n",
    "                        if item_id:\n",
    "                            clean_item_title = re.sub(r'\\|\\s*\\d+', '', item_title).strip() # Remove \"| page\"\n",
    "                            clean_item_title = re.sub(r'\\s*\\.\\s*$', '', clean_item_title).strip() # Remove trailing periods\n",
    "                            clean_item_title = re.sub(r'\\s+', ' ', clean_item_title).strip() # Normalize whitespace\n",
    "                            \n",
    "                            section_type = 'item' if re.match(r'^\\d+[A-C]?$', item_id, re.I) else 'part'\n",
    "                            found_items.append((item_id, clean_item_title, section_type))\n",
    "                            \n",
    "                            if section_type == 'part':\n",
    "                                current_part_id = f\"PART {item_id}\" # Update current part for subsequent items\n",
    "\n",
    "                            break # Found a match for this line, move to next line\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Number-dot format in TOC\n",
    "                        item_id = groups[0].strip() if groups[0] else None\n",
    "                        item_title = groups[1].strip() if len(groups) > 1 and groups[1] else \"\"\n",
    "                        \n",
    "                        if item_id and item_title:\n",
    "                            clean_item_title = re.sub(r'\\|\\s*\\d+', '', item_title).strip()\n",
    "                            clean_item_title = re.sub(r'\\s*\\.\\s*$', '', clean_item_title).strip()\n",
    "                            clean_item_title = re.sub(r'\\s+', ' ', clean_item_title).strip()\n",
    "                            found_items.append((item_id, clean_item_title, 'item'))\n",
    "                            break\n",
    "                    \n",
    "                    # For Pattern 4: Generic Section Titles (e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                    # These don't have item numbers directly in the TOC, but are important sections.\n",
    "                    # We'll assign them a type 'named_section' and associate with current_part_id\n",
    "                    elif pattern == item_patterns[4]: # This is the simple title match pattern (index 4)\n",
    "                        item_title = groups[0].strip() # This is the entire captured title\n",
    "                        if item_title and len(item_title) > 10: # Heuristic: only consider if title is reasonably long\n",
    "                             clean_item_title = re.sub(r'\\|\\s*\\d+', '', item_title).strip()\n",
    "                             clean_item_title = re.sub(r'\\s*\\.\\s*$', '', clean_item_title).strip()\n",
    "                             clean_item_title = re.sub(r'\\s+', ' ', clean_item_title).strip()\n",
    "                             # Assign a dummy ID if no specific item/part ID is present for sorting later.\n",
    "                             # Or better, keep item_id as None and let create_section_info handle it.\n",
    "                             found_items.append((None, clean_item_title, 'named_section'))\n",
    "                             break # Found a match for this line, move to next line\n",
    "\n",
    "    # Refined deduplication and final DocumentSection creation\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    # Process found_items to clean and add part context\n",
    "    processed_items = []\n",
    "    current_part_for_items = None\n",
    "    for item_id, title, section_type_raw in found_items:\n",
    "        if section_type_raw == 'part':\n",
    "            current_part_for_items = f\"PART {item_id}\"\n",
    "            processed_items.append((item_id, title, 'part', current_part_for_items))\n",
    "        elif section_type_raw == 'item':\n",
    "            processed_items.append((item_id, title, 'item', current_part_for_items))\n",
    "        else: # named_section or unknown type\n",
    "            processed_items.append((item_id, title, section_type_raw, current_part_for_items))\n",
    "\n",
    "    # Sort processed items to handle nesting logic: parts first, then items within parts\n",
    "    # Sort key: (is_part, part_roman_value, item_num_value, item_alpha_value, title)\n",
    "    # This is a bit complex for a simple sort, so a robust sort might be iterative or based on final DocumentSection structure.\n",
    "    # For now, let's rely on the deduplication for unique_items below.\n",
    "\n",
    "    for item_id, title, section_type_raw, part_context in processed_items:\n",
    "        # Create a key that is robust for deduplication\n",
    "        key = (item_id, title, section_type_raw, part_context)\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=title,\n",
    "                content=\"\", # Content still empty, to be filled by main strategy\n",
    "                section_type=section_type_raw,\n",
    "                item_number=item_id if section_type_raw == 'item' else None,\n",
    "                part=part_context if section_type_raw == 'item' else (f\"PART {item_id}\" if section_type_raw == 'part' else None),\n",
    "                start_pos=0, # Not set from TOC\n",
    "                end_pos=0    # Not set from TOC\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    # Sort the final unique_items list. This is crucial for content mapping later.\n",
    "    # A simple approach: sort by Item/Part ID, then by title.\n",
    "    unique_items.sort(key=lambda x: (x.part if x.part else '', x.item_number if x.item_number else '', x.title))\n",
    "\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:10]):\n",
    "        logger.info(f\"  ‚Ä¢ {sec.item_number if sec.item_number else sec.part if sec.part else 'NoID'}: {sec.title[:50]}...\")\n",
    "\n",
    "    return unique_items # Return DocumentSection objects directly\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # Create flexible regex for the title/item number to find it in the main content\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers as they are more consistent\n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part:\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")))\n",
    "            \n",
    "            # Add the full cleaned title as a fallback/alternative match\n",
    "            if toc_entry.title:\n",
    "                # Clean title for regex matching in content (remove page numbers, excess pipes, etc.)\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip() # Remove \"| PageNumber\"\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip() # Remove trailing periods\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex) # Replace multiple spaces with \\s+ for flexible matching\n",
    "                pattern_parts.append(re.escape(cleaned_title_for_regex)) # re.escape the cleaned title\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    if next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")))\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part, # Preserve the part info derived from TOC\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a114a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: bad character range \\s-, at position 17\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: bad character range \\s-, at position 17\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: bad character range \\s-, at position 17\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "ERROR:__main__:Error processing processed_filings/KO/KO_10Q_2020-07-22.txt: bad character range \\s-, at position 17\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections |   0 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections |   0 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections |   0 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections |   0 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +0 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba16cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives (e.g., content that looks like a header but isn't)\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., just \"1.\")\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or # Exclude table markers if not part of a valid section header\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line if specific extraction fails\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Determine if the first captured group is a valid Item/Part ID\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        # Clean up title: remove trailing table markers like \"[TABLE_END]\" if they were captured\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip() # Remove pipe characters\n",
    "                    else: # No explicit title captured by a group\n",
    "                        # Try to extract a clean title from the remainder of the line after the ID\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line # Fallback to full line if cleaning is problematic\n",
    "                else: # First captured group was not a standard Item/Part ID, treat as part of title\n",
    "                    section_title = full_line\n",
    "                    # For generic named sections (e.g., \"BUSINESS\"), assign a canonical ID if not part of an Item/Part already\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "                    # Add other named section mappings if needed.\n",
    "\n",
    "            # Store the original start/end of the line for correct content extraction\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns early in the list)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or less specific types\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If both are specific, prefer the one matched by a higher-priority pattern (lower index means earlier in list)\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                # If they have the same ID but the new match offers a cleaner/more robust title\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8: # Heuristic for \"cleaner\"\n",
    "                     final_matches[-1] = current_match\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title: remove \"PART X\" if it's already in the title to avoid redundancy.\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part # Fallback to just \"PART X\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item (inherited)\n",
    "            # Refine title: remove \"Item X\" if it's already in the title\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\" # Fallback to just \"Item X\"\n",
    "        # For named_section (e.g., \"BUSINESS\" when it's not explicitly an Item number)\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # CORRECTED: Significant refinement here. Focused on capturing clean IDs and titles.\n",
    "    # Added more specific patterns to handle the multi-column and sub-section structures.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Page | PART/ITEM | Item_ID. | Title | Page_Num (KO style)\n",
    "        # Captures Part ID (Group 1), Part Title (Group 2), Item ID (Group 3), Item Title (Group 4)\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: PART/ITEM | Title | Page_Num (AMZN style, or simpler)\n",
    "        # Captures Item/Part ID (Group 1), Title (Group 2). Catches \"Item 1. | Financial Statements | 3\" or \"PART I. FINANCIAL INFORMATION | 3\"\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part ID then Title (e.g., \"Item 1A. Risk Factors\" or \"PART II. OTHER INFORMATION\")\n",
    "        # Captures Item/Part ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: TOC lines that are just titles, potentially indented, often sub-sections\n",
    "        # These don't have Item/Part numbers explicitly. Captures Title (Group 1).\n",
    "        # Filters by minimum length to avoid capturing noise like empty lines or short numbers.\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',-]{10,})\\s*(?:\\|\\s*\\d+)?$', re.M), # Title must start with capital letter, be at least 10 chars, allow numbers/symbols\n",
    "        \n",
    "        # Pattern 5: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Captures Item ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None # To associate items with the last seen part\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown' # Default type\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Page | PART/ITEM | Item_ID. | Title | Page_Num\n",
    "                        part_id_cand = match.group(1) if match.group(1) else None\n",
    "                        item_id = match.group(3).strip() if match.group(3) else None # Item ID is group 3\n",
    "                        item_title = match.group(4).strip() if match.group(4) else \"\" # Item Title is group 4\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand}\"\n",
    "                            found_items.append((part_id_cand, match.group(2).strip(), 'part', current_part_id_context)) # Add the PART entry\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break # Move to next line\n",
    "\n",
    "                    elif pattern == item_patterns[1] or pattern == item_patterns[2] or pattern == item_patterns[4]: # Patterns with ID as group 1, Title as group 2\n",
    "                        item_id = match.group(1).strip()\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "\n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4 from above)\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d', item_title): # Ensure not just a number/symbol\n",
    "                             # Assign a None ID, it's a named sub-section\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break # Move to next line\n",
    "            \n",
    "    # Refined deduplication and final DocumentSection creation\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    # Process found_items to clean and add part context\n",
    "    processed_items_with_parts = []\n",
    "    \n",
    "    # Re-apply current_part_id_context correctly after initial parsing (cleaner way)\n",
    "    temp_sections = []\n",
    "    temp_current_part = None\n",
    "    for item_id, title_raw, section_type_raw, _ in found_items:\n",
    "        if section_type_raw == 'part':\n",
    "            temp_current_part = f\"PART {item_id}\"\n",
    "            temp_sections.append({'item_id': item_id, 'title': title_raw, 'type': 'part', 'part': temp_current_part})\n",
    "        elif section_type_raw == 'item':\n",
    "            temp_sections.append({'item_id': item_id, 'title': title_raw, 'type': 'item', 'part': temp_current_part})\n",
    "        else: # named_section or unknown type from TOC\n",
    "            temp_sections.append({'item_id': item_id, 'title': title_raw, 'type': 'named_section', 'part': temp_current_part})\n",
    "\n",
    "\n",
    "    # Deduplicate and create final DocumentSection objects.\n",
    "    # Sort by parts and items for logical ordering.\n",
    "    temp_sections.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in temp_sections:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:10]):\n",
    "        logger.info(f\"  ‚Ä¢ {sec.item_number if sec.item_number else sec.part if sec.part else 'NoID'}: {sec.title[:50]}...\")\n",
    "\n",
    "    return unique_items # Return DocumentSection objects directly\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # Sort TOC entries to ensure correct order for content extraction\n",
    "        # This sorting is already done in detect_sections_from_toc_universal before returning.\n",
    "        # So toc_entries should already be sorted.\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            # Create highly flexible regex for matching TOC entry in main content\n",
    "            # Account for variations in whitespace, periods, and potential parenthetical additions\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers if they exist\n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?\\s*(?:[A-Z][a-z0-9\\s,\\'()-]*)*') # \"Item 1.\" or \"Item 1A\" with potential title after it\n",
    "            elif toc_entry.part:\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?(?:\\s*[-‚Äì‚Äî]?\\s*[A-Z][a-z0-9\\s,\\'()-]*)*') # \"PART I.\" or \"PART I - TITLE\"\n",
    "            \n",
    "            # Fallback to matching the cleaned title from TOC\n",
    "            if toc_entry.title:\n",
    "                # Clean title for regex matching in content (remove page numbers, excess pipes, etc.)\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip() # Remove \"| PageNumber\"\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip() # Remove trailing periods\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex) # Replace multiple spaces with \\s+ for flexible matching\n",
    "                pattern_parts.append(re.escape(cleaned_title_for_regex)) # re.escape the cleaned title\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            # Make it look for these patterns at the beginning of a line, allowing some leading whitespace\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f70178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 2 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ PART I: I...\n",
      "INFO:__main__:  ‚Ä¢ PART PART I | : I...\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 2 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ FINANCIAL INFORMATION: 1...\n",
      "INFO:__main__:  ‚Ä¢ PART PART I. FINANCIAL INFORMATION | : I...\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 22 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - PART I...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - PART II...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Item/Part unknown - Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  14: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  15: Item/Part III - PART III...\n",
      "INFO:__main__:Strategy 1 successful: Found 22 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 22 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +-3 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. PART I\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. PART II\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Notes to Consolidated Financial Statements\n",
      "\n",
      "  12. Opinion on the Financial Statements\n",
      "\n",
      "  13. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  14. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  15. PART III\n",
      "\n",
      "  16. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  17. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  18. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  19. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  20. PART IV\n",
      "\n",
      "  21. (1) All financial statements\n",
      "\n",
      "  22. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03a3540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns: e.g., \"[TABLE_START] Item 1. | Business...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns: e.g., \"[TABLE_START] PART I | FINANCIAL INFORMATION...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line): e.g., \"Item 1. Business\"\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped): e.g., \"Item 1. | Business\"\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line): e.g., \"PART I. FINANCIAL INFORMATION\"\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated): e.g., \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\", usually at start of line)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g., \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers (e.g., \"BUSINESS\", \"RISK FACTORS\")\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives (e.g., content that looks like a header but isn't)\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., just \"1.\")\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or # Exclude table markers if not part of a valid section header\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line if specific extraction fails\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Determine if the first captured group is a valid Item/Part ID\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        # Clean up title: remove trailing table markers like \"[TABLE_END]\" if they were captured\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip() # Remove pipe characters\n",
    "                    else: # No explicit title captured by a group\n",
    "                        # Try to extract a clean title from the remainder of the line after the ID\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line # Fallback to full line if cleaning is problematic\n",
    "                else: # First captured group was not a standard Item/Part ID, treat as part of title\n",
    "                    section_title = full_line\n",
    "                    # For generic named sections (e.g., \"BUSINESS\"), assign a canonical ID if not part of an Item/Part already\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "                    # Add other named section mappings if needed.\n",
    "\n",
    "            # Store the actual start and end positions of the matched content within the document\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(), # Use match.start() for the *exact* start of the regex match\n",
    "                'end_pos': match.end(),     # Use match.end() for the *exact* end of the regex match\n",
    "                'full_line': full_line, # Keep for debugging/context\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns early in the list)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or less specific types\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If both are specific, prefer the one matched by a higher-priority pattern (lower index means earlier in list)\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                # If they have the same ID but the new match offers a cleaner/more robust title\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8: # Heuristic for \"cleaner\"\n",
    "                     final_matches[-1] = current_match\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # The content for this section goes from its start_pos to the start_pos of the *next* matched section\n",
    "        # or to the end of the entire document if it's the last section.\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        # CRITICAL FIX: Ensure section_content is correctly sliced from the original content\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title: remove \"PART X\" if it's already in the title to avoid redundancy.\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part # Fallback to just \"PART X\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item (inherited from last PART)\n",
    "            # Refine title: remove \"Item X\" if it's already in the title\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\" # Fallback to just \"Item X\"\n",
    "        # For named_section (e.g., \"BUSINESS\" when it's not explicitly an Item number)\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content, # Pass the correctly sliced content\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC.\n",
    "    # CORRECTED: Significant refinement here. Focused on capturing clean IDs and titles.\n",
    "    # Added more specific patterns to handle multi-column and sub-section structures.\n",
    "    # Added stricter checks to avoid capturing noise.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Captures: (Optional Page Num) | PART ID | PART Title (Optional) | Item ID | Item Title (Optional) | Page Num\n",
    "        # Group 1: PART ID (e.g., 'I'), Group 2: PART Title, Group 3: Item ID, Group 4: Item Title\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(?:PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title (e.g., \"Item 1. | Financial Statements | 3\")\n",
    "        # Captures: Item/PART ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (e.g., \"Item 1A. Risk Factors\" or \"PART II. OTHER INFORMATION\")\n",
    "        # Captures: Item/PART ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions. Must be long enough to avoid noise.\n",
    "        # Captures: Title (Group 1). Requires a reasonable length and start with a capital letter.\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)-]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M), # Min 15 chars, allow more symbols\n",
    "\n",
    "        # Pattern 5: Simple \"PART X\" line (e.g., \"PART I\")\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Captures Item ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None # To associate items with the last seen part\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip lines that are likely just TOC headers/footers or page numbers\n",
    "            if any(kw in line.lower() for kw in ['page', 'item', 'part', 'description']) and len(line) < 20: # Short headers/footers\n",
    "                continue\n",
    "            if re.match(r'^\\d+$', line.strip()): # Just a page number\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()): # Just \"1\" or \"1A\"\n",
    "                continue\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column (Page | PART/ITEM | Item_ID. | Title)\n",
    "                        part_id_cand = match.group(2) if match.group(2) else None # Group 2 for PART ID\n",
    "                        item_id = match.group(3).strip() if match.group(3) else None # Group 3 for Item ID\n",
    "                        item_title = match.group(4).strip() if match.group(4) else \"\" # Group 4 for Item Title\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand}\"\n",
    "                            found_items.append((part_id_cand, match.group(1).strip() if match.group(1) else f\"PART {part_id_cand}\", 'part', current_part_id_context)) # Add the PART entry\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break # Move to next line (matched)\n",
    "\n",
    "                    elif pattern == item_patterns[1] or pattern == item_patterns[2] or pattern == item_patterns[5]: # Item/PART | Title | Page, or Item/PART. Title, or number. Title\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4 from above)\n",
    "                        item_title = match.group(1).strip()\n",
    "                        # Add a sanity check for extracted titles (e.g., not just numbers or very short)\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()): # Not purely numeric\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5 from above)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    # Deduplicate and create final DocumentSection objects.\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    # Process found_items to ensure correct part context is applied and clean titles\n",
    "    final_processed_items = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        # Clean title to remove trailing page numbers or extraneous characters often seen in TOCs\n",
    "        cleaned_title = re.sub(r'\\s*\\|\\s*\\d+\\s*$', '', title_raw).strip() # Remove \"| PageNum\" from end\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip() # Remove trailing periods\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip() # Remove table end markers\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip() # Normalize internal whitespace\n",
    "\n",
    "        # Filter out titles that are just page numbers or very short/uninformative\n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        # Create a unique key for deduplication. Use a combination of ID, clean title, type, and part context.\n",
    "        key = (item_id, cleaned_title, section_type_raw, part_context)\n",
    "        if key not in seen_keys:\n",
    "            # Create a DocumentSection. content is left empty to be filled later by content mapping.\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=cleaned_title,\n",
    "                content=\"\",\n",
    "                section_type=section_type_raw,\n",
    "                item_number=item_id if section_type_raw == 'item' else None,\n",
    "                part=part_context if section_type_raw == 'item' else (f\"PART {item_id}\" if section_type_raw == 'part' else None), # Store the actual part string\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    # Sort the final unique_items list. This is crucial for content mapping later.\n",
    "    # Sort by part (alphabetical, which works for Roman numerals if prefixed \"PART \"), then by item number (numeric), then by title.\n",
    "    def sort_key_for_doc_section(doc_sec):\n",
    "        part_sort_val = doc_sec.part if doc_sec.part else ''\n",
    "        item_num_sort_val = ''\n",
    "        if doc_sec.item_number:\n",
    "            match_num_alpha = re.match(r'(\\d+)([A-C]?)', doc_sec.item_number)\n",
    "            if match_num_alpha:\n",
    "                item_num_sort_val = (int(match_num_alpha.group(1)), match_num_alpha.group(2))\n",
    "            else: # Fallback for non-standard item numbers\n",
    "                item_num_sort_val = (float('inf'), doc_sec.item_number)\n",
    "        return (part_sort_val, item_num_sort_val, doc_sec.title)\n",
    "\n",
    "    unique_items.sort(key=sort_key_for_doc_section)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]): # Show more for debugging TOC\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # TOC entries are already sorted by `detect_sections_from_toc_universal`\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            # Create highly flexible regex for matching TOC entry in main content\n",
    "            # Account for variations in whitespace, periods, and potential parenthetical additions\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers if they exist\n",
    "            if toc_entry.item_number:\n",
    "                # Be flexible: \"Item 1.\", \"Item 1A\", \"ITEM 1\" etc.\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part:\n",
    "                # Be flexible: \"PART I\", \"PART II.\" etc.\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            # Fallback to matching the full cleaned title from TOC\n",
    "            if toc_entry.title:\n",
    "                # Clean title for regex matching in content (remove page numbers, excess pipes, etc.)\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex) # Handle hyphens in titles\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex) # Replace multiple spaces with \\s+\n",
    "                \n",
    "                # Add word boundaries (\\b) to prevent partial word matches, but be careful with punctuation.\n",
    "                # A balance is needed here. For now, rely on careful stripping and flexible whitespace.\n",
    "                pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            # Match at the beginning of a line, allowing leading whitespace.\n",
    "            # Use non-capturing groups (?:...) where applicable.\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3: # Only consider TOC mapping successful if it yields a good number of sections\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad26800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da402b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 22 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - PART I...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - PART II...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Item/Part unknown - Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  14: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  15: Item/Part III - PART III...\n",
      "INFO:__main__:Strategy 1 successful: Found 22 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 22 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +-3 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. PART I\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. PART II\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Notes to Consolidated Financial Statements\n",
      "\n",
      "  12. Opinion on the Financial Statements\n",
      "\n",
      "  13. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  14. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  15. PART III\n",
      "\n",
      "  16. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  17. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  18. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  19. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  20. PART IV\n",
      "\n",
      "  21. (1) All financial statements\n",
      "\n",
      "  22. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9246cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns: e.g., \"[TABLE_START] Item 1. | Business...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns: e.g., \"[TABLE_START] PART I | FINANCIAL INFORMATION...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line): e.g., \"Item 1. Business\"\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped): e.g., \"Item 1. | Business\"\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line): e.g., \"PART I. FINANCIAL INFORMATION\"\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated): e.g., \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\", usually at start of line)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g., \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers (e.g., \"BUSINESS\", \"RISK FACTORS\")\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives (e.g., content that looks like a header but isn't)\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., just \"1.\")\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or # Exclude table markers if not part of a valid section header\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line if specific extraction fails\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Determine if the first captured group is a valid Item/Part ID\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        # Clean up title: remove trailing table markers like \"[TABLE_END]\" if they were captured\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip() # Remove pipe characters\n",
    "                    else: # No explicit title captured by a group\n",
    "                        # Try to extract a clean title from the remainder of the line after the ID\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line # Fallback to full line if cleaning is problematic\n",
    "                else: # First captured group was not a standard Item/Part ID, treat as part of title\n",
    "                    section_title = full_line\n",
    "                    # For generic named sections (e.g., \"BUSINESS\"), assign a canonical ID if not part of an Item/Part already\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "                    # Add other named section mappings if needed.\n",
    "\n",
    "            # Store the actual start and end positions of the matched content within the document\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(), # Use match.start() for the *exact* start of the regex match\n",
    "                'end_pos': match.end(),     # Use match.end() for the *exact* end of the regex match\n",
    "                'full_line': full_line, # Keep for debugging/context\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns early in the list)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or less specific types\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If both are specific, prefer the one matched by a higher-priority pattern (lower index means earlier in list)\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                # If they have the same ID but the new match offers a cleaner/more robust title\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8: # Heuristic for \"cleaner\"\n",
    "                     final_matches[-1] = current_match\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # The content for this section goes from its start_pos to the start_pos of the *next* matched section\n",
    "        # or to the end of the entire document if it's the last section.\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        # CRITICAL FIX: Ensure section_content is correctly sliced from the original content\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title: remove \"PART X\" if it's already in the title to avoid redundancy.\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part # Fallback to just \"PART X\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item (inherited from last PART)\n",
    "            # Refine title: remove \"Item X\" if it's already in the title\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\" # Fallback to just \"Item X\"\n",
    "        # For named_section (e.g., \"BUSINESS\" when it's not explicitly an Item number)\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content, # Pass the correctly sliced content\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC.\n",
    "    # CORRECTED: Significant refinement here. Focused on capturing clean IDs and titles.\n",
    "    # Added more specific patterns to handle multi-column and sub-section structures.\n",
    "    # Added stricter checks to avoid capturing noise.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Captures: (Optional Page Num) | PART ID | PART Title (Optional) | Item ID | Item Title (Optional) | Page Num\n",
    "        # Group 1: PART ID, Group 2: PART Title, Group 3: Item ID, Group 4: Item Title\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title (e.g., \"Item 1. | Financial Statements | 3\")\n",
    "        # Captures: Item/PART ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (e.g., \"Item 1A. Risk Factors\" or \"PART II. OTHER INFORMATION\")\n",
    "        # Captures: Item/PART ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions. Must be long enough to avoid noise.\n",
    "        # Captures: Title (Group 1). Requires a reasonable length and start with a capital letter.\n",
    "        # Made more robust to handle various characters and avoid matching short noise.\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M), # Min 15 chars, allow more symbols\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line (e.g., \"PART I\")\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Captures Item ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None # To associate items with the last seen part\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip lines that are likely just TOC headers/footers or page numbers or short noisy lines\n",
    "            if any(kw in line.lower() for kw in ['page', 'item', 'part', 'description']) and len(line) < 20:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()): # Just a page number\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()): # Just \"1\" or \"1A\"\n",
    "                continue\n",
    "            if len(line) < 5: # Very short lines are likely noise\n",
    "                continue\n",
    "\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column (Page | PART/ITEM | Item_ID. | Title)\n",
    "                        part_id_cand = match.group(2) if match.group(2) else None # Group 2 for PART ID\n",
    "                        item_id = match.group(3).strip() if match.group(3) else None # Group 3 for Item ID\n",
    "                        item_title = match.group(4).strip() if match.group(4) else \"\" # Group 4 for Item Title\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand}\"\n",
    "                            # Add the PART entry, title might be from group 1 if available and clean, else generic\n",
    "                            part_title_from_group = match.group(1).strip() if match.group(1) else f\"PART {part_id_cand}\"\n",
    "                            found_items.append((part_id_cand, part_title_from_group, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break # Move to next line (matched)\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\" # Title captured by second group\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        # Add sanity checks for extracted titles (e.g., not just numbers or very short)\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()): # Not purely numeric\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    # Deduplicate and create final DocumentSection objects.\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    # Clean titles and associate with parts\n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        # Clean title to remove trailing page numbers or extraneous characters often seen in TOCs\n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip() # Remove \"| PageNum\" from end\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip() # Remove trailing periods\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip() # Remove table end markers\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip() # Normalize internal whitespace\n",
    "        \n",
    "        # Filter out titles that are just numbers or very short/uninformative after cleaning\n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    # Sort and deduplicate\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\", # Content still empty, to be filled by main strategy\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]): # Show more for debugging TOC\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # TOC entries are already sorted by `detect_sections_from_toc_universal`\n",
    "        # and filtered for quality.\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            # Create highly flexible regex for matching TOC entry in main content\n",
    "            # Account for variations in whitespace, periods, and potential parenthetical additions\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers if they exist\n",
    "            if toc_entry.item_number:\n",
    "                # Be flexible: \"Item 1.\", \"Item 1A\", \"ITEM 1\" etc.\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"): # Ensure it's a valid PART string\n",
    "                # Be flexible: \"PART I\", \"PART II.\" etc.\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            # Fallback to matching the full cleaned title from TOC\n",
    "            if toc_entry.title:\n",
    "                # Clean title for regex matching in content (remove page numbers, excess pipes, etc.)\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip() # Remove \"| PageNumber\"\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip() # Remove trailing periods\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex) # Handle hyphens in titles\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex) # Replace multiple spaces with \\s+\n",
    "                \n",
    "                # Add word boundaries (\\b) only if title is not too short, to prevent partial word matches\n",
    "                if len(cleaned_title_for_regex) > 5: # Heuristic: add \\b for longer titles\n",
    "                    pattern_parts.append(r'\\b' + re.escape(cleaned_title_for_regex) + r'\\b')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            # Match at the beginning of a line, allowing leading whitespace.\n",
    "            # Use non-capturing groups (?:...) where applicable.\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b' + re.escape(next_cleaned_title_for_regex) + r'\\b')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3: # Only consider TOC mapping successful if it yields a good number of sections\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ca0725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 1 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ ID: PART I, Type: part, Title: PART I |...\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 1 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ ID: PART I, Type: part, Title: PART I. FINANCIAL INFORMATION |...\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 22 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - PART I...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - PART II...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Item/Part unknown - Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  14: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  15: Item/Part III - PART III...\n",
      "INFO:__main__:Strategy 1 successful: Found 22 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 22 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +-3 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. PART I\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. PART II\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Notes to Consolidated Financial Statements\n",
      "\n",
      "  12. Opinion on the Financial Statements\n",
      "\n",
      "  13. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  14. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  15. PART III\n",
      "\n",
      "  16. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  17. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  18. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  19. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  20. PART IV\n",
      "\n",
      "  21. (1) All financial statements\n",
      "\n",
      "  22. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f921cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using raw strings `r` and explicitly handling whitespace `\\s*` and literal characters.\n",
    "    # Compiling patterns once for efficiency.\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns: e.g., \"[TABLE_START] Item 1. | Business...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns: e.g., \"[TABLE_START] PART I | FINANCIAL INFORMATION...\"\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (strong indicators, start of line): e.g., \"Item 1. Business\"\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        # Standalone ITEM patterns (pipe-separated but not necessarily table-wrapped): e.g., \"Item 1. | Business\"\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (strong indicators, start of line): e.g., \"PART I. FINANCIAL INFORMATION\"\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        # Standalone PART patterns (pipe-separated): e.g., \"PART I | FINANCIAL INFORMATION\"\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-dot format (e.g., \"1. Business\" not necessarily preceded by \"Item\", usually at start of line)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        # Number-only pattern in tables (e.g., \"[TABLE_START] 1. | Business\")\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Generic Section Titles that often appear as headers (e.g., \"BUSINESS\", \"RISK FACTORS\")\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            # Determine content boundaries for the \"line\" containing the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives (e.g., content that looks like a header but isn't)\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short (e.g., just \"1.\")\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or # Exclude table markers if not part of a valid section header\n",
    "                full_line.count(' ') > 20):  # Too many words, likely not a header\n",
    "                continue\n",
    "\n",
    "            # Heuristic to filter out TOC entries that might match general patterns\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line # Default to full line if specific extraction fails\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                # Determine if the first captured group is a valid Item/Part ID\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]: # If a title group was also captured\n",
    "                        section_title = groups[1].strip()\n",
    "                        # Clean up title: remove trailing table markers like \"[TABLE_END]\" if they were captured\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip() # Remove pipe characters\n",
    "                    else: # No explicit title captured by a group\n",
    "                        # Try to extract a clean title from the remainder of the line after the ID\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200: # Ensure extracted title isn't too long\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line # Fallback to full line if cleaning is problematic\n",
    "                else: # First captured group was not a standard Item/Part ID, treat as part of title\n",
    "                    section_title = full_line\n",
    "                    # For generic named sections (e.g., \"BUSINESS\"), assign a canonical ID if not part of an Item/Part already\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "                    # Add other named section mappings if needed.\n",
    "\n",
    "            # Store the actual start and end positions of the matched content within the document\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(), # Use match.start() for the *exact* start of the regex match\n",
    "                'end_pos': match.end(),     # Use match.end() for the *exact* end of the regex match\n",
    "                'full_line': full_line, # Keep for debugging/context\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches primarily by start_pos, secondarily by pattern_idx (to prefer more specific patterns early in the list)\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    # Filter duplicate/overlapping matches. Prioritize more specific patterns (lower pattern_idx).\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # If current match starts very close to the last added match,\n",
    "            # consider if it's a duplicate or a better alternative.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100: # Within 100 chars\n",
    "                # Prefer matches with a specific Item/Part ID over 'unknown' or less specific types\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                # If both are specific, prefer the one matched by a higher-priority pattern (lower index means earlier in list)\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                # If they have the same ID but the new match offers a cleaner/more robust title\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8: # Heuristic for \"cleaner\"\n",
    "                     final_matches[-1] = current_match\n",
    "                # Otherwise, if it's too close and not a better candidate, skip as duplicate\n",
    "            else:\n",
    "                final_matches.append(current_match) # Add if sufficiently far apart\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # The content for this section goes from its start_pos to the start_pos of the *next* matched section\n",
    "        # or to the end of the entire document if it's the last section.\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        # CRITICAL FIX: Ensure section_content is correctly sliced from the original content\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part for subsequent items\n",
    "            # Refine title: remove \"PART X\" if it's already in the title to avoid redundancy.\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part # Fallback to just \"PART X\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Assign current part context to this item (inherited from last PART)\n",
    "            # Refine title: remove \"Item X\" if it's already in the title\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\" # Fallback to just \"Item X\"\n",
    "        # For named_section (e.g., \"BUSINESS\" when it's not explicitly an Item number)\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content, # Pass the correctly sliced content\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info (either detected directly or inherited)\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns. Using re.escape for literal parts.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC.\n",
    "    # CORRECTED: Significant refinement here. Focused on capturing clean IDs and titles.\n",
    "    # Added more specific patterns to handle multi-column and sub-section structures.\n",
    "    # Added stricter checks to avoid capturing noise.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Captures: (Optional Page Num) | PART ID | PART Title (Optional) | Item ID | Item Title (Optional) | Page Num\n",
    "        # Group 1: PART ID, Group 2: PART Title, Group 3: Item ID, Group 4: Item Title\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title (e.g., \"Item 1. | Financial Statements | 3\")\n",
    "        # Captures: Item/PART ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (e.g., \"Item 1A. Risk Factors\" or \"PART II. OTHER INFORMATION\")\n",
    "        # Captures: Item/PART ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions. Must be long enough to avoid noise.\n",
    "        # Captures: Title (Group 1). Requires a reasonable length and start with a capital letter.\n",
    "        # Made more robust to handle various characters and avoid matching short noise.\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M), # Min 15 chars, allow more symbols\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line (e.g., \"PART I\")\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Captures Item ID (Group 1), Title (Group 2)\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None # To associate items with the last seen part\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip lines that are likely just TOC headers/footers or page numbers or short noisy lines\n",
    "            if any(kw in line.lower() for kw in ['page', 'item', 'part', 'description']) and len(line) < 20:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()): # Just a page number\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()): # Just \"1\" or \"1A\"\n",
    "                continue\n",
    "            if len(line) < 5: # Very short lines are likely noise\n",
    "                continue\n",
    "\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column (Page | PART/ITEM | Item_ID. | Title)\n",
    "                        part_id_cand = match.group(2) if match.group(2) else None # Group 2 for PART ID\n",
    "                        item_id = match.group(3).strip() if match.group(3) else None # Group 3 for Item ID\n",
    "                        item_title = match.group(4).strip() if match.group(4) else \"\" # Group 4 for Item Title\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand}\"\n",
    "                            # Add the PART entry, title might be from group 1 if available and clean, else generic\n",
    "                            part_title_from_group = match.group(1).strip() if match.group(1) else f\"PART {part_id_cand}\"\n",
    "                            found_items.append((part_id_cand, part_title_from_group, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break # Move to next line (matched)\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        # Add sanity checks for extracted titles (e.g., not just numbers or very short)\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()): # Not purely numeric\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    # Deduplicate and create final DocumentSection objects.\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    # Process found_items to ensure correct part context is applied and clean titles\n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        # Clean title to remove trailing page numbers or extraneous characters often seen in TOCs\n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip() # Remove \"| PageNum\" from end\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip() # Remove trailing periods\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip() # Remove table end markers\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip() # Normalize internal whitespace\n",
    "        \n",
    "        # Filter out titles that are just numbers or very short/uninformative after cleaning\n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    # Sort and deduplicate\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\", # Content still empty, to be filled by main strategy\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]): # Show more for debugging TOC\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # TOC entries are already sorted by `detect_sections_from_toc_universal`\n",
    "        # and filtered for quality.\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            # Create highly flexible regex for matching TOC entry in main content\n",
    "            # Account for variations in whitespace, periods, and potential parenthetical additions\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers if they exist\n",
    "            if toc_entry.item_number:\n",
    "                # Be flexible: \"Item 1.\", \"Item 1A\", \"ITEM 1\" etc.\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"): # Ensure it's a valid PART string\n",
    "                # Be flexible: \"PART I\", \"PART II.\" etc.\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            # Fallback to matching the full cleaned title from TOC\n",
    "            if toc_entry.title:\n",
    "                # Clean title for regex matching in content (remove page numbers, excess pipes, etc.)\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip() # Remove \"| PageNumber\"\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip() # Remove trailing periods\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex) # Handle hyphens in titles\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex) # Replace multiple spaces with \\s+\n",
    "                \n",
    "                # Add word boundaries (\\b) only if title is not too short, to prevent partial word matches\n",
    "                # Made word boundaries optional for more flexibility as section titles may not strictly start/end on word boundaries\n",
    "                if len(cleaned_title_for_regex) > 5: # Heuristic: add \\b for longer titles\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?') # Optional word boundaries\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Combine all potential ways to match this section's header\n",
    "            # Match at the beginning of a line, allowing leading whitespace.\n",
    "            # Use non-capturing groups (?:...) where applicable.\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries): # Check the next entry in the *sorted* list\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3: # Only consider TOC mapping successful if it yields a good number of sections\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0effa8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 1 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ ID: PART I, Type: part, Title: PART I |...\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 1 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ ID: PART I, Type: part, Title: PART I. FINANCIAL INFORMATION |...\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 22 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - PART I...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part II - PART II...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Item/Part unknown - Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  14: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  15: Item/Part III - PART III...\n",
      "INFO:__main__:Strategy 1 successful: Found 22 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 22 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +-3 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. PART I\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. PART II\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Notes to Consolidated Financial Statements\n",
      "\n",
      "  12. Opinion on the Financial Statements\n",
      "\n",
      "  13. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  14. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  15. PART III\n",
      "\n",
      "  16. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  17. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  18. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  19. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  20. PART IV\n",
      "\n",
      "  21. (1) All financial statements\n",
      "\n",
      "  22. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb282335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        # escape hyphen-minus at end of class so it's not seen as a range\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0, # These positions are relative to the 'page' or current_section, not whole document\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content)\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else: # Fallback if part not explicitly set for 10Q item\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        # If it's a PART section itself, format it.\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    # Fallback for named_section, content, or document type sections\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Improved regex patterns for better capture of Item/Part numbers and titles.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    final_document_sections = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content, # Pass the correctly sliced content\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q). Very specific.\n",
    "        # Group 1: Optional Page Num, Group 2: PART ID, Group 3: PART Title, Group 4: Item ID, Group 5: Item Title\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title, pipe-separated. Catches \"Item 1. | Financial Statements | 3\"\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (no pipes separating title)\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions. Must be long enough, starts with capital.\n",
    "        # Group 1: Title\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M),\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line\n",
    "        # Group 1: PART ID\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Group 1: Item ID, Group 2: Title\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Strict filtering of TOC lines to remove noise\n",
    "            if any(kw in line.lower() for kw in ['page', 'description', 'signatures']) and len(line) < 30:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()): # Just a page number\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()): # Just \"1\" or \"1A\"\n",
    "                continue\n",
    "            if len(line) < 5: # Very short lines\n",
    "                continue\n",
    "            if 'total' in line.lower() and re.search(r'\\d', line): # Lines with numbers that look like financial totals\n",
    "                continue\n",
    "\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "                    \n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column TOC\n",
    "                        # The groups need careful mapping based on the regex\n",
    "                        # (?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?\n",
    "                        # Match.groups() will return a tuple containing all captured groups in order,\n",
    "                        # including None for optional groups that didn't match.\n",
    "                        # Need to adjust group indices based on how pattern is built.\n",
    "                        \n",
    "                        # The regex provided for Pattern 1 has 5 capturing groups:\n",
    "                        # G1: PART\\s*([IVX]+) -> ([IVX]+) -> part_id_cand\n",
    "                        # G2: (\\s*([^\\n|]+?)) -> ([^\\n|]+?) -> part_title_from_group\n",
    "                        # G3: (\\d{1,2}[A-C]?) -> item_id\n",
    "                        # G4: ([^|]+?) -> item_title\n",
    "                        \n",
    "                        # Let's adjust group access based on actual match.groups() output\n",
    "                        # (None, 'I', 'FINANCIAL INFORMATION', '1', 'Financial Statements', None) for AMZN.\n",
    "                        # Or (None, None, None, '1', 'Business', None) for some K.\n",
    "                        \n",
    "                        # Refined group assignment\n",
    "                        part_id_idx = match.re.groupindex.get('part_id_g', 0) # Use groupindex if named groups\n",
    "                        part_title_idx = match.re.groupindex.get('part_title_g', 0)\n",
    "                        item_id_idx = match.re.groupindex.get('item_id_g', 0)\n",
    "                        item_title_idx = match.re.groupindex.get('item_title_g', 0)\n",
    "\n",
    "                        # Simpler direct access based on fixed order of groups as provided\n",
    "                        part_id_cand = match.group(2) if len(groups) >= 2 else None\n",
    "                        part_title_from_group = match.group(3) if len(groups) >= 3 else None\n",
    "                        item_id = match.group(4) if len(groups) >= 4 else None\n",
    "                        item_title = match.group(5) if len(groups) >= 5 else None\n",
    "\n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand.strip()}\"\n",
    "                            title_for_part = part_title_from_group.strip() if part_title_from_group else f\"PART {part_id_cand.strip()}\"\n",
    "                            found_items.append((part_id_cand.strip(), title_for_part, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            title_for_item = item_title.strip() if item_title else f\"Item {item_id.strip()}\"\n",
    "                            found_items.append((item_id.strip(), title_for_item, section_type_raw, current_part_id_context))\n",
    "                            break # Move to next line (matched)\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()):\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip()\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip()\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip()\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip()\n",
    "        \n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]):\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        # TOC entries are already sorted by `detect_sections_from_toc_universal`\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"):\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex)\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex)\n",
    "                \n",
    "                if len(cleaned_title_for_regex) > 5:\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a9013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections |   0 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections |   0 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unique_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m results_universal = test_universal_detection_fixed()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m old_vs_new_sections = \u001b[43mcompare_old_vs_universal_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1181\u001b[39m, in \u001b[36mcompare_old_vs_universal_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1178\u001b[39m     content = f.read()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning old detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m old_sections = \u001b[43mdetect_sections_robust_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning universal detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1184\u001b[39m new_sections = detect_sections_robust_universal(content)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 340\u001b[39m, in \u001b[36mdetect_sections_robust_old\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03mMulti-strategy section detection with fallbacks (original version)\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAttempting Strategy 1: Regex-based section detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sections) >= \u001b[32m3\u001b[39m:\n\u001b[32m    343\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 successful: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    233\u001b[39m             final_matches.append(current_match)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Improved detection found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43munique_matches\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m potential sections:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_matches[:\u001b[32m15\u001b[39m]):\n\u001b[32m    237\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch[\u001b[33m'\u001b[39m\u001b[33mfull_line\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m80\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'unique_matches' is not defined"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35e201b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections: List[DocumentSection] = []\n",
    "\n",
    "    patterns = [\n",
    "        # PART patterns\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns (hyphens escaped at end of class)\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Named sections\n",
    "        re.compile(r'^.{0,50}\\b(BUSINESS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(RISK FACTORS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(LEGAL PROCEEDINGS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(FINANCIAL STATEMENTS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(MANAGEMENT\\.S DISCUSSION)\\b', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(PROPERTIES)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(CONTROLS AND PROCEDURES)\\b\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for idx, pattern in enumerate(patterns):\n",
    "        for m in pattern.finditer(content):\n",
    "            # extract full line\n",
    "            line_start = content.rfind('\\n', 0, m.start()) + 1\n",
    "            line_end = content.find('\\n', m.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # filter out obvious false positives\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "            if any(tok in full_line.lower() for tok in ['table of contents', 'index']):\n",
    "                continue\n",
    "\n",
    "            groups = m.groups()\n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            if groups:\n",
    "                first = groups[0].strip()\n",
    "                # item vs part\n",
    "                if re.match(r'^\\d+[A-C]?$', first, re.I):\n",
    "                    section_id = first.upper()\n",
    "                elif re.match(r'^[IVX]+$', first, re.I):\n",
    "                    section_id = first.upper()\n",
    "\n",
    "                # if there's a second group (named pattern), use it\n",
    "                if len(groups) > 1 and groups[1]:\n",
    "                    title = groups[1].strip()\n",
    "                    title = re.sub(r'\\[TABLE_END\\].*$', '', title, flags=re.I).replace('|', '').strip()\n",
    "                    if title:\n",
    "                        section_title = title\n",
    "                else:\n",
    "                    # try to parse remainder of the line as title\n",
    "                    rem = full_line[m.end() - line_start :].lstrip(\" .‚Äì‚Äî-\").strip()\n",
    "                    if 0 < len(rem) < 200:\n",
    "                        section_title = rem\n",
    "\n",
    "            # fallback canonical IDs for pure-named sections\n",
    "            if not section_id:\n",
    "                up = full_line.upper()\n",
    "                if 'BUSINESS' in up:\n",
    "                    section_id = '1'\n",
    "                elif 'RISK FACTORS' in up:\n",
    "                    section_id = '1A'\n",
    "                elif 'LEGAL PROCEEDINGS' in up:\n",
    "                    section_id = '3'\n",
    "                # add others if needed...\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'section_id': section_id or 'UNKNOWN',\n",
    "                'section_title': section_title,\n",
    "            })\n",
    "\n",
    "    # sort and dedupe by start_pos\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "    unique = []\n",
    "    seen_starts = set()\n",
    "    for m in all_matches:\n",
    "        if m['start_pos'] not in seen_starts:\n",
    "            seen_starts.add(m['start_pos'])\n",
    "            unique.append(m)\n",
    "\n",
    "    # build DocumentSection list\n",
    "    for i, m in enumerate(unique):\n",
    "        start = m['start_pos']\n",
    "        end = unique[i+1]['start_pos'] if i+1 < len(unique) else len(content)\n",
    "        sections.append(DocumentSection(\n",
    "            id=m['section_id'],\n",
    "            title=m['section_title'],\n",
    "            start_char=start,\n",
    "            end_char=end\n",
    "        ))\n",
    "\n",
    "    logger.info(f\"Strategy 1 found {len(sections)} sections\")\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ea7b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections |   0 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections |   0 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DocumentSection.__init__() got an unexpected keyword argument 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m results_universal = test_universal_detection_fixed()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m old_vs_new_sections = \u001b[43mcompare_old_vs_universal_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1181\u001b[39m, in \u001b[36mcompare_old_vs_universal_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1178\u001b[39m     content = f.read()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning old detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m old_sections = \u001b[43mdetect_sections_robust_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning universal detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1184\u001b[39m new_sections = detect_sections_robust_universal(content)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 340\u001b[39m, in \u001b[36mdetect_sections_robust_old\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03mMulti-strategy section detection with fallbacks (original version)\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAttempting Strategy 1: Regex-based section detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sections) >= \u001b[32m3\u001b[39m:\n\u001b[32m    343\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 successful: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    104\u001b[39m     start = m[\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    105\u001b[39m     end = unique[i+\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(unique) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sections.append(\u001b[43mDocumentSection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_title\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    113\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sections\n",
      "\u001b[31mTypeError\u001b[39m: DocumentSection.__init__() got an unexpected keyword argument 'id'"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d593502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections: List[DocumentSection] = []\n",
    "\n",
    "    patterns = [\n",
    "        # PART patterns\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Named sections\n",
    "        re.compile(r'^.{0,50}\\b(BUSINESS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(RISK FACTORS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(LEGAL PROCEEDINGS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(FINANCIAL STATEMENTS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(MANAGEMENT\\.S DISCUSSION)\\b', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(PROPERTIES)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(CONTROLS AND PROCEDURES)\\b\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "    for idx, pattern in enumerate(patterns):\n",
    "        for m in pattern.finditer(content):\n",
    "            # grab the full line\n",
    "            line_start = content.rfind('\\n', 0, m.start()) + 1\n",
    "            line_end = content.find('\\n', m.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "            # filters\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "            if any(tok in full_line.lower() for tok in ['table of contents', 'index']):\n",
    "                continue\n",
    "\n",
    "            groups = m.groups()\n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            if groups:\n",
    "                first = groups[0].strip()\n",
    "                if re.match(r'^\\d+[A-C]?$', first, re.I):\n",
    "                    section_id = first.upper()\n",
    "                elif re.match(r'^[IVX]+$', first, re.I):\n",
    "                    section_id = first.upper()\n",
    "\n",
    "                # named‚Äêtitle group if captured\n",
    "                if len(groups) > 1 and groups[1]:\n",
    "                    title = groups[1].strip()\n",
    "                    title = re.sub(r'\\[TABLE_END\\].*$', '', title, flags=re.I)\n",
    "                    title = title.replace('|', '').strip()\n",
    "                    if title:\n",
    "                        section_title = title\n",
    "                else:\n",
    "                    # remainder of the line after the ID\n",
    "                    rem = full_line[m.end() - line_start:].lstrip(\" .‚Äì‚Äî-\").strip()\n",
    "                    if 0 < len(rem) < 200:\n",
    "                        section_title = rem\n",
    "\n",
    "            # fallback for pure‚Äênamed sections\n",
    "            up = full_line.upper()\n",
    "            if not section_id:\n",
    "                if 'BUSINESS' in up:\n",
    "                    section_id = '1'\n",
    "                elif 'RISK FACTORS' in up:\n",
    "                    section_id = '1A'\n",
    "                elif 'LEGAL PROCEEDINGS' in up:\n",
    "                    section_id = '3'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'section_id': section_id or 'UNKNOWN',\n",
    "                'section_title': section_title,\n",
    "            })\n",
    "\n",
    "    # dedupe & sort\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for m in all_matches:\n",
    "        if m['start_pos'] not in seen:\n",
    "            seen.add(m['start_pos'])\n",
    "            unique.append(m)\n",
    "\n",
    "    # build your DocumentSection objects *positionally*\n",
    "    for i, m in enumerate(unique):\n",
    "        start = m['start_pos']\n",
    "        end = unique[i+1]['start_pos'] if i+1 < len(unique) else len(content)\n",
    "        # ** POSITIONAL args match your DocumentSection signature **\n",
    "        sections.append(DocumentSection(\n",
    "            m['section_id'],\n",
    "            m['section_title'],\n",
    "            start,\n",
    "            end\n",
    "        ))\n",
    "\n",
    "    logger.info(f\"Strategy 1 found {len(sections)} sections\")\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94e7bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections |   0 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections |   0 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DocumentSection.__init__() got an unexpected keyword argument 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m results_universal = test_universal_detection_fixed()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m old_vs_new_sections = \u001b[43mcompare_old_vs_universal_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1181\u001b[39m, in \u001b[36mcompare_old_vs_universal_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1178\u001b[39m     content = f.read()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning old detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m old_sections = \u001b[43mdetect_sections_robust_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning universal detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1184\u001b[39m new_sections = detect_sections_robust_universal(content)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 340\u001b[39m, in \u001b[36mdetect_sections_robust_old\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03mMulti-strategy section detection with fallbacks (original version)\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAttempting Strategy 1: Regex-based section detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sections) >= \u001b[32m3\u001b[39m:\n\u001b[32m    343\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 successful: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    104\u001b[39m     start = m[\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    105\u001b[39m     end = unique[i+\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(unique) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sections.append(\u001b[43mDocumentSection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_title\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    113\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sections\n",
      "\u001b[31mTypeError\u001b[39m: DocumentSection.__init__() got an unexpected keyword argument 'id'"
     ]
    }
   ],
   "source": [
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ae3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from typing import List\n",
    "from your_module import DocumentSection    # ‚Üê adjust this to wherever DocumentSection lives\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure,\n",
    "    with positional DocumentSection construction.\n",
    "    \"\"\"\n",
    "    sections: List[DocumentSection] = []\n",
    "\n",
    "    patterns = [\n",
    "        # PART headers\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM headers (with optional A-C suffix)\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî-])', re.I | re.M),\n",
    "\n",
    "        # ‚Äú1. BUSINESS‚Äù‚Äìstyle\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Common named sections\n",
    "        re.compile(r'^.{0,50}\\b(BUSINESS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(RISK FACTORS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(LEGAL PROCEEDINGS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(FINANCIAL STATEMENTS)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(MANAGEMENT\\.S DISCUSSION)\\b', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(PROPERTIES)\\b\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}\\b(CONTROLS AND PROCEDURES)\\b\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "    for pat_idx, pat in enumerate(patterns):\n",
    "        for m in pat.finditer(content):\n",
    "            # extract the full line\n",
    "            ln0 = content.rfind('\\n', 0, m.start()) + 1\n",
    "            ln1 = content.find('\\n', m.end())\n",
    "            if ln1 == -1:\n",
    "                ln1 = len(content)\n",
    "            full_line = content[ln0:ln1].strip()\n",
    "\n",
    "            # simple filters\n",
    "            if len(full_line) < 3 or len(full_line) > 400:\n",
    "                continue\n",
    "            up = full_line.upper()\n",
    "            if ('TABLE' in up and ('START' in up or 'END' in up)) or full_line.count(' ') > 20:\n",
    "                continue\n",
    "            if any(tok in full_line.lower() for tok in ('table of contents','index')):\n",
    "                continue\n",
    "\n",
    "            grp = m.groups()\n",
    "            sec_id = None\n",
    "            sec_title = full_line\n",
    "\n",
    "            if grp:\n",
    "                first = grp[0].strip()\n",
    "                # is it an item number or part numeral?\n",
    "                if re.fullmatch(r'\\d+[A-C]?', first, re.I):\n",
    "                    sec_id = first.upper()\n",
    "                elif re.fullmatch(r'[IVX]+', first, re.I):\n",
    "                    sec_id = first.upper()\n",
    "\n",
    "                # if there was a second capture (like ‚ÄúBUSINESS‚Äù), use it\n",
    "                if len(grp) > 1 and grp[1]:\n",
    "                    t = grp[1].strip()\n",
    "                    t = re.sub(r'\\[TABLE_END\\].*$', '', t, flags=re.I).replace('|','').strip()\n",
    "                    if t:\n",
    "                        sec_title = t\n",
    "                else:\n",
    "                    # try to pull remainder of line after the ID\n",
    "                    rem = full_line[m.end() - ln0 :].lstrip(\" .‚Äì‚Äî-\").strip()\n",
    "                    if 0 < len(rem) < 200:\n",
    "                        sec_title = rem\n",
    "\n",
    "            # fallback canonical IDs for pure-named sections\n",
    "            U = sec_title.upper()\n",
    "            if not sec_id:\n",
    "                if 'BUSINESS' in U:\n",
    "                    sec_id = '1'\n",
    "                elif 'RISK FACTORS' in U:\n",
    "                    sec_id = '1A'\n",
    "                elif 'LEGAL PROCEEDINGS' in U:\n",
    "                    sec_id = '3'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start': ln0,\n",
    "                'end': ln1,\n",
    "                'id': sec_id or 'UNKNOWN',\n",
    "                'title': sec_title,\n",
    "            })\n",
    "\n",
    "    # sort & dedupe by start-pos\n",
    "    all_matches.sort(key=lambda x: x['start'])\n",
    "    unique = []\n",
    "    seen_starts = set()\n",
    "    for m in all_matches:\n",
    "        if m['start'] not in seen_starts:\n",
    "            seen_starts.add(m['start'])\n",
    "            unique.append(m)\n",
    "\n",
    "    # build DocumentSection (positional!)\n",
    "    for idx, m in enumerate(unique):\n",
    "        st = m['start']\n",
    "        en = unique[idx+1]['start'] if idx+1 < len(unique) else len(content)\n",
    "        sections.append(\n",
    "            DocumentSection(\n",
    "                m['id'],\n",
    "                m['title'],\n",
    "                st,\n",
    "                en\n",
    "            )\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Strategy 1 found {len(sections)} sections\")\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f780eeb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DocumentSection.__init__() got an unexpected keyword argument 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     content = f.read()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 3. Run your improved strategy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 4. Print out what it found\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sec \u001b[38;5;129;01min\u001b[39;00m sections:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# adjust these attributes to match your DocumentSection fields\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    104\u001b[39m     start = m[\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    105\u001b[39m     end = unique[i+\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(unique) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sections.append(\u001b[43mDocumentSection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_title\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    113\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sections\n",
      "\u001b[31mTypeError\u001b[39m: DocumentSection.__init__() got an unexpected keyword argument 'id'"
     ]
    }
   ],
   "source": [
    "# 2. Read in one of your 10-K or 10-Q text files\n",
    "with open(\"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 3. Run your improved strategy\n",
    "sections = detect_sections_strategy_1_improved(content)\n",
    "\n",
    "# 4. Print out what it found\n",
    "for sec in sections:\n",
    "    # adjust these attributes to match your DocumentSection fields\n",
    "    print(f\"ID: {sec.section_id} | Title: {sec.section_title!r} | \"\n",
    "          f\"Starts @ {sec.start_char}, Ends @ {sec.end_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "548c055d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DocumentSection.__init__() got an unexpected keyword argument 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     content = f.read()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2) Run your new regex-based detector\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 3) Inspect the results\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    104\u001b[39m     start = m[\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    105\u001b[39m     end = unique[i+\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(unique) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sections.append(\u001b[43mDocumentSection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_title\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    113\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sections\n",
      "\u001b[31mTypeError\u001b[39m: DocumentSection.__init__() got an unexpected keyword argument 'id'"
     ]
    }
   ],
   "source": [
    "# 1) Load one of your filings\n",
    "path = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 2) Run your new regex-based detector\n",
    "sections = detect_sections_strategy_1_improved(content)\n",
    "\n",
    "# 3) Inspect the results\n",
    "print(f\"Found {len(sections)} sections:\\n\")\n",
    "for sec in sections:\n",
    "    # Assuming your DocumentSection has these attributes:\n",
    "    print(f\"{sec.section_id:>4} | {sec.section_title!r}\")\n",
    "    print(f\"      chars {sec.start_char}‚Äì{sec.end_char}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content)\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    final_document_sections = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "        logger.debug(f\"Creating DocumentSection: Title='{title}', Type='{section_type}', Item='{item_number}', Part='{part}', Content len: {len(section_content)}, Start: {start_pos}, End: {end_pos}\") # Added debug\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Captures: (Optional Page Num) | PART ID | PART Title (Optional) | Item ID | Item Title (Optional) | Page Num\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title, pipe-separated. Catches \"Item 1. | Financial Statements | 3\"\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (no pipes separating title)\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions. Must be long enough, starts with capital.\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M),\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Strict filtering of TOC lines to remove noise\n",
    "            if any(kw in line.lower() for kw in ['page', 'signatures', 'exhibit', 'index', 'table of contents']) and len(line) < 30:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()): # Just a page number\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()): # Just \"1\" or \"1A\"\n",
    "                continue\n",
    "            if len(line) < 5: # Very short lines\n",
    "                continue\n",
    "            if re.search(r'\\d+\\s*$', line.strip()) and not re.match(r'(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?', line, re.I): # Looks like page number at end, but not a clear item/part line\n",
    "                continue\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column TOC\n",
    "                        part_id_cand = match.group(2) if len(match.groups()) >= 2 else None\n",
    "                        part_title_from_group = match.group(3) if len(match.groups()) >= 3 else None\n",
    "                        item_id = match.group(4).strip() if len(match.groups()) >= 4 else None\n",
    "                        item_title = match.group(5).strip() if len(match.groups()) >= 5 else \"\"\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand.strip()}\"\n",
    "                            title_for_part = part_title_from_group.strip() if part_title_from_group else f\"PART {part_id_cand.strip()}\"\n",
    "                            found_items.append((part_id_cand.strip(), title_for_part, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            title_for_item = item_title.strip() if item_title else f\"Item {item_id.strip()}\"\n",
    "                            found_items.append((item_id.strip(), title_for_item, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()):\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip()\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip()\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip()\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip()\n",
    "        \n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]):\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"):\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex)\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex)\n",
    "                \n",
    "                if len(cleaned_title_for_regex) > 5:\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff89ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content)\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    final_document_sections = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "        logger.debug(f\"Creating DocumentSection: Title='{title}', Type='{section_type}', Item='{item_number}', Part='{part}', Content len: {len(section_content)}, Start: {start_pos}, End: {end_pos}\") # Added debug\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC.\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Captures: (Optional Page Num) | PART ID | PART Title (Optional) | Item ID | Item Title (Optional) | Page Num\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title, pipe-separated. Catches \"Item 1. | Financial Statements | 3\"\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (no pipes separating title)\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions. Must be long enough, starts with capital.\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M),\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Strict filtering of TOC lines to remove noise\n",
    "            if any(kw in line.lower() for kw in ['page', 'signatures', 'exhibit', 'index', 'table of contents']) and len(line) < 30:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()): # Just a page number\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()): # Just \"1\" or \"1A\"\n",
    "                continue\n",
    "            if len(line) < 5: # Very short lines\n",
    "                continue\n",
    "            if re.search(r'\\d+\\s*$', line.strip()) and not re.match(r'(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?', line, re.I): # Looks like page number at end, but not a clear item/part line\n",
    "                continue\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column TOC\n",
    "                        part_id_cand = match.group(2) if len(match.groups()) >= 2 else None\n",
    "                        part_title_from_group = match.group(3) if len(match.groups()) >= 3 else None\n",
    "                        item_id = match.group(4).strip() if len(match.groups()) >= 4 else None\n",
    "                        item_title = match.group(5).strip() if len(match.groups()) >= 5 else \"\"\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand.strip()}\"\n",
    "                            title_for_part = part_title_from_group.strip() if part_title_from_group else f\"PART {part_id_cand.strip()}\"\n",
    "                            found_items.append((part_id_cand.strip(), title_for_part, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            title_for_item = item_title.strip() if item_title else f\"Item {item_id.strip()}\"\n",
    "                            found_items.append((item_id.strip(), title_for_item, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()):\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip()\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip()\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip()\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip()\n",
    "        \n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]):\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"):\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex)\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex)\n",
    "                \n",
    "                if len(cleaned_title_for_regex) > 5:\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a758294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ Initiating all test suites!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: name 'groups' is not defined\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections |   0 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections |   0 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DocumentSection.__init__() got an unexpected keyword argument 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m results_universal = test_universal_detection_fixed()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m old_vs_new_sections = \u001b[43mcompare_old_vs_universal_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m quick_pattern_test_fixed()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1181\u001b[39m, in \u001b[36mcompare_old_vs_universal_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1178\u001b[39m     content = f.read()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning old detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m old_sections = \u001b[43mdetect_sections_robust_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning universal detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1184\u001b[39m new_sections = detect_sections_robust_universal(content)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 340\u001b[39m, in \u001b[36mdetect_sections_robust_old\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03mMulti-strategy section detection with fallbacks (original version)\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAttempting Strategy 1: Regex-based section detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sections) >= \u001b[32m3\u001b[39m:\n\u001b[32m    343\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 successful: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    104\u001b[39m     start = m[\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    105\u001b[39m     end = unique[i+\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstart_pos\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(unique) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sections.append(\u001b[43mDocumentSection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msection_title\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    113\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sections\n",
      "\u001b[31mTypeError\u001b[39m: DocumentSection.__init__() got an unexpected keyword argument 'id'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Run the fixed tests\n",
    "# These calls should be at the very end of your notebook or script,\n",
    "# after all function definitions.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Initiating all test suites!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All test suites completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b3396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n",
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmp4bq4d8qd_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 22 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: PART II...\n",
      "  7: Item 6.    Selected Financial Data...\n",
      "  8: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  9: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item 8.    Financial Statements and Supplementary Data...\n",
      "  11: Notes to Consolidated Financial Statements...\n",
      "  12: Opinion on the Financial Statements...\n",
      "  13: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  14: Item 9A.    Controls and Procedures...\n",
      "  15: PART III...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1713\u001b[39m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# Run the fixed tests\u001b[39;00m\n\u001b[32m   1712\u001b[39m results_universal = test_universal_detection_fixed()\n\u001b[32m-> \u001b[39m\u001b[32m1713\u001b[39m old_vs_new_sections = \u001b[43mcompare_old_vs_universal_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1714\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1663\u001b[39m, in \u001b[36mcompare_old_vs_universal_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1660\u001b[39m     content = f.read()\n\u001b[32m   1662\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning old detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m old_sections = \u001b[43mdetect_sections_robust_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning universal detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1666\u001b[39m new_sections = detect_sections_robust_universal(content)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 621\u001b[39m, in \u001b[36mdetect_sections_robust_old\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    618\u001b[39m \u001b[33;03mMulti-strategy section detection with fallbacks (original version)\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    620\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAttempting Strategy 1: Regex-based section detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Fixed argument name for DocumentSection constructor\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sections) >= \u001b[32m3\u001b[39m:\n\u001b[32m    624\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 successful: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 255\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    252\u001b[39m part = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    253\u001b[39m title = match[\u001b[33m'\u001b[39m\u001b[33msection_title\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Use captured title\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m^[IVX]+$\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_id\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    256\u001b[39m     section_type = \u001b[33m'\u001b[39m\u001b[33mpart\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m     part = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPART \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/__init__.py:166\u001b[39m, in \u001b[36mmatch\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatch\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    164\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'NoneType'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = unique_matches[-1] # Fix: Use unique_matches, not final_matches\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     unique_matches[-1] = current_match\n",
    "            else:\n",
    "                unique_matches.append(current_match)\n",
    "\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    sections_to_return = [] # Renamed to avoid conflict with outer 'sections'\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        section_type = 'content' # Default\n",
    "        item_number = None\n",
    "        part = None\n",
    "        title = match['section_title'] # Use captured title\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update for inheritance\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Inherit part\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        sections_to_return.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections_to_return\n",
    "\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content)\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else: # Fallback if part not explicitly set for 10Q item\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting.\n",
    "    Ensures content for each DocumentSection is correctly sliced.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s*([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\.?\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(r'^\\s*(BUSINESS|RISK FACTORS|LEGAL PROCEEDINGS|FINANCIAL STATEMENTS|MANAGEMENT\\'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS|PROPERTIES|CONTROLS AND PROCEDURES)\\s*$', re.I | re.M)\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     final_matches[-1] = current_match\n",
    "            else:\n",
    "                final_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    final_document_sections = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part\n",
    "            clean_title_part = title.upper().replace(part, '').strip(' -.')\n",
    "            if clean_title_part:\n",
    "                title = f\"{part} - {clean_title_part}\"\n",
    "            else:\n",
    "                title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part\n",
    "            clean_title_item = title.upper().replace(f\"ITEM {item_number}\", '').strip(' -.')\n",
    "            if clean_title_item:\n",
    "                title = f\"Item {item_number} - {clean_title_item}\"\n",
    "            else:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in title.upper() for keyword in ['BUSINESS', 'RISK FACTORS', 'LEGAL PROCEEDINGS', 'FINANCIAL STATEMENTS', 'MANAGEMENT\\'S DISCUSSION', 'PROPERTIES', 'CONTROLS AND PROCEDURES']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "        logger.debug(f\"Creating DocumentSection: Title='{title}', Type='{section_type}', Item='{item_number}', Part='{part}', Content len: {len(section_content)}, Start: {start_pos}, End: {end_pos}\")\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Fixed argument name for DocumentSection constructor\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Group 1: Optional Page Num | Part ID (Group 2) | Part Title (Group 3) | Item ID (Group 4) | Item Title (Group 5)\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title, pipe-separated.\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (no pipes separating title)\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions.\n",
    "        # Group 1: Title\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M),\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line\n",
    "        # Group 1: PART ID\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Group 1: Item ID, Group 2: Title\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if any(kw in line.lower() for kw in ['page', 'signatures', 'exhibit', 'index', 'table of contents']) and len(line) < 30:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()):\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()):\n",
    "                continue\n",
    "            if len(line) < 5:\n",
    "                continue\n",
    "            if re.search(r'\\d+\\s*$', line.strip()) and not re.match(r'(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?', line, re.I):\n",
    "                continue\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column TOC\n",
    "                        part_id_cand = match.group(2) if len(match.groups()) >= 2 and match.group(2) else None\n",
    "                        part_title_from_group = match.group(3) if len(match.groups()) >= 3 and match.group(3) else None\n",
    "                        item_id = match.group(4).strip() if len(match.groups()) >= 4 and match.group(4) else None\n",
    "                        item_title = match.group(5).strip() if len(match.groups()) >= 5 and match.group(5) else \"\"\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand.strip()}\"\n",
    "                            title_for_part = part_title_from_group.strip() if part_title_from_group else f\"PART {part_id_cand.strip()}\"\n",
    "                            found_items.append((part_id_cand.strip(), title_for_part, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            title_for_item = item_title.strip() if item_title else f\"Item {item_id.strip()}\"\n",
    "                            found_items.append((item_id.strip(), title_for_item, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()):\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip()\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip()\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip()\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip()\n",
    "        \n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]):\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"):\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex)\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex)\n",
    "                \n",
    "                if len(cleaned_title_for_regex) > 5:\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            logger.debug(f\"Processing section: '{section.title}', Content len: {len(section.content)}, Start: {section.start_pos}, End: {section.end_pos}\") # Added debug\n",
    "\n",
    "            if not section.content.strip():\n",
    "                continue\n",
    "\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            if narrative_content_in_section.strip():\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0\n",
    "\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    sections_1 = detect_sections_strategy_1_improved(content)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    sections_2 = detect_sections_strategy_2(content)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "if chunks:\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison)\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    sections_dist = {}\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    if form_type == '10K' and filing_date.month <= 3:\n",
    "        fiscal_year -= 1\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0411b",
   "metadata": {},
   "source": [
    "*** All chuinks Processed ^ now just need to get typeerror working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11428e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:üîç Improved detection found 0 potential sections:\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "WARNING:__main__:Malformed filename: tmpiwyszon__bad_name.txt. Using default metadata.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "WARNING:__main__:No table of contents found in detect_sections_from_toc_universal.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in tmpiwyszon__bad_name.txt\n",
      "INFO:__main__:Created 0 chunks for tmpiwyszon__bad_name.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n",
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Improved detection found 22 potential sections:\n",
      "INFO:__main__:  1: PART I...\n",
      "INFO:__main__:  2: Item 1A.    Risk Factors...\n",
      "INFO:__main__:  3: Item 1B.    Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item 3.    Legal Proceedings...\n",
      "INFO:__main__:  5: Item 4.    Mine Safety Disclosures...\n",
      "INFO:__main__:  6: PART II...\n",
      "INFO:__main__:  7: Item 6.    Selected Financial Data...\n",
      "INFO:__main__:  8: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "INFO:__main__:  9: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item 8.    Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "INFO:__main__:  14: Item 9A.    Controls and Procedures...\n",
      "INFO:__main__:  15: PART III...\n",
      "INFO:__main__:Strategy 1 successful: Found 22 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 22 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +-3 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. PART I\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. PART II\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "  11. Notes to Consolidated Financial Statements\n",
      "\n",
      "  12. Opinion on the Financial Statements\n",
      "\n",
      "  13. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  14. Controls and Procedures\n",
      "\n",
      "  15. PART III\n",
      "\n",
      "  16. Executive Compensation\n",
      "\n",
      "  17. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  18. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  19. Principal Accountant Fees and Services\n",
      "\n",
      "  20. PART IV\n",
      "\n",
      "  21. (1) All financial statements\n",
      "\n",
      "  22. Form 10-K Summary\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to DEBUG level to see detailed process\n",
    "logging.basicConfig(level=logging.DEBUG) # CRITICAL CHANGE: Set to DEBUG\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = unique_matches[-1] # Corrected from final_matches to unique_matches\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     unique_matches[-1] = current_match\n",
    "            else:\n",
    "                unique_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    sections_to_return = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "        title = match['section_title']\n",
    "\n",
    "        if section_id and re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                title = part\n",
    "        elif section_id and re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        sections_to_return.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections_to_return\n",
    "\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Corrected argument name for DocumentSection constructor\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else: # Fallback if part not explicitly set for 10Q item\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Group 1: Optional Page Num | Part ID (Group 2) | Part Title (Group 3) | Item ID (Group 4) | Item Title (Group 5)\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title, pipe-separated. Catches \"Item 1. | Financial Statements | 3\"\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (no pipes separating title)\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions.\n",
    "        # Group 1: Title\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M),\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line\n",
    "        # Group 1: PART ID\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Group 1: Item ID, Group 2: Title\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if any(kw in line.lower() for kw in ['page', 'signatures', 'exhibit', 'index', 'table of contents']) and len(line) < 30:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()):\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()):\n",
    "                continue\n",
    "            if len(line) < 5:\n",
    "                continue\n",
    "            if re.search(r'\\d+\\s*$', line.strip()) and not re.match(r'(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?', line, re.I):\n",
    "                continue\n",
    "\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column TOC\n",
    "                        part_id_cand = match.group(2) if len(match.groups()) >= 2 and match.group(2) else None\n",
    "                        part_title_from_group = match.group(3) if len(match.groups()) >= 3 and match.group(3) else None\n",
    "                        item_id = match.group(4).strip() if len(match.groups()) >= 4 and match.group(4) else None\n",
    "                        item_title = match.group(5).strip() if len(match.groups()) >= 5 and match.group(5) else \"\"\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand.strip()}\"\n",
    "                            title_for_part = part_title_from_group.strip() if part_title_from_group else f\"PART {part_id_cand.strip()}\"\n",
    "                            found_items.append((part_id_cand.strip(), title_for_part, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            title_for_item = item_title.strip() if item_title else f\"Item {item_id.strip()}\"\n",
    "                            found_items.append((item_id.strip(), title_for_item, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()):\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip()\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip()\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip()\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip()\n",
    "        \n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]):\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"):\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex)\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex)\n",
    "                \n",
    "                if len(cleaned_title_for_regex) > 5:\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # DEBUG: Log content length of incoming section\n",
    "            logger.debug(f\"Processing section: '{section.title}', Content len: {len(section.content)}, Start: {section.start_pos}, End: {section.end_pos}\")\n",
    "\n",
    "            if not section.content.strip():\n",
    "                continue\n",
    "\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            if narrative_content_in_section.strip():\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0\n",
    "\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    sections_1 = detect_sections_strategy_1_improved(content)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    sections_2 = detect_sections_strategy_2(content)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "if chunks:\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison)\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    sections_dist = {}\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    if form_type == '10K' and filing_date.month <= 3:\n",
    "        fiscal_year -= 1\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7464eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = unique_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     unique_matches[-1] = current_match\n",
    "            else:\n",
    "                unique_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    sections_to_return = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # FIX: Guard re.match with a check for section_id being a string\n",
    "        section_id_str = match['section_id'].upper() if match['section_id'] else '' # Ensure it's an empty string if None\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "        title = match['section_title']\n",
    "\n",
    "        if section_id_str and re.match(r'^[IVX]+$', section_id_str): # Guarded check\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id_str}\"\n",
    "            current_part = part\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id_str:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                title = part\n",
    "        elif section_id_str and re.match(r'^\\d+[A-C]?$', section_id_str): # Guarded check\n",
    "            section_type = 'item'\n",
    "            item_number = section_id_str\n",
    "            part = current_part\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id_str:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        sections_to_return.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d4bcf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Improved detection found 22 potential sections:\n",
      "INFO:__main__:  1: PART I...\n",
      "INFO:__main__:  2: Item 1A.    Risk Factors...\n",
      "INFO:__main__:  3: Item 1B.    Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item 3.    Legal Proceedings...\n",
      "INFO:__main__:  5: Item 4.    Mine Safety Disclosures...\n",
      "INFO:__main__:  6: PART II...\n",
      "INFO:__main__:  7: Item 6.    Selected Financial Data...\n",
      "INFO:__main__:  8: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "INFO:__main__:  9: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item 8.    Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "INFO:__main__:  14: Item 9A.    Controls and Procedures...\n",
      "INFO:__main__:  15: PART III...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_line_upper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the fixed tests\u001b[39;00m\n\u001b[32m      2\u001b[39m results_universal = test_universal_detection_fixed()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m old_vs_new_sections = \u001b[43mcompare_old_vs_universal_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1373\u001b[39m, in \u001b[36mcompare_old_vs_universal_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1370\u001b[39m     content = f.read()\n\u001b[32m   1372\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning old detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m old_sections = \u001b[43mdetect_sections_robust_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning universal detection...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1376\u001b[39m new_sections = detect_sections_robust_universal(content)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 330\u001b[39m, in \u001b[36mdetect_sections_robust_old\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03mMulti-strategy section detection with fallbacks (original version)\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    329\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAttempting Strategy 1: Regex-based section detection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m sections = \u001b[43mdetect_sections_strategy_1_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Corrected argument name for DocumentSection constructor\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sections) >= \u001b[32m3\u001b[39m:\n\u001b[32m    333\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategy 1 successful: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mdetect_sections_strategy_1_improved\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m title:\n\u001b[32m    135\u001b[39m         title = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mItem \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(keyword \u001b[38;5;129;01min\u001b[39;00m full_line_upper \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    137\u001b[39m         [\u001b[33m'\u001b[39m\u001b[33mBUSINESS\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRISK\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLEGAL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFINANCIAL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMANAGEMENT\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPROPERTIES\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCONTROLS\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m    138\u001b[39m     section_type = \u001b[33m'\u001b[39m\u001b[33mnamed_section\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    141\u001b[39m sections_to_return.append(DocumentSection(\n\u001b[32m    142\u001b[39m     title=title,\n\u001b[32m    143\u001b[39m     content=section_content,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     end_pos=end_pos\n\u001b[32m    149\u001b[39m ))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m title:\n\u001b[32m    135\u001b[39m         title = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mItem \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(keyword \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfull_line_upper\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    137\u001b[39m         [\u001b[33m'\u001b[39m\u001b[33mBUSINESS\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRISK\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLEGAL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFINANCIAL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMANAGEMENT\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPROPERTIES\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCONTROLS\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m    138\u001b[39m     section_type = \u001b[33m'\u001b[39m\u001b[33mnamed_section\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    141\u001b[39m sections_to_return.append(DocumentSection(\n\u001b[32m    142\u001b[39m     title=title,\n\u001b[32m    143\u001b[39m     content=section_content,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     end_pos=end_pos\n\u001b[32m    149\u001b[39m ))\n",
      "\u001b[31mNameError\u001b[39m: name 'full_line_upper' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6fc1a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:üîç Improved detection found 0 potential sections:\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n",
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmp34udbd2z_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 21 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 2 - Properties...\n",
      "INFO:__main__:  5: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  7: Item/Part 5 - Market for the Registrant‚Äôs Common Stock, Related Shareholde...\n",
      "INFO:__main__:  8: Item/Part 6 - Reserved...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  12: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "INFO:__main__:  13: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  14: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  15: Item/Part 10 - Directors, Executive Officers, and Corporate Governance...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 21 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 19 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,266 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 108 chars\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 4,182 chars\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 21 sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "     Type: item, Length: 13,286 chars\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 55,961 chars\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "     Type: item, Length: 107 chars\n",
      "\n",
      "  4. Item 2 - PROPERTIES\n",
      "\n",
      "     Type: item, Length: 1,438 chars\n",
      "\n",
      "  5. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 186 chars\n",
      "\n",
      "  6. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "     Type: item, Length: 123 chars\n",
      "\n",
      "  7. Item 5 - MARKET FOR THE REGISTRANT‚ÄôS COMMON STOCK, RELATED SHAREHOLDER MATTERS, AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "     Type: item, Length: 508 chars\n",
      "\n",
      "  8. Item 6 - RESERVED\n",
      "\n",
      "     Type: item, Length: 50,498 chars\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 6,525 chars\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "     Type: item, Length: 86,332 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 11 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements...\n",
      "INFO:__main__:  2: Item/Part unknown - Legal Proceedings...\n",
      "INFO:__main__:  3: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  4: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  5: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  6: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  7: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  8: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  9: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "INFO:__main__:  10: Item/Part 5 - Other Information...\n",
      "INFO:__main__:  11: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 11 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Financial Statements (Unaudited)...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part 1 - Legal Proceedings...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:üîç Improved detection found 22 potential sections:\n",
      "INFO:__main__:  1: PART I...\n",
      "INFO:__main__:  2: Item 1A.    Risk Factors...\n",
      "INFO:__main__:  3: Item 1B.    Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item 3.    Legal Proceedings...\n",
      "INFO:__main__:  5: Item 4.    Mine Safety Disclosures...\n",
      "INFO:__main__:  6: PART II...\n",
      "INFO:__main__:  7: Item 6.    Selected Financial Data...\n",
      "INFO:__main__:  8: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "INFO:__main__:  9: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item 8.    Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Notes to Consolidated Financial Statements...\n",
      "INFO:__main__:  12: Opinion on the Financial Statements...\n",
      "INFO:__main__:  13: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "INFO:__main__:  14: Item 9A.    Controls and Procedures...\n",
      "INFO:__main__:  15: PART III...\n",
      "INFO:__main__:Strategy 1 successful: Found 22 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 19 unique sections:\n",
      "INFO:__main__:  1: Item/Part 1 - Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 5 - Market for Registrant‚Äôs Common Equity, Related Stockholder M...\n",
      "INFO:__main__:  7: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  12: Item/Part 9A - Controls and Procedures...\n",
      "INFO:__main__:  13: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  14: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 11 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS\n",
      "\n",
      "     Type: item, Length: 34,940 chars\n",
      "\n",
      "  2. Legal Proceedings\n",
      "\n",
      "     Type: named_section, Length: 32,116 chars\n",
      "\n",
      "  3. Item 2 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 45,107 chars\n",
      "\n",
      "  4. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 4,405 chars\n",
      "\n",
      "  5. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 2,104 chars\n",
      "\n",
      "  6. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 162 chars\n",
      "\n",
      "  7. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 59,433 chars\n",
      "\n",
      "  8. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 103 chars\n",
      "\n",
      "  9. Item 3 - DEFAULTS UPON SENIOR SECURITIES\n",
      "\n",
      "     Type: item, Length: 153 chars\n",
      "\n",
      "  10. Item 5 - OTHER INFORMATION\n",
      "\n",
      "     Type: item, Length: 3,031 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. Item 1 - FINANCIAL STATEMENTS (UNAUDITED)\n",
      "\n",
      "     Type: item, Length: 115,893 chars\n",
      "\n",
      "  2. Item 2 - MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Item 3 - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Item 4 - CONTROLS AND PROCEDURES\n",
      "\n",
      "     Type: item, Length: 1,032 chars\n",
      "\n",
      "  5. Item 1 - LEGAL PROCEEDINGS\n",
      "\n",
      "     Type: item, Length: 220 chars\n",
      "\n",
      "  6. Item 1A - RISK FACTORS\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Item 2 - UNREGISTERED SALES OF EQUITY SECURITIES AND USE OF PROCEEDS\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Item 6 - EXHIBITS\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   | 21 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   | 11 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "Running universal detection...\n",
      "\n",
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 22 sections\n",
      "\n",
      "  Universal detection: 19 sections\n",
      "\n",
      "  Improvement: +-3 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. PART I\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. PART II\n",
      "\n",
      "  7. Selected Financial Data\n",
      "\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  10. Financial Statements and Supplementary Data\n",
      "\n",
      "  11. Notes to Consolidated Financial Statements\n",
      "\n",
      "  12. Opinion on the Financial Statements\n",
      "\n",
      "  13. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  14. Controls and Procedures\n",
      "\n",
      "  15. PART III\n",
      "\n",
      "  16. Executive Compensation\n",
      "\n",
      "  17. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  18. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  19. Principal Accountant Fees and Services\n",
      "\n",
      "  20. PART IV\n",
      "\n",
      "  21. (1) All financial statements\n",
      "\n",
      "  22. Form 10-K Summary\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1 - BUSINESS\n",
      "\n",
      "  2. Item 1A - RISK FACTORS\n",
      "\n",
      "  3. Item 1B - UNRESOLVED STAFF COMMENTS\n",
      "\n",
      "  4. Item 3 - LEGAL PROCEEDINGS\n",
      "\n",
      "  5. Item 4 - MINE SAFETY DISCLOSURES\n",
      "\n",
      "  6. Item 5 - MARKET FOR REGISTRANT‚ÄôS COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "\n",
      "  7. Item 6 - SELECTED FINANCIAL DATA\n",
      "\n",
      "  8. Item 7 - MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "\n",
      "  9. Item 7A - QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "\n",
      "  10. Item 8 - FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "\n",
      "  11. Item 9 - CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "\n",
      "  12. Item 9A - CONTROLS AND PROCEDURES\n",
      "\n",
      "  13. Item 9B - OTHER INFORMATION\n",
      "\n",
      "  14. Item 11 - EXECUTIVE COMPENSATION\n",
      "\n",
      "  15. Item 12 - SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "\n",
      "  16. Item 13 - CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "\n",
      "  17. Item 14 - PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "\n",
      "  18. Item 15 - EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "\n",
      "  19. Item 16 - FORM 10-K SUMMARY\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to DEBUG level to see detailed process\n",
    "logging.basicConfig(level=logging.DEBUG) # CRITICAL CHANGE: Set to DEBUG\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content):\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            if (len(full_line) > 400 or\n",
    "                len(full_line) < 3 or\n",
    "                ('TABLE' in full_line.upper() and ('START' in full_line.upper() or 'END' in full_line.upper())) or\n",
    "                full_line.count(' ') > 20):\n",
    "                continue\n",
    "\n",
    "            if any(toc_indicator in full_line.lower() for toc_indicator in ['table of contents', 'index']):\n",
    "                continue\n",
    "            \n",
    "            section_id = None\n",
    "            section_title = full_line\n",
    "\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                potential_id = groups[0].strip()\n",
    "                is_item_id = re.match(r'^\\d+[A-C]?$', potential_id, re.I)\n",
    "                is_part_id = re.match(r'^[IVX]+$', potential_id, re.I)\n",
    "\n",
    "                if is_item_id or is_part_id:\n",
    "                    section_id = potential_id\n",
    "                    if len(groups) > 1 and groups[1]:\n",
    "                        section_title = groups[1].strip()\n",
    "                        section_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', section_title, flags=re.I).strip()\n",
    "                        section_title = section_title.replace('|', '').strip()\n",
    "                    else:\n",
    "                        remaining_line_after_id = full_line[match.end() - line_start:].strip()\n",
    "                        clean_line = re.sub(r'^\\s*\\.?\\s*[-‚Äì‚Äî]?\\s*', '', remaining_line_after_id).strip()\n",
    "                        if clean_line and len(clean_line) < 200:\n",
    "                            section_title = clean_line\n",
    "                        else:\n",
    "                             section_title = full_line\n",
    "                else:\n",
    "                    section_title = full_line\n",
    "                    if 'BUSINESS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1'\n",
    "                    elif 'RISK FACTORS' in full_line.upper() and not is_item_id and not is_part_id: section_id = '1A'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id if section_id else 'unknown',\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    all_matches.sort(key=lambda x: (x['start_pos'], x['pattern_idx']))\n",
    "\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = unique_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] < 100:\n",
    "                if current_match['section_id'] != 'unknown' and last_added_match['section_id'] == 'unknown':\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] != 'unknown' and last_added_match['section_id'] != 'unknown' and current_match['pattern_idx'] < last_added_match['pattern_idx']:\n",
    "                    unique_matches[-1] = current_match\n",
    "                elif current_match['section_id'] == last_added_match['section_id'] and len(current_match['section_title']) < len(last_added_match['section_title']) * 0.8:\n",
    "                     unique_matches[-1] = current_match\n",
    "            else:\n",
    "                unique_matches.append(current_match)\n",
    "\n",
    "    logger.info(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    sections_to_return = []\n",
    "    current_part = None\n",
    "\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # FIX for NameError: full_line_upper needs to be derived from 'match' in this loop's scope\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        # FIX: Ensure section_id is always a string when passed to re.match\n",
    "        section_id_str = match['section_id'].upper() if match['section_id'] is not None else '' # Use '' instead of None if match['section_id'] is None\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "        title = match['section_title']\n",
    "\n",
    "        if section_id_str and re.match(r'^[IVX]+$', section_id_str):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id_str}\"\n",
    "            current_part = part\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id_str:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                title = part\n",
    "        elif section_id_str and re.match(r'^\\d+[A-C]?$', section_id_str):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id_str\n",
    "            part = current_part\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id_str:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                title = f\"Item {item_number}\"\n",
    "        elif any(keyword in full_line_upper for keyword in # This 'full_line_upper' is now correctly scoped\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "\n",
    "\n",
    "        sections_to_return.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections_to_return\n",
    "\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Corrected argument name for DocumentSection constructor\n",
    "\n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    Handles 10K/10Q specific mappings and part/item inheritance.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            if part_number == 'PART I':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II':\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else: # Fallback if part not explicitly set for 10Q item\n",
    "                if item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_I[item_number]\n",
    "                    return f\"Part I, Item {item_number} - {item_name}\"\n",
    "                elif item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                    item_name = ITEM_NAME_MAP_10Q_PART_II[item_number]\n",
    "                    return f\"Part II, Item {item_number} - {item_name}\"\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number:\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            clean_title_suffix = section.title.replace(part_number, '').strip(' -.')\n",
    "            return f\"{part_number} - {clean_title_suffix}\"\n",
    "        return part_number\n",
    "\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    item_patterns = [\n",
    "        # Pattern 1: Multi-column TOC entry with PART, Item, and Title (e.g., KO 10-Q)\n",
    "        # Group 1: Optional Page Num | Part ID (Group 2) | Part Title (Group 3) | Item ID (Group 4) | Item Title (Group 5)\n",
    "        re.compile(r'(?i)(?:Page\\s*\\|\\s*)?\\s*(PART\\s*([IVX]+)\\.?(?:\\s*([^\\n|]+?))?\\s*\\|\\s*)?Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 2: Simpler Item/Part line with Title, pipe-separated. Catches \"Item 1. | Financial Statements | 3\"\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*\\|\\s*([^\\n|]+?)(?:\\s*\\|\\s*\\d+)?', re.M),\n",
    "        \n",
    "        # Pattern 3: Standalone Item/Part line with Title (no pipes separating title)\n",
    "        # Group 1: Item/PART ID, Group 2: Title\n",
    "        re.compile(r'(?i)^\\s*(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        \n",
    "        # Pattern 4: Generic TOC titles, often sub-sections or long descriptions.\n",
    "        # Group 1: Title\n",
    "        re.compile(r'^\\s*([A-Z][A-Za-z0-9\\s\\',&\\(\\)\\-\\.]{15,})\\s*(?:\\|\\s*\\d+)?$', re.M),\n",
    "        \n",
    "        # Pattern 5: Simple \"PART X\" line\n",
    "        # Group 1: PART ID\n",
    "        re.compile(r'(?i)^\\s*PART\\s*([IVX]+)\\s*$', re.M),\n",
    "        \n",
    "        # Pattern 6: Number-dot format (e.g., \"1. Business\") usually at start of line\n",
    "        # Group 1: Item ID, Group 2: Title\n",
    "        re.compile(r'^\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n|]+)', re.M),\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    current_part_id_context = None\n",
    "\n",
    "    if toc_content:\n",
    "        for line in toc_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if any(kw in line.lower() for kw in ['page', 'signatures', 'exhibit', 'index', 'table of contents']) and len(line) < 30:\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()):\n",
    "                continue\n",
    "            if re.match(r'^\\s*(\\d{1,2}[A-C]?)\\s*$', line.strip()):\n",
    "                continue\n",
    "            if len(line) < 5:\n",
    "                continue\n",
    "            if re.search(r'\\d+\\s*$', line.strip()) and not re.match(r'(?:Item|PART)\\s*(\\d{1,2}[A-C]?|[IVX]+)\\.?', line, re.I):\n",
    "                continue\n",
    "\n",
    "\n",
    "            for pattern in item_patterns:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    item_id = None\n",
    "                    item_title = \"\"\n",
    "                    section_type_raw = 'unknown'\n",
    "\n",
    "                    if pattern == item_patterns[0]: # Pattern 1: Complex multi-column TOC\n",
    "                        part_id_cand = match.group(2) if len(match.groups()) >= 2 and match.group(2) else None\n",
    "                        part_title_from_group = match.group(3) if len(match.groups()) >= 3 and match.group(3) else None\n",
    "                        item_id = match.group(4).strip() if len(match.groups()) >= 4 and match.group(4) else None\n",
    "                        item_title = match.group(5).strip() if len(match.groups()) >= 5 and match.group(5) else \"\"\n",
    "                        \n",
    "                        if part_id_cand:\n",
    "                            current_part_id_context = f\"PART {part_id_cand.strip()}\"\n",
    "                            title_for_part = part_title_from_group.strip() if part_title_from_group else f\"PART {part_id_cand.strip()}\"\n",
    "                            found_items.append((part_id_cand.strip(), title_for_part, 'part', current_part_id_context))\n",
    "                        \n",
    "                        if item_id:\n",
    "                            section_type_raw = 'item'\n",
    "                            title_for_item = item_title.strip() if item_title else f\"Item {item_id.strip()}\"\n",
    "                            found_items.append((item_id.strip(), title_for_item, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "\n",
    "                    elif pattern in [item_patterns[1], item_patterns[2], item_patterns[5]]: # Patterns with ID as group 1, Title as group 2 (or inferred from line)\n",
    "                        item_id = match.group(1).strip() if match.group(1) else None\n",
    "                        item_title = match.group(2).strip() if len(match.groups()) > 1 and match.group(2) else \"\"\n",
    "\n",
    "                        is_item = re.match(r'^\\d+[A-C]?$', item_id, re.I)\n",
    "                        is_part = re.match(r'^[IVX]+$', item_id, re.I)\n",
    "\n",
    "                        if is_item:\n",
    "                            section_type_raw = 'item'\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                        elif is_part:\n",
    "                            section_type_raw = 'part'\n",
    "                            current_part_id_context = f\"PART {item_id}\"\n",
    "                            found_items.append((item_id, item_title, section_type_raw, current_part_id_context))\n",
    "                            break\n",
    "                    \n",
    "                    elif pattern == item_patterns[3]: # Generic titles (Pattern 4: e.g., \"Consolidated Statements of Cash Flows\")\n",
    "                        item_title = match.group(1).strip()\n",
    "                        if item_title and len(item_title) > 10 and not re.match(r'^\\d+(\\.\\d+)?$', item_title.replace('.', '').strip()):\n",
    "                             found_items.append((None, item_title, 'named_section', current_part_id_context))\n",
    "                             break\n",
    "                    \n",
    "                    elif pattern == item_patterns[4]: # Simple \"PART X\" line (Pattern 5)\n",
    "                        item_id = match.group(1).strip()\n",
    "                        current_part_id_context = f\"PART {item_id}\"\n",
    "                        found_items.append((item_id, f\"PART {item_id}\", 'part', current_part_id_context))\n",
    "                        break\n",
    "\n",
    "    unique_items = []\n",
    "    seen_keys = set()\n",
    "    \n",
    "    processed_items_for_dedup = []\n",
    "    for item_data in found_items:\n",
    "        item_id, title_raw, section_type_raw, part_context = item_data\n",
    "        \n",
    "        cleaned_title = re.sub(r'\\|\\s*\\d+\\s*$', '', title_raw).strip()\n",
    "        cleaned_title = re.sub(r'\\s*\\.\\s*$', '', cleaned_title).strip()\n",
    "        cleaned_title = re.sub(r'\\[TABLE_END\\]\\s*.*', '', cleaned_title, flags=re.I).strip()\n",
    "        cleaned_title = re.sub(r'\\s+', ' ', cleaned_title).strip()\n",
    "        \n",
    "        if not cleaned_title or len(cleaned_title) < 5 or re.match(r'^\\d+(\\.\\d+)?$', cleaned_title):\n",
    "            continue\n",
    "\n",
    "        processed_items_for_dedup.append({\n",
    "            'item_id': item_id,\n",
    "            'title': cleaned_title,\n",
    "            'type': section_type_raw,\n",
    "            'part': part_context\n",
    "        })\n",
    "\n",
    "    processed_items_for_dedup.sort(key=lambda x: (x['part'] if x['part'] else '', x['item_id'] if x['item_id'] else '', x['title']))\n",
    "\n",
    "    for item in processed_items_for_dedup:\n",
    "        key = (item['item_id'], item['title'], item['type'], item['part'])\n",
    "        if key not in seen_keys:\n",
    "            unique_items.append(DocumentSection(\n",
    "                title=item['title'],\n",
    "                content=\"\",\n",
    "                section_type=item['type'],\n",
    "                item_number=item['item_id'] if item['type'] == 'item' else None,\n",
    "                part=item['part'],\n",
    "                start_pos=0,\n",
    "                end_pos=0\n",
    "            ))\n",
    "            seen_keys.add(key)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for i, sec in enumerate(unique_items[:15]):\n",
    "        logger.info(f\"  ‚Ä¢ ID: {sec.item_number if sec.item_number else sec.part if sec.part else 'None'}, Type: {sec.section_type}, Title: {sec.title[:60]}...\")\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            pattern_parts = []\n",
    "            \n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?')\n",
    "            if toc_entry.part and toc_entry.part.startswith(\"PART \"):\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "            \n",
    "            if toc_entry.title:\n",
    "                cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', toc_entry.title).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', cleaned_title_for_regex).strip()\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', cleaned_title_for_regex)\n",
    "                cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', cleaned_title_for_regex)\n",
    "                \n",
    "                if len(cleaned_title_for_regex) > 5:\n",
    "                    pattern_parts.append(r'\\b?' + re.escape(cleaned_title_for_regex) + r'\\b?')\n",
    "                else:\n",
    "                    pattern_parts.append(re.escape(cleaned_title_for_regex))\n",
    "                \n",
    "            if not pattern_parts:\n",
    "                logger.warning(f\"No valid pattern parts for TOC entry: '{toc_entry.title}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    elif next_toc_entry.part and next_toc_entry.part.startswith(\"PART \"):\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")) + r'\\.?')\n",
    "                    if next_toc_entry.title:\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\|\\s*\\d+', '', next_toc_entry.title).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s*\\.\\s*$', '', next_cleaned_title_for_regex).strip()\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+-\\s+', r'\\s*[-‚Äì‚Äî]?\\s*', next_cleaned_title_for_regex)\n",
    "                        next_cleaned_title_for_regex = re.sub(r'\\s+', r'\\s+', next_cleaned_title_for_regex)\n",
    "                        if len(next_cleaned_title_for_regex) > 5:\n",
    "                            next_pattern_parts.append(r'\\b?' + re.escape(next_cleaned_title_for_regex) + r'\\b?')\n",
    "                        else:\n",
    "                            next_pattern_parts.append(re.escape(next_cleaned_title_for_regex))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # DEBUG: Log content length of incoming section\n",
    "            logger.debug(f\"Processing section: '{section.title}', Content len: {len(section.content)}, Start: {section.start_pos}, End: {section.end_pos}\")\n",
    "\n",
    "            if not section.content.strip():\n",
    "                continue\n",
    "\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            if narrative_content_in_section.strip():\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0\n",
    "\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    sections_1 = detect_sections_strategy_1_improved(content)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    sections_2 = detect_sections_strategy_2(content)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "if chunks:\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison)\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    sections_dist = {}\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts[0], parts[1], parts[2] # Corrected unpacking\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    if form_type == '10K' and filing_date.month <= 3:\n",
    "        fiscal_year -= 1\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take-home-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
