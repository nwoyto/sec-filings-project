{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6da480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing fixed section detection...\n",
      "Add this to your notebook and replace the detect_sections_robust function\n",
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "‚úÖ Robust error handling and logging\n",
      "‚úÖ Structured data classes for better organization\n",
      "‚úÖ Quality validation and statistics\n",
      "‚úÖ Separate table and narrative processing\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SEC Filing Preprocessing Strategy - From Scratch\n",
    "# Let's build a robust chunking approach step by step\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. IMPROVED SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "# Keep the excellent domain knowledge from the original\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\", \n",
    "    \"1A\": \"Risk Factors\", \n",
    "    \"1B\": \"Unresolved Staff Comments\", \n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\", \n",
    "    \"3\": \"Legal Proceedings\", \n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\", \n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\", \n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\", \n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\", \n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\", \n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\", \n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\", \n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "    \n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "    \n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        r'(?im)^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$',\n",
    "        r'(?im)^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$',\n",
    "        \n",
    "        # ITEM patterns - much more flexible\n",
    "        r'(?im)^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        r'(?im)^ITEM\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        r'(?im)Item\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        \n",
    "        # Number-dot format common in SEC filings\n",
    "        r'(?im)^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}',\n",
    "        \n",
    "        # Content-based patterns for known sections\n",
    "        r'(?im)^.{0,50}(BUSINESS)\\s*$',\n",
    "        r'(?im)^.{0,50}(RISK FACTORS)\\s*$',\n",
    "        r'(?im)^.{0,50}(LEGAL PROCEEDINGS)\\s*$',\n",
    "        r'(?im)^.{0,50}(FINANCIAL STATEMENTS)\\s*$',\n",
    "        r'(?im)^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*',\n",
    "        r'(?im)^.{0,50}(PROPERTIES)\\s*$',\n",
    "        r'(?im)^.{0,50}(CONTROLS AND PROCEDURES)\\s*$',\n",
    "    ]\n",
    "    \n",
    "    all_matches = []\n",
    "    \n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in re.finditer(pattern, content):\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "            \n",
    "            full_line = content[line_start:line_end].strip()\n",
    "            \n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "            \n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "            \n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200 \n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "    \n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "        \n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "        \n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in \n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        \n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Update the main detect_sections_robust function\n",
    "def detect_sections_robust_fixed(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Updated robust detection with the fixed strategy\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Fixed Strategy 1: Improved regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_fixed(content)\n",
    "    \n",
    "    if len(sections) >= 3:  # Good result\n",
    "        logger.info(f\"Fixed Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"Fixed Strategy 1 found few sections, trying Strategy 2\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "    \n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# Test the fixed detection\n",
    "print(\"üß™ Testing fixed section detection...\")\n",
    "print(\"Add this to your notebook and replace the detect_sections_robust function\")\n",
    "\n",
    "\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "    \n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "    \n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "            \n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "        \n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "        \n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "            \n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1(content)\n",
    "    \n",
    "    if len(sections) >= 3:  # Good result\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "    \n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    \n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100, \n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "    \n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "        \n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "            \n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            overlap_tokens = 0\n",
    "            \n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            for sent in reversed(current_chunk_sentences):\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "        \n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "    \n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "        \n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "    \n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "    \n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MAIN PROCESSING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def process_filing_robust(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Main function that puts it all together\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "        parts = file_id.split('_')\n",
    "        \n",
    "        if len(parts) != 3:\n",
    "            logger.error(f\"Invalid filename format: {filename}\")\n",
    "            return []\n",
    "        \n",
    "        ticker, form_type, filing_date_str = parts\n",
    "        \n",
    "        # Create filing metadata\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "        \n",
    "        # Adjust fiscal year for 10-K filings\n",
    "        if form_type == '10K' and filing_date.month < 4:\n",
    "            fiscal_year -= 1\n",
    "        \n",
    "        filing_metadata = FilingMetadata(\n",
    "            ticker=ticker,\n",
    "            form_type=form_type,\n",
    "            filing_date=filing_date_str,\n",
    "            fiscal_year=fiscal_year,\n",
    "            fiscal_quarter=fiscal_quarter,\n",
    "            file_path=file_path\n",
    "        )\n",
    "        \n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        \n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "        \n",
    "        # Detect sections\n",
    "        sections = detect_sections_robust(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "        \n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "        \n",
    "        for section in sections:\n",
    "            # Extract tables from this section\n",
    "            tables, narrative_content = extract_and_process_tables(section.content)\n",
    "            \n",
    "            # Create section info string\n",
    "            section_info = create_section_info(section, form_type)\n",
    "            \n",
    "            # Process tables\n",
    "            for table in tables:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "            \n",
    "            # Process narrative content\n",
    "            if narrative_content.strip():\n",
    "                narrative_chunks = create_overlapping_chunks(\n",
    "                    narrative_content, target_tokens, overlap_tokens\n",
    "                )\n",
    "                \n",
    "                for chunk_data in narrative_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "        \n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "    \n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "    \n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "    \n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "    \n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\")\n",
    "print(\"‚úÖ Robust error handling and logging\")\n",
    "print(\"‚úÖ Structured data classes for better organization\")\n",
    "print(\"‚úÖ Quality validation and statistics\")\n",
    "print(\"‚úÖ Separate table and narrative processing\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb547fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 152 chunks for AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "  total_chunks: 152\n",
      "  avg_tokens: 365.07236842105266\n",
      "  min_tokens: 38\n",
      "  max_tokens: 1692\n",
      "  chunks_with_overlap: 85\n",
      "  table_chunks: 66\n",
      "  narrative_chunks: 86\n",
      "  unique_sections: 1\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "Chunk 1 (table):\n",
      "  Section: Full Document\n",
      "  Tokens: 58\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "Chunk 2 (table):\n",
      "  Section: Full Document\n",
      "  Tokens: 240\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "Chunk 3 (table):\n",
      "  Section: Full Document\n",
      "  Tokens: 41\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Test with a single file\n",
    "# =============================================================================\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        chunks = process_filing_robust(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "        \n",
    "        print(\"üìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(\"\\nüìù Sample Chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\")\n",
    "            print(f\"  Section: {chunk.section_info}\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\")\n",
    "        \n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\")\n",
    "        print(\"Please update the file path to match your data structure\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf43460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comparing Section Detection Strategies\n",
      "==================================================\n",
      "üîç Improved detection found 1 potential sections:\n",
      "  1: PART I...\n",
      "Strategy 1 (Regex): 1 sections\n",
      "  1. Part I...\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "  1. Document Content...\n"
     ]
    }
   ],
   "source": [
    "def compare_section_strategies(content_sample: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content_sample)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content_sample)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "    \n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        sample_content = f.read()[:10000]  # First 10k characters\n",
    "    \n",
    "    sections_1, sections_2 = compare_section_strategies(sample_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a7bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunking Quality Analysis\n",
      "==================================================\n",
      "Token Distribution:\n",
      "  Mean: 365.1\n",
      "  Median: 435\n",
      "  Min: 38\n",
      "  Max: 1692\n",
      "\n",
      "Chunk Types:\n",
      "  table: 66\n",
      "  narrative: 86\n",
      "\n",
      "Section Distribution:\n",
      "  Full Document: 152 chunks\n",
      "\n",
      "Overlap Analysis:\n",
      "  Chunks with overlap: 85/152 (55.9%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Chunking Quality Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "    \n",
    "    print(f\"Token Distribution:\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    print(f\"  Min: {min(token_counts)}\")\n",
    "    print(f\"  Max: {max(token_counts)}\")\n",
    "    \n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunk Types:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "    \n",
    "    # Section distribution\n",
    "    sections = {}\n",
    "    for chunk in chunks:\n",
    "        sections[chunk.section_info] = sections.get(chunk.section_info, 0) + 1\n",
    "    \n",
    "    print(f\"\\nSection Distribution:\")\n",
    "    for section, count in sorted(sections.items()):\n",
    "        print(f\"  {section}: {count} chunks\")\n",
    "    \n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fbec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 214 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 152 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 117 chunks for AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing Different Chunking Parameters\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "  Total chunks: 214\n",
      "  Avg tokens: 259.3\n",
      "  Overlap rate: 147/214\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "  Total chunks: 152\n",
      "  Avg tokens: 365.1\n",
      "  Overlap rate: 85/152\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "  Total chunks: 117\n",
      "  Avg tokens: 474.3\n",
      "  Overlap rate: 50/117\n"
     ]
    }
   ],
   "source": [
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\")\n",
    "        return\n",
    "    \n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    \n",
    "    print(\"üîß Testing Different Chunking Parameters\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\")\n",
    "        test_chunks = process_filing_robust(\n",
    "            test_file, \n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "        \n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "        \n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f9e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmp4dqo7j8s_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Testing Error Handling\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 2: Empty content\n",
      "  Result: 1 sections\n",
      "\n",
      "Test 3: Malformed filename\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 4: Very short text\n",
      "  Result: 0 chunks\n"
     ]
    }
   ],
   "source": [
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\")\n",
    "    fake_chunks = process_filing_robust(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\")\n",
    "    \n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\")\n",
    "    empty_sections = detect_sections_robust(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\")\n",
    "    \n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "    \n",
    "    bad_chunks = process_filing_robust(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\")\n",
    "    \n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "    \n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\")\n",
    "\n",
    "test_error_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 109 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 183 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 106 chunks for AMZN_10Q_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "  Total files processed: 3\n",
      "  Total chunks created: 398\n",
      "  Average chunks per file: 132.7\n",
      "\n",
      "üìã Per-file results:\n",
      "  AMZN_10Q_2022-04-29.txt: 109 chunks, 1 sections, 51 tables\n",
      "  AMZN_10Q_2020-05-01.txt: 183 chunks, 1 sections, 131 tables\n",
      "  AMZN_10Q_2020-10-30.txt: 106 chunks, 1 sections, 48 tables\n"
     ]
    }
   ],
   "source": [
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        file_chunks = process_filing_robust(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "        \n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "    \n",
    "    print(f\"  Total files processed: {len(all_results)}\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìã Per-file results:\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0680f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Final Analysis Summary\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "  ‚Ä¢ Total chunks: 152\n",
      "  ‚Ä¢ Average chunk size: 365 tokens\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "  ‚Ä¢ Overlap rate: 55.9%\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "  ‚Ä¢ narrative: 86 chunks (56.6%)\n",
      "  ‚Ä¢ table: 66 chunks (43.4%)\n",
      "\n",
      "üìö Section Breakdown:\n",
      "  ‚Ä¢ Full Document: 152 chunks\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.3%)\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (2.0%)\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "    Section: Full Document\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "    Section: Full Document\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n"
     ]
    }
   ],
   "source": [
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\")\n",
    "        return\n",
    "    \n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(chunk_data)\n",
    "    \n",
    "    print(\"üéØ Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìö Section Breakdown:\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\")\n",
    "    \n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\")\n",
    "    \n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\")\n",
    "    \n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\")\n",
    "        print(f\"    Section: {sample['section']}\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "012734b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "  4. Implement incremental processing for large datasets\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "    \n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Key Improvements:\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\")\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Recommended Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Preprocessing Strategy Testing Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next step: Convert this notebook into modular Python files\")\n",
    "print(\"Then: Implement the embedding pipeline and MCP server!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f61c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Universal SEC section detection ready!\n",
      "This should work for all your SEC filings with table-based formatting\n",
      "Replace process_filing_robust with process_filing_robust_universal\n"
     ]
    }
   ],
   "source": [
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns (most common in your files)\n",
    "        r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Table-based PART patterns\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\[TABLE_END\\]',\n",
    "        \n",
    "        # Standalone ITEM patterns (fallback)\n",
    "        r'(?i)^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)',\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Standalone PART patterns (fallback)\n",
    "        r'(?i)^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)',\n",
    "        r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Number-only patterns in tables\n",
    "        r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Headers that might appear standalone\n",
    "        r'(?i)^(Item\\s+\\d{1,2}[A-C]?\\.\\s+[^|]+?)$',\n",
    "        r'(?i)^(PART\\s+[IVX]+)(?:\\s*[-‚Äì‚Äî]\\s*(.+))?$',\n",
    "    ]\n",
    "    \n",
    "    all_matches = []\n",
    "    \n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in re.finditer(pattern, content, re.MULTILINE):\n",
    "            # Get context around the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "            \n",
    "            full_line = content[line_start:line_end].strip()\n",
    "            \n",
    "            # Skip if this looks like metadata or page headers\n",
    "            if any(skip_word in full_line.lower() for skip_word in \n",
    "                   ['page', 'signature', 'exhibit', 'index', 'table of contents']):\n",
    "                continue\n",
    "                \n",
    "            # Skip very long lines that are probably not headers\n",
    "            if len(full_line) > 500:\n",
    "                continue\n",
    "            \n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "            \n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title\n",
    "                section_title = re.sub(r'\\[TABLE_END\\].*', '', section_title).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) >= 1:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = f\"Section {section_id}\"\n",
    "            else:\n",
    "                section_id = 'unknown'\n",
    "                section_title = full_line\n",
    "            \n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates - matches within 100 characters\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 100 \n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "    \n",
    "    # Remove very similar section IDs (e.g., multiple \"1\" entries)\n",
    "    final_matches = []\n",
    "    seen_section_ids = set()\n",
    "    for match in unique_matches:\n",
    "        section_key = f\"{match['section_id'].upper()}_{match['section_title'][:20]}\"\n",
    "        if section_key not in seen_section_ids:\n",
    "            final_matches.append(match)\n",
    "            seen_section_ids.add(section_key)\n",
    "    \n",
    "    print(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        print(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "    \n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "        \n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Determine section type and metadata\n",
    "        section_id = match['section_id'].upper()\n",
    "        \n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            # This is a PART\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "            if match['section_title'] and match['section_title'] != f\"Section {section_id}\":\n",
    "                title = f\"Part {section_id} - {match['section_title']}\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            # This is an ITEM\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "            if match['section_title'] and match['section_title'] != f\"Section {section_id}\":\n",
    "                title = f\"Item {section_id} - {match['section_title']}\"\n",
    "        else:\n",
    "            # Unknown section type\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['section_title'] if match['section_title'] else match['full_line']\n",
    "        \n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Look for table of contents patterns\n",
    "    toc_patterns = [\n",
    "        r'(?i)INDEX.*?(?=\\[PAGE BREAK\\])',\n",
    "        r'(?i)TABLE OF CONTENTS.*?(?=\\[PAGE BREAK\\])',\n",
    "        r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\[PAGE BREAK\\])',\n",
    "        r'(?i)\\[TABLE_START\\].*?Page.*?\\[TABLE_END\\].*?(?=\\[PAGE BREAK\\])',\n",
    "    ]\n",
    "    \n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "    \n",
    "    if not toc_content:\n",
    "        print(\"No table of contents found\")\n",
    "        return sections\n",
    "    \n",
    "    print(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "    \n",
    "    # Extract sections from TOC using multiple patterns\n",
    "    item_patterns = [\n",
    "        # Standard table format: Item 1. | Business | Page\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+',\n",
    "        r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        # Alternative formats\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s+([^|\\d]+)',\n",
    "        r'(?i)(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+',\n",
    "    ]\n",
    "    \n",
    "    found_items = []\n",
    "    for pattern in item_patterns:\n",
    "        for match in re.finditer(pattern, toc_content):\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2:\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                # Clean up the title\n",
    "                item_title = re.sub(r'\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:20]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "    \n",
    "    print(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        print(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "    \n",
    "    return sections  # For now, just return the found items info\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "    \n",
    "    # Strategy 1: Direct pattern matching for table-formatted sections\n",
    "    sections = detect_sections_universal_sec(content)\n",
    "    \n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Universal detection successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    # Strategy 2: Table of contents analysis\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents\")\n",
    "    detect_sections_from_toc_universal(content)  # For debugging info\n",
    "    \n",
    "    # Strategy 3: Page-based fallback\n",
    "    logger.warning(\"Trying page-based detection as fallback\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "    \n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    # Final fallback\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# Universal processing function\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "        parts = file_id.split('_')\n",
    "        \n",
    "        if len(parts) != 3:\n",
    "            logger.error(f\"Invalid filename format: {filename}\")\n",
    "            return []\n",
    "        \n",
    "        ticker, form_type, filing_date_str = parts\n",
    "        \n",
    "        # Create filing metadata\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "        \n",
    "        if form_type == '10K' and filing_date.month < 4:\n",
    "            fiscal_year -= 1\n",
    "        \n",
    "        filing_metadata = FilingMetadata(\n",
    "            ticker=ticker,\n",
    "            form_type=form_type,\n",
    "            filing_date=filing_date_str,\n",
    "            fiscal_year=fiscal_year,\n",
    "            fiscal_quarter=fiscal_quarter,\n",
    "            file_path=file_path\n",
    "        )\n",
    "        \n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        \n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "        \n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "        \n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "        \n",
    "        for section in sections:\n",
    "            # Extract tables from this section\n",
    "            tables, narrative_content = extract_and_process_tables(section.content)\n",
    "            \n",
    "            # Create section info string\n",
    "            section_info = create_section_info_improved(section, form_type)\n",
    "            \n",
    "            # Process tables\n",
    "            for table in tables:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "            \n",
    "            # Process narrative content\n",
    "            if narrative_content.strip():\n",
    "                narrative_chunks = create_overlapping_chunks(\n",
    "                    narrative_content, target_tokens, overlap_tokens\n",
    "                )\n",
    "                \n",
    "                for chunk_data in narrative_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "        \n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"üöÄ Universal SEC section detection ready!\")\n",
    "print(\"This should work for all your SEC filings with table-based formatting\")\n",
    "print(\"Replace process_filing_robust with process_filing_robust_universal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e2f1a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "1. Run test_universal_detection() to test all files\n",
      "2. Run compare_old_vs_universal() to see the improvement\n",
      "3. Run quick_pattern_test() to see what patterns match\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST UNIVERSAL SEC DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def test_universal_detection():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "    \n",
    "    # Test different files to verify universal approach\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\", \n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"  # If you have this one\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüß™ Testing: {test_file}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\")\n",
    "        for i, section in enumerate(sections[:10]):  # Show first 10\n",
    "            print(f\"  {i+1}. {section.title}\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\")\n",
    "        \n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "        \n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        if chunks:\n",
    "            # Show section distribution\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:  # Sample first 20\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "            \n",
    "            print(f\"\\nüìö Section Distribution (sample):\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\")\n",
    "        return\n",
    "    \n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Old detection\n",
    "    print(\"Running old detection...\")\n",
    "    old_sections = detect_sections_robust(content)\n",
    "    \n",
    "    # New universal detection  \n",
    "    print(\"Running universal detection...\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "    \n",
    "    print(f\"\\nüìä Comparison Results:\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\")\n",
    "    \n",
    "    print(f\"\\nüìã Old Sections:\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\")\n",
    "    \n",
    "    print(f\"\\nüìã Universal Sections:\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\")\n",
    "    \n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\")\n",
    "        return\n",
    "    \n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(\"üîç QUICK PATTERN TEST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test key patterns\n",
    "    patterns = [\n",
    "        (r'\\[TABLE_START\\].*?Item.*?\\[TABLE_END\\]', \"Table-wrapped Items\"),\n",
    "        (r'Item\\s+\\d+[A-C]?\\.\\s*\\|', \"Pipe-separated Items\"),\n",
    "        (r'PART\\s+[IVX]+', \"Part headers\"),\n",
    "        (r'\\[TABLE_START\\].*?PART.*?\\[TABLE_END\\]', \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "    \n",
    "    for pattern, description in patterns:\n",
    "        matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            # Clean up match for display\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\")\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\")\n",
    "print(\"\\n1. Run test_universal_detection() to test all files\")\n",
    "print(\"2. Run compare_old_vs_universal() to see the improvement\") \n",
    "print(\"3. Run quick_pattern_test() to see what patterns match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ae8d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING FUNCTION FIX - Add this to your notebook\n",
    "# =============================================================================\n",
    "\n",
    "def create_section_info_improved(section_title: str, section_type: str = \"unknown\") -> str:\n",
    "    \"\"\"\n",
    "    Create standardized section info for chunks\n",
    "    \"\"\"\n",
    "    if not section_title or section_title.strip() == \"\":\n",
    "        return \"Full Document\"\n",
    "    \n",
    "    # Clean up the section title\n",
    "    clean_title = section_title.strip()\n",
    "    \n",
    "    # Remove redundant prefixes\n",
    "    clean_title = re.sub(r'^(Item/Part\\s+)', '', clean_title)\n",
    "    clean_title = re.sub(r'^(Part\\s+[IVX]+\\s+-\\s+)', '', clean_title)\n",
    "    \n",
    "    # Standardize format\n",
    "    if clean_title.startswith(\"Item \"):\n",
    "        return clean_title\n",
    "    elif clean_title.startswith(\"Part \"):\n",
    "        return clean_title\n",
    "    elif section_type == \"item\":\n",
    "        return f\"Item {clean_title}\"\n",
    "    elif section_type == \"part\":\n",
    "        return f\"Part {clean_title}\"\n",
    "    else:\n",
    "        return clean_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cdbd803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 19 unique sections:\n",
      "  1: Item/Part I - Item 1.    Business...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 3 - Legal Proceedings...\n",
      "  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "  7: Item/Part 6 - Selected Financial Data...\n",
      "  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part 11 - Executive Compensation...\n",
      "  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "  1. Part I - Item 1.    Business\n",
      "     Type: part, Length: 13,274 chars\n",
      "  2. Item 1A - Risk Factors\n",
      "     Type: item, Length: 61,136 chars\n",
      "  3. Item 1B - Unresolved Staff Comments\n",
      "     Type: item, Length: 582 chars\n",
      "  4. Item 3 - Legal Proceedings\n",
      "     Type: item, Length: 898 chars\n",
      "  5. Item 4 - Mine Safety Disclosures\n",
      "     Type: item, Length: 99 chars\n",
      "  6. Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "     Type: part, Length: 4,191 chars\n",
      "  7. Item 6 - Selected Financial Data\n",
      "     Type: item, Length: 1,745 chars\n",
      "  8. Item 7 - Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 33,154 chars\n",
      "  9. Item 7A - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 6,799 chars\n",
      "  10. Item 8 - Financial Statements and Supplementary Data\n",
      "     Type: item, Length: 103,042 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 20 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 9 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 20 unique sections:\n",
      "  1: Item/Part I - [TABLE_START]...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 2 - Properties...\n",
      "  5: Item/Part 3 - Legal Proceedings...\n",
      "  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "  7: Item/Part II - [TABLE_START]...\n",
      "  8: Item/Part 6 - Reserved...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part III - [TABLE_START]...\n",
      "  15: Item/Part 11 - Executive Compensation...\n",
      "\n",
      "‚úÖ Found 20 sections:\n",
      "  1. Part I - [TABLE_START]\n",
      "     Type: part, Length: 13,293 chars\n",
      "  2. Item 1A - Risk Factors\n",
      "     Type: item, Length: 55,960 chars\n",
      "  3. Item 1B - Unresolved Staff Comments\n",
      "     Type: item, Length: 106 chars\n",
      "  4. Item 2 - Properties\n",
      "     Type: item, Length: 1,437 chars\n",
      "  5. Item 3 - Legal Proceedings\n",
      "     Type: item, Length: 185 chars\n",
      "  6. Item 4 - Mine Safety Disclosures\n",
      "     Type: item, Length: 113 chars\n",
      "  7. Part II - [TABLE_START]\n",
      "     Type: part, Length: 516 chars\n",
      "  8. Item 6 - Reserved\n",
      "     Type: item, Length: 50,497 chars\n",
      "  9. Item 7A - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 6,524 chars\n",
      "  10. Item 8 - Financial Statements and Supplementary Data\n",
      "     Type: item, Length: 123,990 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 9 unique sections:\n",
      "  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "  2: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  4: Item/Part 4 - Controls and Procedures...\n",
      "  5: Item/Part II - . OTHER INFORMATION...\n",
      "  6: Item/Part 1A - Risk Factors...\n",
      "  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "  8: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "  9: Item/Part 5 - Other Information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 7 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "ERROR:__main__:Error processing processed_filings/KO/KO_10Q_2020-07-22.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 25 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 9 sections:\n",
      "  1. Part I - . FINANCIAL INFORMATION\n",
      "     Type: part, Length: 67,088 chars\n",
      "  2. Item 2 - Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 45,106 chars\n",
      "  3. Item 3 - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 4,404 chars\n",
      "  4. Item 4 - Controls and Procedures\n",
      "     Type: item, Length: 2,075 chars\n",
      "  5. Part II - . OTHER INFORMATION\n",
      "     Type: part, Length: 189 chars\n",
      "  6. Item 1A - Risk Factors\n",
      "     Type: item, Length: 59,432 chars\n",
      "  7. Item 2 - Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "     Type: item, Length: 102 chars\n",
      "  8. Item 3 - Defaults Upon Senior Securities\n",
      "     Type: item, Length: 152 chars\n",
      "  9. Item 5 - Other Information\n",
      "     Type: item, Length: 5,327 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 7 unique sections:\n",
      "  1: Item/Part I - . Financial Information...\n",
      "  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  4: Item/Part 4 - Controls and Procedures...\n",
      "  5: Item/Part II - . Other Information...\n",
      "  6: Item/Part 1A - Risk Factors...\n",
      "  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "\n",
      "‚úÖ Found 7 sections:\n",
      "  1. Part I - . Financial Information\n",
      "     Type: part, Length: 115,924 chars\n",
      "  2. Item 2 - Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 87,923 chars\n",
      "  3. Item 3 - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 207 chars\n",
      "  4. Item 4 - Controls and Procedures\n",
      "     Type: item, Length: 1,004 chars\n",
      "  5. Part II - . Other Information\n",
      "     Type: part, Length: 248 chars\n",
      "  6. Item 1A - Risk Factors\n",
      "     Type: item, Length: 11,661 chars\n",
      "  7. Item 2 - Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "     Type: item, Length: 16,047 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections |   0 chunks\n",
      "AMZN_10K_2023-02-03.txt   | 20 sections |   0 chunks\n",
      "AMZN_10Q_2024-11-01.txt   |  9 sections |   0 chunks\n",
      "KO_10Q_2020-07-22.txt     |  7 sections |   0 chunks\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "============================================================\n",
      "Running old detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 19 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running universal detection...\n",
      "üîç Universal SEC detection found 19 unique sections:\n",
      "  1: Item/Part I - Item 1.    Business...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 3 - Legal Proceedings...\n",
      "  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "  7: Item/Part 6 - Selected Financial Data...\n",
      "  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part 11 - Executive Compensation...\n",
      "  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "\n",
      "üìä Comparison Results:\n",
      "  Old detection: 25 sections\n",
      "  Universal detection: 19 sections\n",
      "  Improvement: +-6 sections\n",
      "\n",
      "üìã Old Sections:\n",
      "  1. PART I\n",
      "  2. Item 1.    Business\n",
      "  3. Item 1A.    Risk Factors\n",
      "  4. Item 1B.    Unresolved Staff Comments\n",
      "  5. Item 2.    Properties\n",
      "  6. Item 3.    Legal Proceedings\n",
      "  7. Item 4.    Mine Safety Disclosures\n",
      "  8. PART II\n",
      "  9. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "  10. Item 6.    Selected Financial Data\n",
      "  11. Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "  12. Item 7A.    Quantitative and Qualitative Disclosures About Market Risk\n",
      "  13. Item 8.    Financial Statements and Supplementary Data\n",
      "  14. Item 9.    Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "  15. Item 9A.    Controls and Procedures\n",
      "  16. Item 9B.    Other Information\n",
      "  17. PART III\n",
      "  18. Item 10.    Directors, Executive Officers and Corporate Governance\n",
      "  19. Item 11.    Executive Compensation\n",
      "  20. Item 12.    Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "  21. Item 13.    Certain Relationships and Related Transactions, and Director Independence\n",
      "  22. Item 14.    Principal Accountant Fees and Services\n",
      "  23. PART IV\n",
      "  24. Item 15.    Exhibit and Financial Statement Schedules\n",
      "  25. Item 16.    Form 10-K Summary\n",
      "\n",
      "üìã Universal Sections:\n",
      "  1. Part I - Item 1.    Business\n",
      "  2. Item 1A - Risk Factors\n",
      "  3. Item 1B - Unresolved Staff Comments\n",
      "  4. Item 3 - Legal Proceedings\n",
      "  5. Item 4 - Mine Safety Disclosures\n",
      "  6. Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "  7. Item 6 - Selected Financial Data\n",
      "  8. Item 7 - Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "  9. Item 7A - Quantitative and Qualitative Disclosures About Market Risk\n",
      "  10. Item 8 - Financial Statements and Supplementary Data\n",
      "  11. Item 9 - Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "  12. Item 9A - Controls and Procedures\n",
      "  13. Item 9B - Other Information\n",
      "  14. Item 11 - Executive Compensation\n",
      "  15. Item 12 - Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "  16. Item 13 - Certain Relationships and Related Transactions, and Director Independence\n",
      "  17. Item 14 - Principal Accountant Fees and Services\n",
      "  18. Section PART IV\n",
      "  19. Item 16 - Form 10-K Summary\n",
      "üîç QUICK PATTERN TEST\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "  1: Item 1. |...\n",
      "  2: Item 1A. |...\n",
      "  3: Item 1B. |...\n",
      "\n",
      "Part headers: 33 matches\n",
      "  1: Part III...\n",
      "  2: Part I...\n",
      "  3: Part II...\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n"
     ]
    }
   ],
   "source": [
    "# Test the universal detection\n",
    "results = test_universal_detection()\n",
    "\n",
    "# Compare old vs new\n",
    "old_sections, new_sections = compare_old_vs_universal()\n",
    "\n",
    "# See what patterns actually match\n",
    "quick_pattern_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7738a7e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChunkWithMetadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     29\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m clean_title\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_filing_robust_universal\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[43mChunkWithMetadata\u001b[49m]:\n\u001b[32m     32\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    Updated processing function with the missing function included\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'ChunkWithMetadata' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MISSING FUNCTION FIX - Add this to your notebook\n",
    "# =============================================================================\n",
    "\n",
    "def create_section_info_improved(section_title: str, section_type: str = \"unknown\") -> str:\n",
    "    \"\"\"\n",
    "    Create standardized section info for chunks\n",
    "    \"\"\"\n",
    "    if not section_title or section_title.strip() == \"\":\n",
    "        return \"Full Document\"\n",
    "    \n",
    "    # Clean up the section title\n",
    "    clean_title = section_title.strip()\n",
    "    \n",
    "    # Remove redundant prefixes\n",
    "    clean_title = re.sub(r'^(Item/Part\\s+)', '', clean_title)\n",
    "    clean_title = re.sub(r'^(Part\\s+[IVX]+\\s+-\\s+)', '', clean_title)\n",
    "    \n",
    "    # Standardize format\n",
    "    if clean_title.startswith(\"Item \"):\n",
    "        return clean_title\n",
    "    elif clean_title.startswith(\"Part \"):\n",
    "        return clean_title\n",
    "    elif section_type == \"item\":\n",
    "        return f\"Item {clean_title}\"\n",
    "    elif section_type == \"part\":\n",
    "        return f\"Part {clean_title}\"\n",
    "    else:\n",
    "        return clean_title\n",
    "\n",
    "def process_filing_robust_universal(file_path: str) -> List[ChunkWithMetadata]:\n",
    "    \"\"\"\n",
    "    Updated processing function with the missing function included\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing {file_path}\")\n",
    "        \n",
    "        # Read file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_metadata_from_filename(file_path)\n",
    "        \n",
    "        # Detect sections using universal approach\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        \n",
    "        for section in sections:\n",
    "            # Create chunks for this section\n",
    "            section_chunks = create_chunks_from_section(\n",
    "                section.content, \n",
    "                chunk_size=512, \n",
    "                overlap=0.2\n",
    "            )\n",
    "            \n",
    "            # Add metadata to each chunk\n",
    "            for chunk in section_chunks:\n",
    "                chunk_with_meta = ChunkWithMetadata(\n",
    "                    content=chunk.content,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        'section_info': create_section_info_improved(section.title, section.section_type),\n",
    "                        'section_type': section.section_type,\n",
    "                        'start_pos': chunk.start_pos,\n",
    "                        'end_pos': chunk.end_pos,\n",
    "                        'chunk_index': len(all_chunks),\n",
    "                        'token_count': chunk.token_count,\n",
    "                        'content_type': chunk.content_type\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(chunk_with_meta)\n",
    "        \n",
    "        return all_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Test the fix\n",
    "def test_fixed_processing():\n",
    "    \"\"\"Test that the missing function fix works\"\"\"\n",
    "    \n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\"\n",
    "    ]\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüß™ Testing FIXED processing: {test_file}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Process with fixed function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        \n",
    "        if chunks:\n",
    "            print(f\"‚úÖ SUCCESS: Created {len(chunks)} chunks!\")\n",
    "            \n",
    "            # Show chunk distribution by section\n",
    "            section_counts = {}\n",
    "            for chunk in chunks:\n",
    "                section = chunk.metadata.get('section_info', 'Unknown')\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "            \n",
    "            print(f\"\\nüìö Chunk Distribution by Section:\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "            \n",
    "            # Show sample chunks\n",
    "            print(f\"\\nüìÑ Sample Chunks:\")\n",
    "            for i, chunk in enumerate(chunks[:3]):\n",
    "                section = chunk.metadata.get('section_info', 'Unknown')\n",
    "                content_preview = ' '.join(chunk.content.split()[:15])\n",
    "                print(f\"  {i+1}. [{section}] {content_preview}...\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå FAILED: No chunks created\")\n",
    "    \n",
    "    return chunks if 'chunks' in locals() else []\n",
    "\n",
    "print(\"üöÄ Missing function fix added!\")\n",
    "print(\"\\nRun test_fixed_processing() to verify the fix works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "940829a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: unbalanced parenthesis at position 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "‚úÖ Robust error handling and logging\n",
      "‚úÖ Structured data classes for better organization\n",
      "‚úÖ Quality validation and statistics\n",
      "‚úÖ Separate table and narrative processing\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "==================================================\n",
      "üîç Universal SEC detection found 0 unique sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmparuw5yik_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2022-04-29.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2020-05-01.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2020-10-30.txt: unbalanced parenthesis at position 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üìù Sample Chunks:\n",
      "No test file processed yet\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 2: Empty content\n",
      "Empty content provided to detect_sections_universal_sec\n",
      "Empty content provided to detect_sections_from_toc_universal\n",
      "  Result: 1 sections\n",
      "\n",
      "Test 3: Malformed filename\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 4: Very short text\n",
      "  Result: 0 chunks\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 0\n",
      "  Average chunks per file: 0.0\n",
      "\n",
      "üìã Per-file results:\n",
      "  AMZN_10Q_2022-04-29.txt: 0 chunks, 0 sections, 0 tables\n",
      "  AMZN_10Q_2020-05-01.txt: 0 chunks, 0 sections, 0 tables\n",
      "  AMZN_10Q_2020-10-30.txt: 0 chunks, 0 sections, 0 tables\n",
      "üìà Final Analysis Summary\n",
      "============================================================\n",
      "No chunks to analyze - run test_single_file() first\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "  4. Implement incremental processing for large datasets\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_universal_detection_fixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1244\u001b[39m\n\u001b[32m   1241\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m2. Run compare_old_vs_universal_fixed() to see the improvement\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1242\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m3. Run quick_pattern_test_fixed() to see what patterns match\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m results_universal = \u001b[43mtest_universal_detection_fixed\u001b[49m()\n\u001b[32m   1245\u001b[39m old_vs_new_sections = compare_old_vs_universal_fixed()\n\u001b[32m   1246\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[31mNameError\u001b[39m: name 'test_universal_detection_fixed' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        r'(?im)^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$',\n",
    "        r'(?im)^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$',\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        r'(?im)^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        r'(?im)^ITEM\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        r'(?im)Item\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        r'(?im)^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}',\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        r'(?im)^.{0,50}(BUSINESS)\\s*$',\n",
    "        r'(?im)^.{0,50}(RISK FACTORS)\\s*$',\n",
    "        r'(?im)^.{0,50}(LEGAL PROCEEDINGS)\\s*$',\n",
    "        r'(?im)^.{0,50}(FINANCIAL STATEMENTS)\\s*$',\n",
    "        r'(?im)^.{0,50}(MANAGEMENT.S DISCUSSION)\\\\s*',\n",
    "        r'(?im)^.{0,50}(PROPERTIES)\\s*$',\n",
    "        r'(?im)^.{0,50}(CONTROLS AND PROCEDURES)\\s*$',\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in re.finditer(pattern, content):\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # Good result\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "\n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "\n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # ADD THIS CHECK\n",
    "        print(\"Empty content provided to detect_sections_universal_sec\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns (most common in your files)\n",
    "        r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "\n",
    "        # Table-based PART patterns\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\[TABLE_END\\]',\n",
    "\n",
    "        # Standalone ITEM patterns (fallback)\n",
    "        r'(?i)^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)',\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "\n",
    "        # Standalone PART patterns (fallback)\n",
    "        r'(?i)^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)',\n",
    "        r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "\n",
    "        # Number-only patterns in tables\n",
    "        r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "\n",
    "        # Headers that might appear standalone\n",
    "        r'(?i)^(Item\\\\s+\\\\d{1,2}[A-C]?\\\\.\\\\s+[^|]+?)$',\n",
    "        r'(?i)^(PART\\\\s+[IVX]+)(?:\\\\s*[-‚Äì‚Äî]\\\\s*(.+))?$',\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in re.finditer(pattern, content, re.MULTILINE):\n",
    "            # Get context around the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Skip if this looks like metadata or page headers\n",
    "            if any(skip_word in full_line.lower() for skip_word in\n",
    "                   ['page', 'signature', 'exhibit', 'index', 'table of contents']):\n",
    "                continue\n",
    "\n",
    "            # Skip very long lines that are probably not headers\n",
    "            if len(full_line) > 500:\n",
    "                continue\n",
    "\n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "\n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title\n",
    "                section_title = re.sub(r'\\[TABLE_END\\].*', '', section_title).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) >= 1:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = f\"Section {section_id}\" # Default title if no specific title captured\n",
    "            else:\n",
    "                section_id = 'unknown'\n",
    "                section_title = full_line # Fallback to full line as title\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 100 characters\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 100\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Remove very similar section IDs (e.g., multiple \"1\" entries)\n",
    "    final_matches = []\n",
    "    seen_section_ids = set()\n",
    "    for match in unique_matches:\n",
    "        section_key = f\"{match['section_id'].upper()}_{match['section_title'][:20]}\"\n",
    "        if section_key not in seen_section_ids:\n",
    "            final_matches.append(match)\n",
    "            seen_section_ids.add(section_key)\n",
    "\n",
    "    print(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        print(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        section_id = match['section_id'].upper()\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            # This is a PART\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "            if match['section_title'] and match['section_title'] != f\"Section {section_id}\":\n",
    "                title = f\"Part {section_id} - {match['section_title']}\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            # This is an ITEM\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "            if match['section_title'] and match['section_title'] != f\"Section {section_id}\":\n",
    "                title = f\"Item {section_id} - {match['section_title']}\"\n",
    "        else:\n",
    "            # Unknown section type\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['section_title'] if match['section_title'] else match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # ADD THIS CHECK\n",
    "        print(\"Empty content provided to detect_sections_from_toc_universal\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    toc_patterns = [\n",
    "        r'(?i)INDEX.*?(?=\\\\[PAGE BREAK\\\\])',\n",
    "        r'(?i)TABLE OF CONTENTS.*?(?=\\\\[PAGE BREAK\\\\])',\n",
    "        r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\\\[PAGE BREAK\\\\])', # Including INDEX as it's common\n",
    "        r'(?i)\\[TABLE_START\\].*?Page.*?\\\\[TABLE_END\\].*?(?=\\\\[PAGE BREAK\\\\])',\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        print(\"No table of contents found\")\n",
    "        return sections # Return empty list if no TOC found\n",
    "\n",
    "    print(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # ... (rest of the function remains the same)\n",
    "    item_patterns = [\n",
    "        # Standard table format: Item 1. | Business | Page\n",
    "        r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+',\n",
    "        r'(?i)PART\\\\s+([IVX]+)\\\\s*\\\\|\\\\s*([^|]+)',\n",
    "        # Alternative formats\n",
    "        r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s+([^|\\\\d]+)',\n",
    "        r'(?i)(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+',\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    # This loop also needs to handle empty toc_content if previous steps fail\n",
    "    if not toc_content: # Defensive check\n",
    "        return sections\n",
    "\n",
    "    for pattern in item_patterns:\n",
    "        for match in re.finditer(pattern, toc_content): # Use toc_content here\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2:\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "\n",
    "    # ... (rest of the function for unique_items and DocumentSection creation)\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:20]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    print(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        print(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'item' if re.match(r'^\\d+[A-C]?$', item_id) else ('part' if re.match(r'^[IVX]+$', item_id) else 'unknown')\n",
    "        item_number = item_id if section_type == 'item' else None\n",
    "        part_num = item_id if section_type == 'part' else None\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\",\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:20]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    print(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        print(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    # Convert extracted TOC items into DocumentSection objects\n",
    "    toc_sections = []\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'item' if re.match(r'^\\d+[A-C]?$', item_id) else ('part' if re.match(r'^[IVX]+$', item_id) else 'unknown')\n",
    "        item_number = item_id if section_type == 'item' else None\n",
    "        part_num = item_id if section_type == 'part' else None\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is empty, as it's just from TOC. Needs actual content extraction later.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching, then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (more robust than previous strategy_1_improved on its own)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3: # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents. If successful, use these as sections.\n",
    "    # Note: This strategy only extracts titles/item_numbers, not content.\n",
    "    # If this strategy is chosen, you would need a mechanism to map these titles back\n",
    "    # to the main document to extract actual content.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents\")\n",
    "    sections_toc = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if sections_toc and len(sections_toc) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(sections_toc)} potential sections. Will attempt to map content to them.\")\n",
    "        # This is where a sophisticated mapping from TOC titles to content blocks would go.\n",
    "        # For simplicity in this structure, if TOC looks good, we try to use it to\n",
    "        # derive actual content sections. A simple approach is to use regex matching\n",
    "        # from the TOC titles to split the original document.\n",
    "\n",
    "        # For this example, let's try a simple re-split based on TOC titles if available\n",
    "        # This can be complex and might need further refinement based on document structure.\n",
    "        # A quick way to leverage TOC is to combine with simple regex/page split\n",
    "        # or use it to re-label existing sections.\n",
    "        \n",
    "        # Given the previous output, `detect_sections_universal_sec` seems to be the primary workhorse,\n",
    "        # and TOC as a fallback/enhancement. Let's stick to the flow in the prompt where\n",
    "        # if strategy 1 is not good, it tries strategy 2 (page-based), and if that's not good,\n",
    "        # it just creates one big chunk.\n",
    "        # The prompt's logging showed \"No table of contents found\" and then \"All strategies failed\".\n",
    "        # This implies sections_toc is often empty or not used for content splitting.\n",
    "        pass # The logic below will handle fallbacks if TOC is not used for primary sectioning.\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        # Fallback for malformed filenames, though process_filing_robust_universal checks this\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    if form_type == '10K' and filing_date.month < 4:\n",
    "        fiscal_year -= 1\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        # Moved metadata extraction here as it's robust and used early\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            # This is the original, more robust create_section_info which expects DocumentSection\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\")\n",
    "print(\"‚úÖ Robust error handling and logging\")\n",
    "print(\"‚úÖ Structured data classes for better organization\")\n",
    "print(\"‚úÖ Quality validation and statistics\")\n",
    "print(\"‚úÖ Separate table and narrative processing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\")\n",
    "            print(f\"  Section: {chunk.section_info}\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\")\n",
    "        print(\"Please update the file path to match your data structure\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    print(f\"  Min: {min(token_counts)}\")\n",
    "    print(f\"  Max: {max(token_counts)}\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\")\n",
    "        print(f\"    Section: {sample['section']}\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test the universal detection (adapted from your existing test_universal_detection)\n",
    "# This part of the code was duplicated/re-defined in the ipynb.\n",
    "# I'm placing the call here as it was in your original structure.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\")\n",
    "\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8d1e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: unbalanced parenthesis at position 65\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmp2i0ej49u_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2022-04-29.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2020-05-01.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2020-10-30.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 19 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 4 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: unbalanced parenthesis at position 65\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "‚úÖ Robust error handling and logging\n",
      "‚úÖ Structured data classes for better organization\n",
      "‚úÖ Quality validation and statistics\n",
      "‚úÖ Separate table and narrative processing\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "==================================================\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üìù Sample Chunks:\n",
      "No test file processed yet\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 2: Empty content\n",
      "  Result: 1 sections\n",
      "\n",
      "Test 3: Malformed filename\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 4: Very short text\n",
      "  Result: 0 chunks\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 0\n",
      "  Average chunks per file: 0.0\n",
      "\n",
      "üìã Per-file results:\n",
      "  AMZN_10Q_2022-04-29.txt: 0 chunks, 0 sections, 0 tables\n",
      "  AMZN_10Q_2020-05-01.txt: 0 chunks, 0 sections, 0 tables\n",
      "  AMZN_10Q_2020-10-30.txt: 0 chunks, 0 sections, 0 tables\n",
      "üìà Final Analysis Summary\n",
      "============================================================\n",
      "No chunks to analyze - run test_single_file() first\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "  4. Implement incremental processing for large datasets\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 19 unique sections:\n",
      "  1: Item/Part I - Item 1.    Business...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 3 - Legal Proceedings...\n",
      "  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "  7: Item/Part 6 - Selected Financial Data...\n",
      "  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part 11 - Executive Compensation...\n",
      "  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "  1. Item 1.    Business\n",
      "     Type: part, Length: 13,274 chars\n",
      "  2. Risk Factors\n",
      "     Type: item, Length: 61,136 chars\n",
      "  3. Unresolved Staff Comments\n",
      "     Type: item, Length: 582 chars\n",
      "  4. Legal Proceedings\n",
      "     Type: item, Length: 898 chars\n",
      "  5. Mine Safety Disclosures\n",
      "     Type: item, Length: 99 chars\n",
      "  6. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "     Type: part, Length: 4,191 chars\n",
      "  7. Selected Financial Data\n",
      "     Type: item, Length: 1,745 chars\n",
      "  8. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 33,154 chars\n",
      "  9. Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 6,799 chars\n",
      "  10. Financial Statements and Supplementary Data\n",
      "     Type: item, Length: 103,042 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 4 unique sections:\n",
      "  1: Item/Part I - [TABLE_START]...\n",
      "  2: Item/Part II - [TABLE_START]...\n",
      "  3: Item/Part III - [TABLE_START]...\n",
      "  4: Item/Part IV - [TABLE_START]...\n",
      "\n",
      "‚úÖ Found 4 sections:\n",
      "  1. [TABLE_START]\n",
      "     Type: part, Length: 71,104 chars\n",
      "  2. [TABLE_START]\n",
      "     Type: part, Length: 189,316 chars\n",
      "  3. [TABLE_START]\n",
      "     Type: part, Length: 2,224 chars\n",
      "  4. [TABLE_START]\n",
      "     Type: part, Length: 10,492 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 2 unique sections:\n",
      "  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "  2: Item/Part II - . OTHER INFORMATION...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis at position 65",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1560\u001b[39m\n\u001b[32m   1557\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_match\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1559\u001b[39m \u001b[38;5;66;03m# Run the fixed tests\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m results_universal = \u001b[43mtest_universal_detection_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m old_vs_new_sections = compare_old_vs_universal_fixed()\n\u001b[32m   1562\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1449\u001b[39m, in \u001b[36mtest_universal_detection_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1446\u001b[39m     content = f.read()\n\u001b[32m   1448\u001b[39m \u001b[38;5;66;03m# Test universal detection\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1449\u001b[39m sections = \u001b[43mdetect_sections_robust_universal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, section \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sections[:\u001b[32m10\u001b[39m]):  \u001b[38;5;66;03m# Show first 10\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 643\u001b[39m, in \u001b[36mdetect_sections_robust_universal\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# Strategy 2: Try parsing Table of Contents. If successful and provides many sections,\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;66;03m# it indicates a structured document. We'll use these titles and re-scan the document\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[38;5;66;03m# for their content.\u001b[39;00m\n\u001b[32m    642\u001b[39m logger.warning(\u001b[33m\"\u001b[39m\u001b[33mDirect detection found few sections, analyzing table of contents.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m toc_entries = \u001b[43mdetect_sections_from_toc_universal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# These are DocumentSections with only title/metadata, no content\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m toc_entries \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(toc_entries) >= \u001b[32m3\u001b[39m: \u001b[38;5;66;03m# If TOC parsing yielded a good number of entries\u001b[39;00m\n\u001b[32m    646\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTOC analysis found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(toc_entries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m potential sections. Attempting to extract content based on TOC titles.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 542\u001b[39m, in \u001b[36mdetect_sections_from_toc_universal\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sections\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Look for table of contents patterns\u001b[39;00m\n\u001b[32m    538\u001b[39m toc_patterns = [\n\u001b[32m    539\u001b[39m     re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?i)INDEX.*?(?=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m[PAGE BREAK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, re.DOTALL),\n\u001b[32m    540\u001b[39m     re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?i)TABLE OF CONTENTS.*?(?=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m[PAGE BREAK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, re.DOTALL),\n\u001b[32m    541\u001b[39m     re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?i)FORM 10-[KQ].*?INDEX.*?(?=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m[PAGE BREAK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, re.DOTALL), \u001b[38;5;66;03m# Common variant\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(?i)\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m[TABLE_START\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m].*?Page.*?\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m[TABLE_END\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m].*?(?=\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m[PAGE BREAK\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m])\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOTALL\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    543\u001b[39m ]\n\u001b[32m    545\u001b[39m toc_content = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m toc_patterns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/__init__.py:227\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(pattern, flags)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile\u001b[39m(pattern, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    226\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/__init__.py:294\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(pattern, flags)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    289\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mas it is an undocumented flag \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mwithout an obvious purpose. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mDon\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    293\u001b[39m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m p = \u001b[43m_compiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags & DEBUG):\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) >= _MAXCACHE:\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/_compiler.py:745\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(p, flags)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[32m    744\u001b[39m     pattern = p\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     p = \u001b[43m_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    747\u001b[39m     pattern = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/_parser.py:994\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(str, flags, state)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source.next \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m source.next == \u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m source.error(\u001b[33m\"\u001b[39m\u001b[33munbalanced parenthesis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p.state.grouprefpos:\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m g >= p.state.groups:\n",
      "\u001b[31merror\u001b[39m: unbalanced parenthesis at position 65"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "\n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "\n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns (most common in your files)\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Table-based PART patterns\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\[TABLE_END\\]', re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (fallback)\n",
    "        re.compile(r'(?i)^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)', re.M),\n",
    "        re.compile(r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (fallback)\n",
    "        re.compile(r'(?i)^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)', re.M),\n",
    "        re.compile(r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Number-only patterns in tables\n",
    "        re.compile(r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        # Headers that might appear standalone\n",
    "        re.compile(r'(?i)^(Item\\\\s+\\\\d{1,2}[A-C]?\\\\.\\\\s+[^|]+?)$', re.M),\n",
    "        re.compile(r'^(PART\\\\s+[IVX]+)(?:\\\\s*[-‚Äì‚Äî]\\\\s*(.+))?$', re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get context around the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "\n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title\n",
    "                section_title = re.sub(r'\\[TABLE_END\\].*', '', section_title).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) >= 1:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = f\"Section {section_id}\" # Default title if no specific title captured\n",
    "            else:\n",
    "                section_id = 'unknown'\n",
    "                section_title = full_line # Fallback to full line as title\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(), # Use match.start() for more precise position of regex match\n",
    "                'end_pos': match.end(),     # Use match.end()\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches by their start position\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    # Remove duplicates - matches within 100 characters (adjusted logic for overlap)\n",
    "    unique_matches = []\n",
    "    if all_matches:\n",
    "        unique_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            # If current match is far enough from previous unique match, add it\n",
    "            if all_matches[i]['start_pos'] - unique_matches[-1]['start_pos'] > 100:\n",
    "                unique_matches.append(all_matches[i])\n",
    "            # Else, if the new match is a 'better' match (e.g., lower pattern_idx indicates higher priority pattern)\n",
    "            # This logic can be more complex, but for now a simple distance check is good.\n",
    "            # Or if it's a \"cleaner\" item/part header over a generic one, replace\n",
    "            elif all_matches[i]['section_id'] != 'unknown' and unique_matches[-1]['section_id'] == 'unknown':\n",
    "                 unique_matches[-1] = all_matches[i]\n",
    "\n",
    "    # Further refine unique matches by avoiding similar section IDs too close if not from primary patterns\n",
    "    final_matches = []\n",
    "    seen_primary_sections = {} # For \"Item X\" and \"Part Y\" that are typically unique\n",
    "    for match_data in unique_matches:\n",
    "        section_id_upper = match_data['section_id'].upper()\n",
    "        if re.match(r'^(ITEM|PART)\\s', match_data['full_line'].upper()): # If it's a strong ITEM/PART header\n",
    "            if section_id_upper not in seen_primary_sections:\n",
    "                final_matches.append(match_data)\n",
    "                seen_primary_sections[section_id_upper] = match_data['start_pos']\n",
    "            else:\n",
    "                # If a primary section ID is repeated, pick the earlier one or a \"better\" one\n",
    "                # For simplicity, we just keep the first one found unless a significant distance\n",
    "                if match_data['start_pos'] - seen_primary_sections[section_id_upper] > 500: # New occurrence of the same item far away\n",
    "                     final_matches.append(match_data)\n",
    "                     seen_primary_sections[section_id_upper] = match_data['start_pos']\n",
    "        else: # For other types of matches, add if not too close to the last added match\n",
    "            if not final_matches or (match_data['start_pos'] - final_matches[-1]['start_pos'] > 100):\n",
    "                final_matches.append(match_data)\n",
    "\n",
    "    print(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        print(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    # This loop requires calculating end_pos based on the *next* detected section's start_pos\n",
    "    final_document_sections = []\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # End position is the start of the next matched section, or end of content if it's the last one\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title'] # Use the extracted section title\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            if title == f\"Section {section_id}\": # If it's just a generic \"Section X\" title\n",
    "                title = part # Use the standardized PART title\n",
    "            elif not title: # If no specific title was captured\n",
    "                 title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            if title == f\"Section {section_id}\": # If it's just a generic \"Section X\" title\n",
    "                title = f\"Item {item_number}\" # Use the standardized ITEM title\n",
    "            elif not title: # If no specific title was captured\n",
    "                 title = f\"Item {item_number}\"\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL), # Common variant\n",
    "        re.compile(r'(?i)\\[TABLE_START\\].*?Page.*?\\\\[TABLE_END\\].*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content) # Use pre-compiled pattern\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections # Return empty list if no TOC found\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    item_patterns = [\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL), # Standard table format\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)\\\\s*\\\\|\\\\s*([^|]+)', re.DOTALL), # Part in table\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s+([^|\\\\d]+)', re.M), # Standalone Item line\n",
    "        re.compile(r'(?i)(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL), # Number-dot item in table\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)', re.M) # Simple PART line\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    for pattern in item_patterns:\n",
    "        for match in pattern.finditer(toc_content): # Use pre-compiled pattern and toc_content\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2: # Pattern captured both ID and Title\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title) # Normalize whitespace\n",
    "                found_items.append((item_id, item_title))\n",
    "            elif len(groups) == 1: # Pattern only captured ID (e.g., simple PART)\n",
    "                item_id = groups[0].strip()\n",
    "                # Attempt to get text immediately following the item_id if available, otherwise use a generic title\n",
    "                # This makes the TOC parsing more robust for less structured TOCs.\n",
    "                remaining_text = toc_content[match.end():].split('\\n')[0].strip()\n",
    "                if remaining_text:\n",
    "                    item_title = remaining_text\n",
    "                else:\n",
    "                    item_title = f\"Section {item_id}\"\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:50]}\" # Use a longer slice for better uniqueness\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = item_id\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is intentionally empty here; will be filled by main sectioning if this strategy is chosen.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3: # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents. If successful and provides many sections,\n",
    "    # it indicates a structured document. We'll use these titles and re-scan the document\n",
    "    # for their content.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "        # This is a critical step: we need to use the TOC titles to find the actual content.\n",
    "        # A robust way is to dynamically build regex patterns from the TOC titles\n",
    "        # and search for them in the main content.\n",
    "\n",
    "        combined_sections = []\n",
    "        last_end_pos = 0\n",
    "\n",
    "        # Create regex patterns from TOC titles, prioritizing item/part numbers if available\n",
    "        # Example: \"Item 1A. Risk Factors\" or \"PART I. FINANCIAL INFORMATION\"\n",
    "        patterns_from_toc = []\n",
    "        for entry in toc_entries:\n",
    "            if entry.item_number:\n",
    "                patterns_from_toc.append(re.escape(f\"Item {entry.item_number}\").replace('\\\\ ', '\\\\s*'))\n",
    "            elif entry.part:\n",
    "                patterns_from_toc.append(re.escape(f\"PART {entry.part}\").replace('\\\\ ', '\\\\s*'))\n",
    "            else:\n",
    "                patterns_from_toc.append(re.escape(entry.title).replace('\\\\ ', '\\\\s*')) # Escape for regex use\n",
    "\n",
    "        # Combine patterns with flexible whitespace and make sure they match start of line\n",
    "        full_pattern = '|'.join(f'(?:^\\\\s*{p})' for p in patterns_from_toc if p)\n",
    "        if full_pattern:\n",
    "            compiled_full_pattern = re.compile(full_pattern, re.I | re.M)\n",
    "            \n",
    "            matches = list(compiled_full_pattern.finditer(content))\n",
    "            \n",
    "            for i, match in enumerate(matches):\n",
    "                start_pos = match.start()\n",
    "                end_pos = matches[i+1].start() if i+1 < len(matches) else len(content)\n",
    "                \n",
    "                section_content = content[start_pos:end_pos].strip()\n",
    "                \n",
    "                # Try to map this content block back to a TOC entry\n",
    "                # This is heuristic and might need refinement\n",
    "                matched_toc_entry = None\n",
    "                for entry in toc_entries:\n",
    "                    if entry.item_number and f\"Item {entry.item_number}\".upper() in match.group(0).upper():\n",
    "                        matched_toc_entry = entry\n",
    "                        break\n",
    "                    elif entry.part and f\"PART {entry.part}\".upper() in match.group(0).upper():\n",
    "                        matched_toc_entry = entry\n",
    "                        break\n",
    "                    elif entry.title.upper() in match.group(0).upper():\n",
    "                        matched_toc_entry = entry\n",
    "                        break\n",
    "                \n",
    "                if matched_toc_entry:\n",
    "                    combined_sections.append(DocumentSection(\n",
    "                        title=matched_toc_entry.title,\n",
    "                        content=section_content,\n",
    "                        section_type=matched_toc_entry.section_type,\n",
    "                        item_number=matched_toc_entry.item_number,\n",
    "                        part=matched_toc_entry.part,\n",
    "                        start_pos=start_pos,\n",
    "                        end_pos=end_pos\n",
    "                    ))\n",
    "                else: # Fallback if TOC entry not clearly matched\n",
    "                    title = content[start_pos:content.find('\\n', start_pos)].strip() or \"Unknown Section\"\n",
    "                    combined_sections.append(DocumentSection(\n",
    "                        title=title,\n",
    "                        content=section_content,\n",
    "                        section_type='content',\n",
    "                        start_pos=start_pos,\n",
    "                        end_pos=end_pos\n",
    "                    ))\n",
    "            \n",
    "            if len(combined_sections) >= 3:\n",
    "                logger.info(f\"Universal detection successful (TOC-based): Found {len(combined_sections)} sections.\")\n",
    "                return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"No valid patterns generated from TOC for content mapping.\")\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        # Fallback for malformed filenames, though process_filing_robust_universal checks this\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Extract tables and narrative from this section's content\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0 # Renamed variable to avoid conflict with function parameter 'overlap_tokens'\n",
    "\n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            # Ensure we don't go past the start of the chunk\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # If after trying to create overlap, we still don't have enough tokens for overlap\n",
    "            # (e.g., first few sentences are very long), just take some minimal content.\n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                # Fallback to last sentence if no other overlap possible and current chunk exists\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\")\n",
    "print(\"‚úÖ Robust error handling and logging\")\n",
    "print(\"‚úÖ Structured data classes for better organization\")\n",
    "print(\"‚úÖ Quality validation and statistics\")\n",
    "print(\"‚úÖ Separate table and narrative processing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\")\n",
    "            print(f\"  Section: {chunk.section_info}\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\")\n",
    "        print(\"Please update the file path to match your data structure\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    print(f\"  Min: {min(token_counts)}\")\n",
    "    print(f\"  Max: {max(token_counts)}\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test functions adapted to _fixed suffix to avoid NameErrors from notebook re-runs\n",
    "# Ensure these are called after all function definitions.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"  # If you have this one\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\")\n",
    "        for i, section in enumerate(sections[:10]):  # Show first 10\n",
    "            print(f\"  {i+1}. {section.title}\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\")\n",
    "\n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        if chunks:\n",
    "            # Show section distribution\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:  # Sample first 20\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Summary comparison\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Old detection\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content) # Use detect_sections_robust_old\n",
    "\n",
    "    # New universal detection\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test key patterns\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\].*?Item.*?\\\\[TABLE_END\\\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\\\s+\\\\d+[A-C]?\\\\.\\\\s*\\\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\].*?PART.*?\\\\[TABLE_END\\\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content) # Use compiled pattern\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            # Clean up match for display\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b0d5c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: unbalanced parenthesis at position 73\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmpc3gm1vf0_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2022-04-29.txt: unbalanced parenthesis at position 73\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2020-05-01.txt: unbalanced parenthesis at position 73\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2020-10-30.txt: unbalanced parenthesis at position 73\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: unbalanced parenthesis at position 73\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 4 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  3: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:  4: Item/Part IV - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 4 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: unbalanced parenthesis at position 73\n",
      "INFO:__main__:Attempting universal SEC section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "‚úÖ Robust error handling and logging\n",
      "‚úÖ Structured data classes for better organization\n",
      "‚úÖ Quality validation and statistics\n",
      "‚úÖ Separate table and narrative processing\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üìù Sample Chunks:\n",
      "No test file processed yet\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 2: Empty content\n",
      "  Result: 1 sections\n",
      "\n",
      "Test 3: Malformed filename\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 4: Very short text\n",
      "  Result: 0 chunks\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 0\n",
      "  Average chunks per file: 0.0\n",
      "\n",
      "üìã Per-file results:\n",
      "  AMZN_10Q_2022-04-29.txt: 0 chunks, 0 sections, 0 tables\n",
      "  AMZN_10Q_2020-05-01.txt: 0 chunks, 0 sections, 0 tables\n",
      "  AMZN_10Q_2020-10-30.txt: 0 chunks, 0 sections, 0 tables\n",
      "üìà Final Analysis Summary\n",
      "============================================================\n",
      "No chunks to analyze - run test_single_file() first\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "  4. Implement incremental processing for large datasets\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 17 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 4,292 chars\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "     Type: item, Length: 4,635 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 4 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 71,104 chars\n",
      "\n",
      "  2. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 189,316 chars\n",
      "\n",
      "  3. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 2,224 chars\n",
      "\n",
      "  4. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 10,492 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  error: No chunks created\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç Universal SEC detection found 2 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part II - . OTHER INFORMATION...\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis at position 73",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1551\u001b[39m\n\u001b[32m   1548\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_match\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1550\u001b[39m \u001b[38;5;66;03m# Run the fixed tests\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m results_universal = \u001b[43mtest_universal_detection_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m old_vs_new_sections = compare_old_vs_universal_fixed()\n\u001b[32m   1553\u001b[39m quick_pattern_test_fixed()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1440\u001b[39m, in \u001b[36mtest_universal_detection_fixed\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1437\u001b[39m     content = f.read()\n\u001b[32m   1439\u001b[39m \u001b[38;5;66;03m# Test universal detection\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1440\u001b[39m sections = \u001b[43mdetect_sections_robust_universal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1443\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, section \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sections[:\u001b[32m10\u001b[39m]):  \u001b[38;5;66;03m# Show first 10\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 632\u001b[39m, in \u001b[36mdetect_sections_robust_universal\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# Strategy 2: Try parsing Table of Contents. If successful and provides many sections,\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# it indicates a structured document. We'll use these titles and re-scan the document\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# for their content.\u001b[39;00m\n\u001b[32m    631\u001b[39m logger.warning(\u001b[33m\"\u001b[39m\u001b[33mDirect detection found few sections, analyzing table of contents.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m toc_entries = \u001b[43mdetect_sections_from_toc_universal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# These are DocumentSections with only title/metadata, no content\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m toc_entries \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(toc_entries) >= \u001b[32m3\u001b[39m: \u001b[38;5;66;03m# If TOC parsing yielded a good number of entries\u001b[39;00m\n\u001b[32m    635\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTOC analysis found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(toc_entries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m potential sections. Attempting to extract content based on TOC titles.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 530\u001b[39m, in \u001b[36mdetect_sections_from_toc_universal\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sections\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# Look for table of contents patterns\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# Using re.escape for literal brackets and compiling patterns once.\u001b[39;00m\n\u001b[32m    526\u001b[39m toc_patterns = [\n\u001b[32m    527\u001b[39m     re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?i)INDEX.*?(?=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m[PAGE BREAK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, re.DOTALL),\n\u001b[32m    528\u001b[39m     re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?i)TABLE OF CONTENTS.*?(?=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m[PAGE BREAK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, re.DOTALL),\n\u001b[32m    529\u001b[39m     re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?i)FORM 10-[KQ].*?INDEX.*?(?=\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m[PAGE BREAK\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m'\u001b[39m, re.DOTALL), \u001b[38;5;66;03m# Common variant\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(?i)\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m[\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m[TABLE_START\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m].*?Page.*?\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m[\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m[TABLE_END\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m].*?(?=\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m[PAGE BREAK\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m])\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOTALL\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;66;03m# Hyper-defensive escaping\u001b[39;00m\n\u001b[32m    531\u001b[39m ]\n\u001b[32m    533\u001b[39m toc_content = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m toc_patterns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/__init__.py:227\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(pattern, flags)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile\u001b[39m(pattern, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    226\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/__init__.py:294\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(pattern, flags)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    289\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mas it is an undocumented flag \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mwithout an obvious purpose. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mDon\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    293\u001b[39m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m p = \u001b[43m_compiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags & DEBUG):\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) >= _MAXCACHE:\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/_compiler.py:745\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(p, flags)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[32m    744\u001b[39m     pattern = p\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     p = \u001b[43m_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    747\u001b[39m     pattern = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/take-home-project/lib/python3.11/re/_parser.py:994\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(str, flags, state)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source.next \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m source.next == \u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m source.error(\u001b[33m\"\u001b[39m\u001b[33munbalanced parenthesis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p.state.grouprefpos:\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m g >= p.state.groups:\n",
      "\u001b[31merror\u001b[39m: unbalanced parenthesis at position 73"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start, # Changed from match.start() for consistency with line-based detection\n",
    "                'end_pos': line_end,     # Changed from match.end()\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "\n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "\n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using re.escape for literal brackets and compiling patterns once.\n",
    "    patterns = [\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*\\[\\[TABLE_END\\]\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[\\[TABLE_END\\]\\]', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?PART\\s+([IVX]+)\\s*\\[\\[TABLE_END\\]\\]', re.DOTALL),\n",
    "\n",
    "        re.compile(r'(?i)^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)', re.M),\n",
    "        re.compile(r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        re.compile(r'(?i)^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)', re.M),\n",
    "        re.compile(r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "\n",
    "        re.compile(r'(?i)^(Item\\\\s+\\\\d{1,2}[A-C]?\\\\.\\\\s+[^|]+?)$', re.M), # Standardize this escaping\n",
    "        re.compile(r'^(PART\\\\s+[IVX]+)(?:\\\\s*[-‚Äì‚Äî]\\\\s*(.+))?$', re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get context around the match (full line)\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "\n",
    "            section_id = groups[0].strip() if groups else 'unknown'\n",
    "            section_title = \"\"\n",
    "\n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title from table markers\n",
    "                section_title = re.sub(r'\\[\\[TABLE_END\\]\\].*', '', section_title).strip() # Using [[...]]\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) == 1:\n",
    "                # For patterns that only capture an ID, try to get the rest of the line as title\n",
    "                line_after_id_match = content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    section_title = line_after_id_match\n",
    "                else:\n",
    "                    section_title = f\"Section {section_id}\" # Generic fallback title\n",
    "            else:\n",
    "                section_title = full_line # Fallback to full line as title\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches by their start position\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    # Remove duplicates and prioritize 'better' matches\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0]) # Add the first match\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # Heuristic for deciding whether to add or replace:\n",
    "            # 1. If current match is significantly after the last added match, add it.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] > 150: # Increased distance for new section\n",
    "                final_matches.append(current_match)\n",
    "            # 2. If current match is very close, but provides a more specific 'item' or 'part' ID\n",
    "            #    and the last added one was generic 'unknown' or less specific.\n",
    "            elif current_match['start_pos'] - last_added_match['start_pos'] < 50: # Close proximity\n",
    "                if last_added_match['section_id'] == 'unknown' and current_match['section_id'] != 'unknown':\n",
    "                    final_matches[-1] = current_match # Replace with more specific match\n",
    "                elif last_added_match['section_id'] == current_match['section_id'] and last_added_match['pattern_idx'] > current_match['pattern_idx']:\n",
    "                    # If same ID but new pattern has higher priority (lower index means earlier in list)\n",
    "                    final_matches[-1] = current_match\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # End position is the start of the next matched section, or end of content if it's the last one\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title'] # Use the extracted section title\n",
    "\n",
    "        section_type = 'content' # Default type\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id: # If it's just a generic \"Part X\" title\n",
    "                title = part # Use the standardized PART title\n",
    "            elif not title: # If no specific title was captured\n",
    "                 title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id: # If it's just a generic \"Item X\" title\n",
    "                title = f\"Item {item_number}\" # Use the standardized ITEM title\n",
    "            elif not title: # If no specific title was captured\n",
    "                 title = f\"Item {item_number}\"\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    # Using re.escape for literal brackets and compiling patterns once.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL), # Common variant\n",
    "        re.compile(r'(?i)\\[\\[TABLE_START\\]\\].*?Page.*?\\\\[\\[TABLE_END\\]\\].*?(?=\\\\[PAGE BREAK\\\\])', re.DOTALL), # Hyper-defensive escaping\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content) # Use pre-compiled pattern\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections # Return empty list if no TOC found\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # Using re.escape for literal brackets and compiling patterns once.\n",
    "    item_patterns = [\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL), # Standard table format\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)\\\\s*\\\\|\\\\s*([^|]+)', re.DOTALL), # Part in table\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s+([^|\\\\d]+)', re.M), # Standalone Item line\n",
    "        re.compile(r'(?i)(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL), # Number-dot item in table\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)', re.M) # Simple PART line\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    for pattern in item_patterns:\n",
    "        for match in pattern.finditer(toc_content): # Use pre-compiled pattern and toc_content\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2: # Pattern captured both ID and Title\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title) # Normalize whitespace\n",
    "                found_items.append((item_id, item_title))\n",
    "            elif len(groups) == 1: # Pattern only captured ID (e.g., simple PART)\n",
    "                item_id = groups[0].strip()\n",
    "                # Attempt to get text immediately following the item_id if available, otherwise use a generic title\n",
    "                # This makes the TOC parsing more robust for less structured TOCs.\n",
    "                line_after_id_match = toc_content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    item_title = line_after_id_match\n",
    "                else:\n",
    "                    item_title = f\"Section {item_id}\"\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:50]}\" # Use a longer slice for better uniqueness\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = item_id\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is intentionally empty here; will be filled by main sectioning if this strategy is chosen.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3: # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents. If successful and provides many sections,\n",
    "    # it indicates a structured document. We'll use these titles and re-scan the document\n",
    "    # for their content.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3: # If TOC parsing yielded a good number of entries\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "        \n",
    "        combined_sections = []\n",
    "        \n",
    "        # This part of the logic needs to be robust for mapping TOC titles back to content.\n",
    "        # A simple approach would be to find the starting position of each TOC title in the main content.\n",
    "        # This can be error-prone if titles are not unique or are slightly different in the body.\n",
    "        \n",
    "        # For a more reliable \"TOC-based content extraction,\" you'd typically:\n",
    "        # 1. Take the sorted TOC titles.\n",
    "        # 2. Iterate through the main `content` to find each title's occurrence.\n",
    "        # 3. The content of a section is from its title's start to the next title's start.\n",
    "        \n",
    "        # Let's implement a basic version of that here to utilize the TOC entries.\n",
    "        # This is an enhancement to strategy 2, making it more effective than just\n",
    "        # returning the TOC entries without content.\n",
    "        \n",
    "        current_content_pos = 0\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            # Create a flexible regex for the title to find it in the main content\n",
    "            # Escape special regex characters in the title and allow for variable whitespace\n",
    "            search_title = re.escape(toc_entry.title).replace('\\\\ ', '\\\\s*')\n",
    "            \n",
    "            # Prioritize matching by Item/Part numbers if available, as they are more unique\n",
    "            if toc_entry.item_number:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*(?:Item\\s*' + re.escape(toc_entry.item_number) + r'|' + search_title + r')', re.M)\n",
    "            elif toc_entry.part:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*(?:PART\\s*' + re.escape(toc_entry.part) + r'|' + search_title + r')', re.M)\n",
    "            else:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*' + search_title, re.M)\n",
    "\n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "            \n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "                \n",
    "                # The content for this section goes until the start of the next TOC entry, or end of document\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry_title = re.escape(toc_entries[i+1].title).replace('\\\\ ', '\\\\s*')\n",
    "                    if toc_entries[i+1].item_number:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:Item\\s*' + re.escape(toc_entries[i+1].item_number) + r'|' + next_toc_entry_title + r')', re.M)\n",
    "                    elif toc_entries[i+1].part:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:PART\\s*' + re.escape(toc_entries[i+1].part) + r'|' + next_toc_entry_title + r')', re.M)\n",
    "                    else:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*' + next_toc_entry_title, re.M)\n",
    "                    \n",
    "                    next_match = next_pattern.search(content, pos=start_pos + len(match.group(0))) # Search after current match\n",
    "                    if next_match:\n",
    "                        next_start_pos = next_match.start()\n",
    "                \n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "                \n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: {toc_entry.title}. Skipping or appending as part of previous.\")\n",
    "                # If a TOC entry is not found, its content might be part of the previous section,\n",
    "                # or it's a false positive in the TOC. For simplicity, we just move on.\n",
    "                # A more advanced approach would merge its expected content with the previous section.\n",
    "\n",
    "        if len(combined_sections) >= 3: # If TOC-based content extraction yields good results\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0 # Renamed variable to avoid conflict with function parameter 'overlap_tokens'\n",
    "\n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            # Ensure we don't go past the start of the chunk\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # If after trying to create overlap, we still don't have enough tokens for overlap\n",
    "            # (e.g., first few sentences are very long), just take some minimal content.\n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                # Fallback to last sentence if no other overlap possible and current chunk exists\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\")\n",
    "print(\"‚úÖ Robust error handling and logging\")\n",
    "print(\"‚úÖ Structured data classes for better organization\")\n",
    "print(\"‚úÖ Quality validation and statistics\")\n",
    "print(\"‚úÖ Separate table and narrative processing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\")\n",
    "            print(f\"  Section: {chunk.section_info}\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\")\n",
    "        print(\"Please update the file path to match your data structure\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    print(f\"  Min: {min(token_counts)}\")\n",
    "    print(f\"  Max: {max(token_counts)}\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test functions adapted to _fixed suffix to avoid NameErrors from notebook re-runs\n",
    "# Ensure these are called after all function definitions.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"  # If you have this one\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):  # Show first 10\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            # Show section distribution\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:  # Sample first 20\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Summary comparison\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Old detection\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content) # Use detect_sections_robust_old\n",
    "\n",
    "    # New universal detection\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test key patterns\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content) # Use compiled pattern\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            # Clean up match for display\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8021e62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n",
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmpum01nfi7_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 17 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 4,292 chars\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "     Type: item, Length: 4,635 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 4 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  3: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:  4: Item/Part IV - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 4 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 4 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 71,104 chars\n",
      "\n",
      "  2. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 189,316 chars\n",
      "\n",
      "  3. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 2,224 chars\n",
      "\n",
      "  4. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 10,492 chars\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 2 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part II - . OTHER INFORMATION...\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "WARNING:__main__:No table of contents found in detect_sections_from_toc_universal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 1 sections:\n",
      "\n",
      "  1. Full Document\n",
      "\n",
      "     Type: document, Length: 187,951 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - . Financial Information...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part II - . Other Information...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. . Financial Information\n",
      "\n",
      "     Type: part, Length: 115,924 chars\n",
      "\n",
      "  2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 1,004 chars\n",
      "\n",
      "  5. . Other Information\n",
      "\n",
      "     Type: part, Length: 248 chars\n",
      "\n",
      "  6. Risk Factors\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Exhibits\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 17 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   |  4 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   |  1 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 17 sections\n",
      "\n",
      "  Improvement: +-2 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  11. Other Information\n",
      "\n",
      "  12. Executive Compensation\n",
      "\n",
      "  13. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  14. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  15. Principal Accountant Fees and Services\n",
      "\n",
      "  16. Item 15.    Exhibit and Financial Statement Schedules\n",
      "\n",
      "  17. Form 10-K Summary\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start, # Changed from match.start() for consistency with line-based detection\n",
    "                'end_pos': line_end,     # Changed from match.end()\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "\n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "\n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using re.escape for literal brackets, and compiling patterns once.\n",
    "    # Replaced '\\[\\[...\\]\\]' with re.escape('[...]') as it's more standard and robust.\n",
    "    patterns = [\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s+([IVX]+)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(r'^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(r'^(Item\\s+\\d{1,2}[A-C]?\\.\\s+[^|]+?)$', re.I | re.M),\n",
    "        re.compile(r'^(PART\\s+[IVX]+)(?:\\s*[-‚Äì‚Äî]\\s*(.+))?$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get context around the match (full line)\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "\n",
    "            section_id = groups[0].strip() if groups else 'unknown'\n",
    "            section_title = \"\"\n",
    "\n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title from table markers - use actual markers here\n",
    "                section_title = re.sub(re.escape('[TABLE_END]') + r'.*', '', section_title, flags=re.I).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) == 1:\n",
    "                line_after_id_match = content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    section_title = line_after_id_match\n",
    "                else:\n",
    "                    section_title = f\"Section {section_id}\"\n",
    "            else:\n",
    "                section_title = full_line\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches by their start position\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    # Remove duplicates and prioritize 'better' matches\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0])\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] > 150:\n",
    "                final_matches.append(current_match)\n",
    "            elif current_match['start_pos'] - last_added_match['start_pos'] < 50:\n",
    "                if last_added_match['section_id'] == 'unknown' and current_match['section_id'] != 'unknown':\n",
    "                    final_matches[-1] = current_match\n",
    "                elif last_added_match['section_id'] == current_match['section_id'] and last_added_match['pattern_idx'] > current_match['pattern_idx']:\n",
    "                    final_matches[-1] = current_match\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                 title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                 title = f\"Item {item_number}\"\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    # Using re.escape for literal brackets and compiling patterns once.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL), # Simpler page break match\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL), # Simpler page break match\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL), # Simpler page break match\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL), # Simpler page break match\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # Using re.escape for literal brackets and compiling patterns once.\n",
    "    item_patterns = [\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL),\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)\\\\s*\\\\|\\\\s*([^|]+)', re.DOTALL),\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s+([^|\\\\d]+)', re.M),\n",
    "        re.compile(r'(?i)(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL),\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)', re.M)\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    for pattern in item_patterns:\n",
    "        for match in pattern.finditer(toc_content):\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2:\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "            elif len(groups) == 1:\n",
    "                item_id = groups[0].strip()\n",
    "                line_after_id_match = toc_content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    item_title = line_after_id_match\n",
    "                else:\n",
    "                    item_title = f\"Section {item_id}\"\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:50]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = item_id\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\",\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            search_title = re.escape(toc_entry.title).replace('\\\\ ', '\\\\s*')\n",
    "\n",
    "            if toc_entry.item_number:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*(?:Item\\s*' + re.escape(toc_entry.item_number) + r'|' + search_title + r')', re.M)\n",
    "            elif toc_entry.part:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*(?:PART\\s*' + re.escape(toc_entry.part) + r'|' + search_title + r')', re.M)\n",
    "            else:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*' + search_title, re.M)\n",
    "\n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry_title = re.escape(toc_entries[i+1].title).replace('\\\\ ', '\\\\s*')\n",
    "                    if toc_entries[i+1].item_number:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:Item\\s*' + re.escape(toc_entries[i+1].item_number) + r'|' + next_toc_entry_title + r')', re.M)\n",
    "                    elif toc_entries[i+1].part:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:PART\\s*' + re.escape(toc_entries[i+1].part) + r'|' + next_toc_entry_title + r')', re.M)\n",
    "                    else:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*' + next_toc_entry_title, re.M)\n",
    "\n",
    "                    # Search for the next section from the end of the current section's match\n",
    "                    next_match = next_pattern.search(content, pos=match.end()) # Search from end of current match\n",
    "                    if next_match:\n",
    "                        next_start_pos = next_match.start()\n",
    "\n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "\n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: {toc_entry.title}. This section might be merged with previous or skipped.\")\n",
    "                # This could also be a sub-section of a larger item not directly represented in primary TOC.\n",
    "                # For now, it will effectively be skipped if its start isn't found.\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0 # Renamed variable to avoid conflict with function parameter 'overlap_tokens'\n",
    "\n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            # Ensure we don't go past the start of the chunk\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # If after trying to create overlap, we still don't have enough tokens for overlap\n",
    "            # (e.g., first few sentences are very long), just take some minimal content.\n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                # Fallback to last sentence if no other overlap possible and current chunk exists\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test functions adapted to _fixed suffix to avoid NameErrors from notebook re-runs\n",
    "# Ensure these are called after all function definitions.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\", # This file name is in the future based on current date\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54ac2e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n",
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmp5dz7b037_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 4 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 17 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 4,292 chars\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "     Type: item, Length: 4,635 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  3: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:  4: Item/Part IV - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 4 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 2 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part II - . OTHER INFORMATION...\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 4 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 71,104 chars\n",
      "\n",
      "  2. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 189,316 chars\n",
      "\n",
      "  3. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 2,224 chars\n",
      "\n",
      "  4. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 10,492 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:No table of contents found in detect_sections_from_toc_universal.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 1 sections:\n",
      "\n",
      "  1. Full Document\n",
      "\n",
      "     Type: document, Length: 187,951 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - . Financial Information...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part II - . Other Information...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. . Financial Information\n",
      "\n",
      "     Type: part, Length: 115,924 chars\n",
      "\n",
      "  2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 1,004 chars\n",
      "\n",
      "  5. . Other Information\n",
      "\n",
      "     Type: part, Length: 248 chars\n",
      "\n",
      "  6. Risk Factors\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Exhibits\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 17 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   |  4 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   |  1 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 17 sections\n",
      "\n",
      "  Improvement: +-2 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  11. Other Information\n",
      "\n",
      "  12. Executive Compensation\n",
      "\n",
      "  13. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  14. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  15. Principal Accountant Fees and Services\n",
      "\n",
      "  16. Item 15.    Exhibit and Financial Statement Schedules\n",
      "\n",
      "  17. Form 10-K Summary\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start, # Changed from match.start() for consistency with line-based detection\n",
    "                'end_pos': line_end,     # Changed from match.end()\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "\n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "\n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using re.escape for literal brackets, and compiling patterns once.\n",
    "    # Changed to match the exact string '[TABLE_START]' and '[TABLE_END]'\n",
    "    patterns = [\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s+([IVX]+)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (fallback) - using non-escaped word parts\n",
    "        re.compile(r'^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (fallback) - using non-escaped word parts\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-only patterns in tables - using re.escape for markers\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Headers that might appear standalone - using non-escaped word parts\n",
    "        re.compile(r'^(Item\\s+\\d{1,2}[A-C]?\\.\\s+[^|]+?)$', re.I | re.M),\n",
    "        re.compile(r'^(PART\\s+[IVX]+)(?:\\s*[-‚Äì‚Äî]\\s*(.+))?$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get context around the match (full line)\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "\n",
    "            section_id = groups[0].strip() if groups else 'unknown'\n",
    "            section_title = \"\"\n",
    "\n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title from table markers - use actual markers here\n",
    "                section_title = re.sub(re.escape('[TABLE_END]') + r'.*', '', section_title, flags=re.I).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) == 1:\n",
    "                line_after_id_match = content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    section_title = line_after_id_match\n",
    "                else:\n",
    "                    section_title = f\"Section {section_id}\"\n",
    "            else:\n",
    "                section_title = full_line\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches by their start position\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    # Remove duplicates and prioritize 'better' matches\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0]) # Add the first match\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # Heuristic for deciding whether to add or replace:\n",
    "            # 1. If current match is significantly after the last added match, add it.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] > 150: # Increased distance for new section\n",
    "                final_matches.append(current_match)\n",
    "            # 2. If current match is very close, but provides a more specific 'item' or 'part' ID\n",
    "            #    and the last added one was generic 'unknown' or less specific.\n",
    "            elif current_match['start_pos'] - last_added_match['start_pos'] < 50: # Close proximity\n",
    "                if last_added_match['section_id'] == 'unknown' and current_match['section_id'] != 'unknown':\n",
    "                    final_matches[-1] = current_match # Replace with more specific match\n",
    "                elif last_added_match['section_id'] == current_match['section_id'] and last_added_match['pattern_idx'] > current_match['pattern_idx']:\n",
    "                    # If same ID but new pattern has higher priority (lower index means earlier in list)\n",
    "                    final_matches[-1] = current_match\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # End position is the start of the next matched section, or end of content if it's the last one\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                 title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                 title = f\"Item {item_number}\"\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    # Using re.escape for literal brackets and compiling patterns once.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # Removed re.escape from words like \"Item\" and \"PART\" as they are literal words not regex metacharacters here\n",
    "    item_patterns = [\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL), # Standard table format\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)\\\\s*\\\\|\\\\s*([^|]+)', re.DOTALL), # Part in table\n",
    "        re.compile(r'(?i)Item\\\\s+(\\\\d{1,2}[A-C]?)\\\\.\\\\s+([^|\\\\d]+)', re.M), # Standalone Item line (non-table)\n",
    "        re.compile(r'(?i)(\\\\d{1,2}[A-C]?)\\\\.\\\\s*\\\\|\\\\s*([^|]+?)\\\\s*\\\\|\\\\s*\\\\d+', re.DOTALL), # Number-dot item in table\n",
    "        re.compile(r'(?i)PART\\\\s+([IVX]+)', re.M) # Simple PART line (non-table)\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    for pattern in item_patterns:\n",
    "        for match in pattern.finditer(toc_content):\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2:\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "            elif len(groups) == 1:\n",
    "                item_id = groups[0].strip()\n",
    "                line_after_id_match = toc_content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    item_title = line_after_id_match\n",
    "                else:\n",
    "                    item_title = f\"Section {item_id}\"\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:50]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = item_id\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\",\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content)\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            search_title = re.escape(toc_entry.title).replace('\\\\ ', '\\\\s*')\n",
    "\n",
    "            if toc_entry.item_number:\n",
    "                # Prioritize searching for \"Item X. Title\" or \"PART X Title\"\n",
    "                # This regex tries to match either the standardized Item/Part format or the escaped title\n",
    "                pattern_str = r'(?i)^\\s*(?:'\n",
    "                if toc_entry.item_number:\n",
    "                    pattern_str += r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.'\n",
    "                elif toc_entry.part:\n",
    "                    pattern_str += r'PART\\s*' + re.escape(toc_entry.part)\n",
    "                \n",
    "                # Add the title as an alternative match\n",
    "                if search_title:\n",
    "                    pattern_str += r'|' + search_title\n",
    "                pattern_str += r')'\n",
    "                search_pattern = re.compile(pattern_str, re.M)\n",
    "            else:\n",
    "                search_pattern = re.compile(r'(?i)^\\s*' + search_title, re.M)\n",
    "\n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_search_title = re.escape(next_toc_entry.title).replace('\\\\ ', '\\\\s*')\n",
    "\n",
    "                    next_pattern_str = r'(?i)^\\s*(?:'\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_str += r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.'\n",
    "                    elif next_toc_entry.part:\n",
    "                        next_pattern_str += r'PART\\s*' + re.escape(next_toc_entry.part)\n",
    "                    \n",
    "                    if next_search_title:\n",
    "                        next_pattern_str += r'|' + next_search_title\n",
    "                    next_pattern_str += r')'\n",
    "                    next_pattern = re.compile(next_pattern_str, re.M)\n",
    "                    \n",
    "                    # Search for the next section from the end of the current section's match\n",
    "                    next_match = next_pattern.search(content, pos=match.end())\n",
    "                    if next_match:\n",
    "                        next_start_pos = next_match.start()\n",
    "\n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "\n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0 # Renamed variable to avoid conflict with function parameter 'overlap_tokens'\n",
    "\n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            # Ensure we don't go past the start of the chunk\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # If after trying to create overlap, we still don't have enough tokens for overlap\n",
    "            # (e.g., first few sentences are very long), just take some minimal content.\n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                # Fallback to last sentence if no other overlap possible and current chunk exists\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test functions adapted to _fixed suffix to avoid NameErrors from notebook re-runs\n",
    "# Ensure these are called after all function definitions.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\", # This file name is in the future based on current date\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "363d212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 262 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "\n",
      "‚úÖ Robust error handling and logging\n",
      "\n",
      "‚úÖ Structured data classes for better organization\n",
      "\n",
      "‚úÖ Quality validation and statistics\n",
      "\n",
      "‚úÖ Separate table and narrative processing\n",
      "\n",
      "============================================================\n",
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "\n",
      "Chunk 1 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 58\n",
      "\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "\n",
      "Chunk 2 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 240\n",
      "\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "\n",
      "Chunk 3 (table):\n",
      "\n",
      "  Section: Full Document\n",
      "\n",
      "  Tokens: 41\n",
      "\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n",
      "\n",
      "üîç Comparing Section Detection Strategies\n",
      "\n",
      "==================================================\n",
      "üîç Improved detection found 0 potential sections:\n",
      "Strategy 1 (Regex): 0 sections\n",
      "\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "\n",
      "  1. Document Content...\n",
      "\n",
      "üìä Chunking Quality Analysis\n",
      "\n",
      "==================================================\n",
      "Token Distribution:\n",
      "\n",
      "  Mean: 379.9\n",
      "\n",
      "  Median: 445\n",
      "\n",
      "  Min: 38\n",
      "\n",
      "  Max: 1692\n",
      "\n",
      "\n",
      "Chunk Types:\n",
      "\n",
      "  table: 66\n",
      "\n",
      "  narrative: 106\n",
      "\n",
      "\n",
      "Section Distribution:\n",
      "\n",
      "  Full Document: 172 chunks\n",
      "\n",
      "\n",
      "Overlap Analysis:\n",
      "\n",
      "  Chunks with overlap: 105/172 (61.0%)\n",
      "\n",
      "üîß Testing Different Chunking Parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "\n",
      "  Total chunks: 262\n",
      "\n",
      "  Avg tokens: 273.5\n",
      "\n",
      "  Overlap rate: 195/262\n",
      "\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 127 chunks for AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Empty content provided to detect_sections_universal_sec. Returning empty sections.\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmppv1xe7oi_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (912 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 4: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 5: ...\n",
      "INFO:__main__:  ‚Ä¢ 6: ...\n",
      "INFO:__main__:  ‚Ä¢ I: . FINANCIAL INFORMATION | Item 1. | Financial Stat...\n",
      "INFO:__main__:  ‚Ä¢ II: . OTHER INFORMATION | Item 1. | Legal Proceedings ...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated | Statements of Comprehensive | Income (Loss) | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 20 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 4. | Controls and Procedures | 32 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 33 | Item 1A. | Risk Factors | 33 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 43 | Item 3. | Defaults Upon Senior Securities | 43 | Item 4. | Mine Safety Disclosures | 43 | Item 5. | Other Information | 43 | Item 6. | Exhibits | 44 | Signatures | 45=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 125 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 4: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 5: ...\n",
      "INFO:__main__:  ‚Ä¢ 6: ...\n",
      "INFO:__main__:  ‚Ä¢ I: . FINANCIAL INFORMATION | Item 1. | Financial Stat...\n",
      "INFO:__main__:  ‚Ä¢ II: . OTHER INFORMATION | Item 1. | Legal Proceedings ...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total chunks: 172\n",
      "\n",
      "  Avg tokens: 379.9\n",
      "\n",
      "  Overlap rate: 105/172\n",
      "\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "\n",
      "  Total chunks: 127\n",
      "\n",
      "  Avg tokens: 495.8\n",
      "\n",
      "  Overlap rate: 60/127\n",
      "\n",
      "üõ°Ô∏è Testing Error Handling\n",
      "\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 2: Empty content\n",
      "\n",
      "  Result: 1 sections\n",
      "\n",
      "\n",
      "Test 3: Malformed filename\n",
      "\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "\n",
      "Test 4: Very short text\n",
      "\n",
      "  Result: 0 chunks\n",
      "\n",
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created 195 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (901 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 4: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 5: ...\n",
      "INFO:__main__:  ‚Ä¢ 6: ...\n",
      "INFO:__main__:  ‚Ä¢ I: . FINANCIAL INFORMATION | Item 1. | Financial Stat...\n",
      "INFO:__main__:  ‚Ä¢ II: . OTHER INFORMATION | Item 1. | Legal Proceedings ...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 32 | Item 4. | Controls and Procedures | 33 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 34 | Item 1A. | Risk Factors | 34 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 44 | Item 3. | Defaults Upon Senior Securities | 44 | Item 4. | Mine Safety Disclosures | 44 | Item 5. | Other Information | 44 | Item 6. | Exhibits | 45 | Signatures | 46=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 120 chunks for AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n",
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "\n",
      "  Total files processed: 3\n",
      "\n",
      "  Total chunks created: 440\n",
      "\n",
      "  Average chunks per file: 146.7\n",
      "\n",
      "\n",
      "üìã Per-file results:\n",
      "\n",
      "  AMZN_10Q_2022-04-29.txt: 125 chunks, 1 sections, 51 tables\n",
      "\n",
      "  AMZN_10Q_2020-05-01.txt: 195 chunks, 1 sections, 131 tables\n",
      "\n",
      "  AMZN_10Q_2020-10-30.txt: 120 chunks, 1 sections, 48 tables\n",
      "\n",
      "üìà Final Analysis Summary\n",
      "\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "\n",
      "  ‚Ä¢ Total chunks: 172\n",
      "\n",
      "  ‚Ä¢ Average chunk size: 380 tokens\n",
      "\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "\n",
      "  ‚Ä¢ Overlap rate: 61.0%\n",
      "\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "\n",
      "  ‚Ä¢ narrative: 106 chunks (61.6%)\n",
      "\n",
      "  ‚Ä¢ table: 66 chunks (38.4%)\n",
      "\n",
      "\n",
      "üìö Section Breakdown:\n",
      "\n",
      "  ‚Ä¢ Full Document: 172 chunks\n",
      "\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.2%)\n",
      "\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (1.7%)\n",
      "\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "\n",
      "    Section: Full Document\n",
      "\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n",
      "\n",
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "\n",
      "  4. Implement incremental processing for large datasets\n",
      "\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "\n",
      "============================================================\n",
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "\n",
      "1. Run test_universal_detection_fixed() to test all files\n",
      "\n",
      "2. Run compare_old_vs_universal_fixed() to see the improvement\n",
      "\n",
      "3. Run quick_pattern_test_fixed() to see what patterns match\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1367 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 172 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 4 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 17 sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "     Type: part, Length: 13,274 chars\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "     Type: item, Length: 61,136 chars\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "     Type: item, Length: 582 chars\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "     Type: item, Length: 898 chars\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "     Type: item, Length: 4,292 chars\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "     Type: item, Length: 1,745 chars\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 33,154 chars\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 6,799 chars\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "     Type: item, Length: 103,042 chars\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "     Type: item, Length: 4,635 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 172\n",
      "\n",
      "  avg_tokens: 379.86046511627904\n",
      "\n",
      "  min_tokens: 38\n",
      "\n",
      "  max_tokens: 1692\n",
      "\n",
      "  chunks_with_overlap: 105\n",
      "\n",
      "  table_chunks: 66\n",
      "\n",
      "  narrative_chunks: 106\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - [TABLE_START]...\n",
      "INFO:__main__:  2: Item/Part II - [TABLE_START]...\n",
      "INFO:__main__:  3: Item/Part III - [TABLE_START]...\n",
      "INFO:__main__:  4: Item/Part IV - [TABLE_START]...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 4 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (1441 chars)\n",
      "INFO:__main__:Extracted 30 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ I: Item 1....\n",
      "INFO:__main__:  ‚Ä¢ II: Item 5....\n",
      "INFO:__main__:  ‚Ä¢ III: Item 10....\n",
      "INFO:__main__:  ‚Ä¢ IV: Item 15....\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 1B: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 4: ...\n",
      "INFO:__main__:TOC analysis found 30 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 1.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 5.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 10.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: 'Item 15.'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 1. | Business | 3 | Item 1A. | Risk Factors | 6 | Item 1B. | Unresolved Staff Comments | 16 | Item 2. | Properties | 17 | Item 3. | Legal Proceedings | 17 | Item 4. | Mine Safety Disclosures | 17 | PART II | Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 5. | Market for the Registrant‚Äôs Common Stock, Related Shareholder Matters, and Issuer Purchases of Equity Securities | 18 | Item 6. | Reserved | 18 | Item 7. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 19 | Item 7A. | Quantitative and Qualitative Disclosures About Market Risk | 31 | Item 8. | Financial Statements and Supplementary Data | 33 | Item 9. | Changes in and Disagreements with Accountants on Accounting and Financial Disclosure | 69 | Item 9A. | Controls and Procedures | 69 | Item 9B. | Other Information | 71 | Item 9C. | Disclosure Regarding Foreign Jurisdictions that Prevent Inspections | 71 | PART III | Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 10. | Directors, Executive Officers, and Corporate Governance | 71 | Item 11. | Executive Compensation | 71 | Item 12. | Security Ownership of Certain Beneficial Owners and Management and Related Shareholder Matters | 71 | Item 13. | Certain Relationships and Related Transactions, and Director Independence | 71 | Item 14. | Principal Accountant Fees and Services | 71 | PART IV | Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '| Item 15. | Exhibits, Financial Statement Schedules | 72 | Item 16. | Form 10-K Summary | 74 | Signatures | 75=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Created 210 chunks for AMZN_10K_2023-02-03.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 2 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 4 sections:\n",
      "\n",
      "  1. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 71,104 chars\n",
      "\n",
      "  2. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 189,316 chars\n",
      "\n",
      "  3. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 2,224 chars\n",
      "\n",
      "  4. [TABLE_START]\n",
      "\n",
      "     Type: part, Length: 10,492 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 210\n",
      "\n",
      "  avg_tokens: 332.1666666666667\n",
      "\n",
      "  min_tokens: 6\n",
      "\n",
      "  max_tokens: 1157\n",
      "\n",
      "  chunks_with_overlap: 119\n",
      "\n",
      "  table_chunks: 90\n",
      "\n",
      "  narrative_chunks: 120\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "INFO:__main__:  2: Item/Part II - . OTHER INFORMATION...\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "WARNING:__main__:No table of contents found in detect_sections_from_toc_universal.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (903 chars)\n",
      "INFO:__main__:Extracted 9 sections from table of contents:\n",
      "INFO:__main__:  ‚Ä¢ 1: ...\n",
      "INFO:__main__:  ‚Ä¢ 2: ...\n",
      "INFO:__main__:  ‚Ä¢ 3: ...\n",
      "INFO:__main__:  ‚Ä¢ 4: ...\n",
      "INFO:__main__:  ‚Ä¢ 1A: ...\n",
      "INFO:__main__:  ‚Ä¢ 5: ...\n",
      "INFO:__main__:  ‚Ä¢ 6: ...\n",
      "INFO:__main__:  ‚Ä¢ I: . FINANCIAL INFORMATION | Item 1. | Financial Stat...\n",
      "INFO:__main__:  ‚Ä¢ II: . OTHER INFORMATION | Item 1. | Legal Proceedings ...\n",
      "INFO:__main__:TOC analysis found 9 potential sections. Attempting to extract content based on TOC titles.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: ''. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. FINANCIAL INFORMATION | Item 1. | Financial Statements | 3 | Consolidated Statements of Cash Flows | 3 | Consolidated Statements of Operations | 4 | Consolidated Statements of Comprehensive | Income | 5 | Consolidated Balance Sheets | 6 | Notes to Consolidated Financial Statements | 7 | Item 2. | Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations | 22 | Item 3. | Quantitative and Qualitative Disclosures About Market Risk | 33 | Item 4. | Controls and Procedures | 34 | PART II. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:Could not find content for TOC entry: '. OTHER INFORMATION | Item 1. | Legal Proceedings | 35 | Item 1A. | Risk Factors | 35 | Item 2. | Unregistered Sales of Equity Securities and Use of Proceeds | 46 | Item 3. | Defaults Upon Senior Securities | 46 | Item 4. | Mine Safety Disclosures | 46 | Item 5. | Other Information | 46 | Item 6. | Exhibits | 47 | Signatures | 48=== TABLE END ===2'. This section might be merged with previous or skipped.\n",
      "WARNING:__main__:TOC-based content mapping yielded few sections. Falling back to page-based detection.\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Created 132 chunks for AMZN_10Q_2024-11-01.txt\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 8 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 1 sections:\n",
      "\n",
      "  1. Full Document\n",
      "\n",
      "     Type: document, Length: 187,951 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 132\n",
      "\n",
      "  avg_tokens: 366.43939393939394\n",
      "\n",
      "  min_tokens: 7\n",
      "\n",
      "  max_tokens: 1548\n",
      "\n",
      "  chunks_with_overlap: 81\n",
      "\n",
      "  table_chunks: 50\n",
      "\n",
      "  narrative_chunks: 82\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - . Financial Information...\n",
      "INFO:__main__:  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  4: Item/Part 4 - Controls and Procedures...\n",
      "INFO:__main__:  5: Item/Part II - . Other Information...\n",
      "INFO:__main__:  6: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "INFO:__main__:  8: Item/Part 6 - Exhibits...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 8 sections.\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 0 unique sections:\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents.\n",
      "INFO:__main__:Found table of contents (5004 chars)\n",
      "INFO:__main__:Extracted 0 sections from table of contents:\n",
      "WARNING:__main__:Trying page-based detection as fallback.\n",
      "WARNING:__main__:All strategies failed, creating single section.\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Created 161 chunks for KO_10Q_2020-07-22.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:üîç Universal SEC detection found 17 unique sections:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 8 sections:\n",
      "\n",
      "  1. . Financial Information\n",
      "\n",
      "     Type: part, Length: 115,924 chars\n",
      "\n",
      "  2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "     Type: item, Length: 87,923 chars\n",
      "\n",
      "  3. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "     Type: item, Length: 207 chars\n",
      "\n",
      "  4. Controls and Procedures\n",
      "\n",
      "     Type: item, Length: 1,004 chars\n",
      "\n",
      "  5. . Other Information\n",
      "\n",
      "     Type: part, Length: 248 chars\n",
      "\n",
      "  6. Risk Factors\n",
      "\n",
      "     Type: item, Length: 11,661 chars\n",
      "\n",
      "  7. Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "\n",
      "     Type: item, Length: 2,127 chars\n",
      "\n",
      "  8. Exhibits\n",
      "\n",
      "     Type: item, Length: 13,918 chars\n",
      "\n",
      "\n",
      "üìä Processing Results:\n",
      "\n",
      "  total_chunks: 161\n",
      "\n",
      "  avg_tokens: 396.7577639751553\n",
      "\n",
      "  min_tokens: 32\n",
      "\n",
      "  max_tokens: 1451\n",
      "\n",
      "  chunks_with_overlap: 97\n",
      "\n",
      "  table_chunks: 63\n",
      "\n",
      "  narrative_chunks: 98\n",
      "\n",
      "  unique_sections: 1\n",
      "\n",
      "\n",
      "üìö Section Distribution (sample):\n",
      "\n",
      "  ‚Ä¢ Full Document: 20 chunks\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 17 sections | 172 chunks\n",
      "\n",
      "AMZN_10K_2023-02-03.txt   |  4 sections | 210 chunks\n",
      "\n",
      "AMZN_10Q_2024-11-01.txt   |  1 sections | 132 chunks\n",
      "\n",
      "KO_10Q_2020-07-22.txt     |  8 sections | 161 chunks\n",
      "\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "\n",
      "============================================================\n",
      "Running old detection...\n",
      "\n",
      "üîç Improved detection found 19 potential sections:\n",
      "  1: PART I...\n",
      "  2: Item 1A.    Risk Factors...\n",
      "  3: Item 1B.    Unresolved Staff Comments...\n",
      "  4: Item 3.    Legal Proceedings...\n",
      "  5: Item 4.    Mine Safety Disclosures...\n",
      "  6: Item 6.    Selected Financial Data...\n",
      "  7: Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Resul...\n",
      "  8: Item 7A.    Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  9: Item 8.    Financial Statements and Supplementary Data...\n",
      "  10: Notes to Consolidated Financial Statements...\n",
      "  11: Opinion on the Financial Statements...\n",
      "  12: Item 9.    Changes in and Disagreements with Accountants on Accounting and Finan...\n",
      "  13: Item 9B.    Other Information...\n",
      "  14: Item 11.    Executive Compensation...\n",
      "  15: Item 12.    Security Ownership of Certain Beneficial Owners and Management and R...\n",
      "Running universal detection...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:  1: Item/Part I - Item 1.    Business...\n",
      "INFO:__main__:  2: Item/Part 1A - Risk Factors...\n",
      "INFO:__main__:  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "INFO:__main__:  4: Item/Part 3 - Legal Proceedings...\n",
      "INFO:__main__:  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "INFO:__main__:  6: Item/Part 6 - Selected Financial Data...\n",
      "INFO:__main__:  7: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "INFO:__main__:  8: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "INFO:__main__:  9: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "INFO:__main__:  10: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "INFO:__main__:  11: Item/Part 9B - Other Information...\n",
      "INFO:__main__:  12: Item/Part 11 - Executive Compensation...\n",
      "INFO:__main__:  13: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "INFO:__main__:  14: Item/Part 13 - Certain Relationships and Related Transactions, and Director...\n",
      "INFO:__main__:  15: Item/Part 14 - Principal Accountant Fees and Services...\n",
      "INFO:__main__:Universal detection successful (Strategy 1): Found 17 sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparison Results:\n",
      "\n",
      "  Old detection: 19 sections\n",
      "\n",
      "  Universal detection: 17 sections\n",
      "\n",
      "  Improvement: +-2 sections\n",
      "\n",
      "\n",
      "üìã Old Sections:\n",
      "\n",
      "  1. Part I\n",
      "\n",
      "  2. Item 1A\n",
      "\n",
      "  3. Item 1B\n",
      "\n",
      "  4. Item 3\n",
      "\n",
      "  5. Item 4\n",
      "\n",
      "  6. Item 6\n",
      "\n",
      "  7. Item 7\n",
      "\n",
      "  8. Item 7A\n",
      "\n",
      "  9. Item 8\n",
      "\n",
      "  10. Notes to Consolidated Financial Statements\n",
      "\n",
      "  11. Opinion on the Financial Statements\n",
      "\n",
      "  12. Item 9\n",
      "\n",
      "  13. Item 9B\n",
      "\n",
      "  14. Item 11\n",
      "\n",
      "  15. Item 12\n",
      "\n",
      "  16. Item 13\n",
      "\n",
      "  17. Item 14\n",
      "\n",
      "  18. Part IV\n",
      "\n",
      "  19. Item 16\n",
      "\n",
      "\n",
      "üìã Universal Sections:\n",
      "\n",
      "  1. Item 1.    Business\n",
      "\n",
      "  2. Risk Factors\n",
      "\n",
      "  3. Unresolved Staff Comments\n",
      "\n",
      "  4. Legal Proceedings\n",
      "\n",
      "  5. Mine Safety Disclosures\n",
      "\n",
      "  6. Selected Financial Data\n",
      "\n",
      "  7. Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "\n",
      "  8. Quantitative and Qualitative Disclosures About Market Risk\n",
      "\n",
      "  9. Financial Statements and Supplementary Data\n",
      "\n",
      "  10. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "\n",
      "  11. Other Information\n",
      "\n",
      "  12. Executive Compensation\n",
      "\n",
      "  13. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "\n",
      "  14. Certain Relationships and Related Transactions, and Director Independence\n",
      "\n",
      "  15. Principal Accountant Fees and Services\n",
      "\n",
      "  16. Item 15.    Exhibit and Financial Statement Schedules\n",
      "\n",
      "  17. Form 10-K Summary\n",
      "\n",
      "üîç QUICK PATTERN TEST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "\n",
      "  1: Item 1. |...\n",
      "\n",
      "  2: Item 1A. |...\n",
      "\n",
      "  3: Item 1B. |...\n",
      "\n",
      "\n",
      "Part headers: 33 matches\n",
      "\n",
      "  1: Part III...\n",
      "\n",
      "  2: Part I...\n",
      "\n",
      "  3: Part II...\n",
      "\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\",\n",
    "    \"1A\": \"Risk Factors\",\n",
    "    \"1B\": \"Unresolved Staff Comments\",\n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\",\n",
    "    \"3\": \"Legal Proceedings\",\n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\",\n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\",\n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\",\n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\",\n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\",\n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\",\n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "\n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "\n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        re.compile(r'^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "        re.compile(r'^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$', re.I | re.M),\n",
    "\n",
    "        # ITEM patterns - much more flexible\n",
    "        re.compile(r'^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'^ITEM\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "        re.compile(r'Item\\s+(\\d{1,2}[A-C]?)(?:[.\\s‚Äì‚Äî])', re.I | re.M),\n",
    "\n",
    "        # Number-dot format common in SEC filings\n",
    "        re.compile(r'^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}', re.I | re.M),\n",
    "\n",
    "        # Content-based patterns for known sections\n",
    "        re.compile(r'^.{0,50}(BUSINESS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(RISK FACTORS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(LEGAL PROCEEDINGS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(FINANCIAL STATEMENTS)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(PROPERTIES)\\s*$', re.I | re.M),\n",
    "        re.compile(r'^.{0,50}(CONTROLS AND PROCEDURES)\\s*$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': line_start, # Changed from match.start() for consistency with line-based detection\n",
    "                'end_pos': line_end,     # Changed from match.end()\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200\n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "\n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "\n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in\n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "\n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "\n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "\n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "\n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "\n",
    "    return sections\n",
    "\n",
    "# The `detect_sections_robust` function from your original code (renamed detect_sections_robust_old to avoid conflict)\n",
    "def detect_sections_robust_old(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks (original version)\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_improved(content) # Original called detect_sections_strategy_1, updated to _improved\n",
    "\n",
    "    if len(sections) >= 3:  # A reasonable number of sections to consider it successful\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information for DocumentSection objects,\n",
    "    using form_type to select the correct item name map.\n",
    "    \"\"\"\n",
    "    item_number = section.item_number\n",
    "    section_type = section.section_type\n",
    "    part_number = section.part # Get part from DocumentSection\n",
    "\n",
    "    if section_type == 'item' and item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(item_number, \"Unknown Section\")\n",
    "            return f\"Item {item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Use part_number from DocumentSection if available, or try to infer from item_number\n",
    "            if part_number == 'PART I' or item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part I, Item {item_number} - {item_name}\"\n",
    "            elif part_number == 'PART II' or item_number in ITEM_NAME_MAP_10Q_PART_II:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {item_number} - {item_name}\"\n",
    "            else:\n",
    "                return f\"Item {item_number} - Unknown 10Q Section\"\n",
    "    \n",
    "    elif section_type == 'part' and part_number: # Use part_number from DocumentSection\n",
    "        # Check if the part title also contains an item number and include it\n",
    "        if \"Item\" in section.title and section.item_number:\n",
    "            return f\"{part_number} - Item {section.item_number}\"\n",
    "        return part_number\n",
    "\n",
    "    # Fallback for named_section, content, or document type sections\n",
    "    return section.title or \"Document Content\"\n",
    "\n",
    "\n",
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content: # Added check for empty content\n",
    "        logger.info(\"Empty content provided to detect_sections_universal_sec. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    # Using re.escape for literal brackets, and compiling patterns once.\n",
    "    # Corrected regex for pattern components\n",
    "    patterns = [\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Item\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Item\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s*([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?PART\\s*([IVX]+)\\s*' + re.escape('[TABLE_END]'), re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone ITEM patterns (fallback) - no re.escape for Item/Part\n",
    "        re.compile(r'^\\s*Item\\s*(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)', re.I | re.M),\n",
    "        re.compile(r'Item\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Standalone PART patterns (fallback) - no re.escape for Item/Part\n",
    "        re.compile(r'^\\s*PART\\s*([IVX]+)\\s*([^\\n]*)', re.I | re.M),\n",
    "        re.compile(r'PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Number-only patterns in tables - using re.escape for markers\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)', re.I | re.DOTALL),\n",
    "\n",
    "        # Headers that might appear standalone - no re.escape for Item/Part\n",
    "        re.compile(r'^(Item\\s*\\d{1,2}[A-C]?\\.\\s+[^|]+?)$', re.I | re.M),\n",
    "        re.compile(r'^(PART\\s*[IVX]+)(?:\\s*[-‚Äì‚Äî]\\s*(.+))?$', re.I | re.M),\n",
    "    ]\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in pattern.finditer(content): # Use pre-compiled pattern\n",
    "            # Get context around the match (full line)\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "\n",
    "            full_line = content[line_start:line_end].strip()\n",
    "\n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "\n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "\n",
    "            section_id = groups[0].strip() if groups else 'unknown'\n",
    "            section_title = \"\"\n",
    "\n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title from table markers - use actual markers here\n",
    "                section_title = re.sub(re.escape('[TABLE_END]') + r'.*', '', section_title, flags=re.I).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) == 1:\n",
    "                line_after_id_match = content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    section_title = line_after_id_match\n",
    "                else:\n",
    "                    section_title = f\"Section {section_id}\"\n",
    "            else:\n",
    "                section_title = full_line\n",
    "\n",
    "            all_matches.append({\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end(),\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "\n",
    "    # Sort matches by their start position\n",
    "    all_matches.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    # Remove duplicates and prioritize 'better' matches\n",
    "    final_matches = []\n",
    "    if all_matches:\n",
    "        final_matches.append(all_matches[0]) # Add the first match\n",
    "        for i in range(1, len(all_matches)):\n",
    "            current_match = all_matches[i]\n",
    "            last_added_match = final_matches[-1]\n",
    "\n",
    "            # Heuristic for deciding whether to add or replace:\n",
    "            # 1. If current match is significantly after the last added match, add it.\n",
    "            if current_match['start_pos'] - last_added_match['start_pos'] > 150: # Increased distance for new section\n",
    "                final_matches.append(current_match)\n",
    "            # 2. If current match is very close, but provides a more specific 'item' or 'part' ID\n",
    "            #    and the last added one was generic 'unknown' or less specific.\n",
    "            elif current_match['start_pos'] - last_added_match['start_pos'] < 50: # Close proximity\n",
    "                if last_added_match['section_id'] == 'unknown' and current_match['section_id'] != 'unknown':\n",
    "                    final_matches[-1] = current_match # Replace with more specific match\n",
    "                elif last_added_match['section_id'] == current_match['section_id'] and last_added_match['pattern_idx'] > current_match['pattern_idx']:\n",
    "                    # If same ID but new pattern has higher priority (lower index means earlier in list)\n",
    "                    final_matches[-1] = current_match\n",
    "\n",
    "    logger.info(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        logger.info(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "\n",
    "    # Convert to DocumentSection objects\n",
    "    final_document_sections = []\n",
    "    current_part = None # Track current part for 10Q item context\n",
    "\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        # End position is the start of the next matched section, or end of content if it's the last one\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "\n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "\n",
    "        section_id = match['section_id'].upper()\n",
    "        title = match['section_title']\n",
    "\n",
    "        section_type = 'content'\n",
    "        item_number = None\n",
    "        part = None\n",
    "\n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            current_part = part # Update current part\n",
    "            if title.upper().startswith(\"PART \") and title.upper().replace(\"PART \", \"\").strip() == section_id:\n",
    "                title = part\n",
    "            elif not title:\n",
    "                 title = part\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            section_type = 'item'\n",
    "            item_number = section_id\n",
    "            part = current_part # Inherit part from the last detected PART\n",
    "            if title.upper().startswith(\"ITEM \") and title.upper().replace(\"ITEM \", \"\").strip() == section_id:\n",
    "                title = f\"Item {item_number}\"\n",
    "            elif not title:\n",
    "                 title = f\"Item {item_number}\"\n",
    "\n",
    "        final_document_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part, # Store the part info\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "\n",
    "    return final_document_sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing.\n",
    "    This function primarily identifies section titles and item numbers from TOC,\n",
    "    but does not extract their content directly.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    if not content:\n",
    "        logger.info(\"Empty content provided to detect_sections_from_toc_universal. Returning empty sections.\")\n",
    "        return sections\n",
    "\n",
    "    # Look for table of contents patterns\n",
    "    # Using re.escape for literal brackets, and compiling patterns once.\n",
    "    toc_patterns = [\n",
    "        re.compile(r'(?i)INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)TABLE OF CONTENTS.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "        re.compile(re.escape('[TABLE_START]') + r'.*?Page.*?' + re.escape('[TABLE_END]') + r'.*?(?=\\s*--- PAGE BREAK ---)', re.DOTALL),\n",
    "    ]\n",
    "\n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "\n",
    "    if not toc_content:\n",
    "        logger.warning(\"No table of contents found in detect_sections_from_toc_universal.\")\n",
    "        return sections\n",
    "\n",
    "    logger.info(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "\n",
    "    # Define patterns for items/parts within the TOC\n",
    "    # CORRECTED: Removed re.escape from words like \"Item\" and \"PART\" within the patterns.\n",
    "    # Added flexibility for optional periods and variations in spacing.\n",
    "    item_patterns = [\n",
    "        # Match Item X. [Title] | Page\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\\\d+', re.DOTALL),\n",
    "        # Match PART X | [Title]\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)\\s*\\|\\s*([^|]+)', re.DOTALL),\n",
    "        # Match Item X. [Title] (no pipe/page)\n",
    "        re.compile(r'(?i)Item\\s*(\\d{1,2}[A-C]?)\\.?\\s*([^\\n|]+)', re.M),\n",
    "        # Match X. | [Title] | Page (number-dot format in table)\n",
    "        re.compile(r'(?i)(\\d{1,2}[A-C]?)\\.?\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\\\d+', re.DOTALL),\n",
    "        # Match simple PART X line\n",
    "        re.compile(r'(?i)PART\\s*([IVX]+)', re.M)\n",
    "    ]\n",
    "\n",
    "    found_items = []\n",
    "    # This loop also needs to handle empty toc_content if previous steps fail\n",
    "    if not toc_content: # Defensive check\n",
    "        return sections\n",
    "\n",
    "    for pattern in item_patterns:\n",
    "        for match in pattern.finditer(toc_content):\n",
    "            groups = match.groups()\n",
    "            item_id = None\n",
    "            item_title = \"\"\n",
    "\n",
    "            if len(groups) >= 2: # Pattern captured both ID and Title\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "            elif len(groups) == 1: # Pattern only captured ID (e.g., simple PART)\n",
    "                item_id = groups[0].strip()\n",
    "                # Attempt to get text immediately following the item_id if available, otherwise use a generic title\n",
    "                line_after_id_match = toc_content[match.end():].split('\\n')[0].strip()\n",
    "                if line_after_id_match:\n",
    "                    item_title = line_after_id_match\n",
    "                else:\n",
    "                    item_title = f\"Section {item_id}\"\n",
    "\n",
    "            if item_id: # Only add if we successfully got an ID\n",
    "                item_title = re.sub(r'\\\\s+', ' ', item_title).strip() # Normalize whitespace\n",
    "                found_items.append((item_id, item_title))\n",
    "\n",
    "\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:50]}\" # Use a longer slice for better uniqueness\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "\n",
    "    logger.info(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        logger.info(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "\n",
    "    toc_sections = []\n",
    "    current_part = None # Track current part for TOC items\n",
    "\n",
    "    for item_id, title in unique_items:\n",
    "        section_type = 'unknown'\n",
    "        item_number = None\n",
    "        part_num = None\n",
    "\n",
    "        if re.match(r'^\\d+[A-C]?$', item_id):\n",
    "            section_type = 'item'\n",
    "            item_number = item_id\n",
    "            part_num = current_part # Assign current part context\n",
    "        elif re.match(r'^[IVX]+$', item_id):\n",
    "            section_type = 'part'\n",
    "            part_num = f\"PART {item_id}\"\n",
    "            current_part = part_num # Update current part\n",
    "        else:\n",
    "            section_type = 'content' # Treat as generic content section if no item/part found\n",
    "\n",
    "        toc_sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=\"\", # Content is intentionally empty here; will be filled by main sectioning if this strategy is chosen.\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part_num # Store the identified part\n",
    "        ))\n",
    "    return toc_sections\n",
    "\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings.\n",
    "    Prioritizes direct pattern matching (which handles tables well), then TOC, then page-based.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "\n",
    "    # Strategy 1: Direct pattern matching for sections (designed to work well with common SEC patterns)\n",
    "    sections_strategy1 = detect_sections_universal_sec(content)\n",
    "\n",
    "    if len(sections_strategy1) >= 3:\n",
    "        logger.info(f\"Universal detection successful (Strategy 1): Found {len(sections_strategy1)} sections.\")\n",
    "        return sections_strategy1\n",
    "\n",
    "    # Strategy 2: Try parsing Table of Contents.\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents.\")\n",
    "    toc_entries = detect_sections_from_toc_universal(content) # These are DocumentSections with only title/metadata, no content\n",
    "\n",
    "    if toc_entries and len(toc_entries) >= 3:\n",
    "        logger.info(f\"TOC analysis found {len(toc_entries)} potential sections. Attempting to extract content based on TOC titles.\")\n",
    "\n",
    "        combined_sections = []\n",
    "        current_content_pos = 0\n",
    "\n",
    "        for i, toc_entry in enumerate(toc_entries):\n",
    "            # Create flexible regex for the title/item number to find it in the main content\n",
    "            pattern_parts = []\n",
    "            if toc_entry.item_number:\n",
    "                pattern_parts.append(r'Item\\s*' + re.escape(toc_entry.item_number) + r'\\.?') # \"Item 1.\" or \"Item 1A\"\n",
    "            if toc_entry.part:\n",
    "                pattern_parts.append(r'PART\\s*' + re.escape(toc_entry.part.replace(\"PART \", \"\"))) # \"PART I\"\n",
    "            \n",
    "            # Use the full title as a fallback if item/part number not found in text\n",
    "            if toc_entry.title:\n",
    "                pattern_parts.append(re.escape(toc_entry.title).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "            if not pattern_parts: # Should not happen if TOC parsing is good\n",
    "                continue\n",
    "\n",
    "            search_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(pattern_parts) + r')', re.M)\n",
    "            \n",
    "            match = search_pattern.search(content, pos=current_content_pos)\n",
    "\n",
    "            if match:\n",
    "                start_pos = match.start()\n",
    "\n",
    "                next_start_pos = len(content)\n",
    "                if i + 1 < len(toc_entries):\n",
    "                    next_toc_entry = toc_entries[i+1]\n",
    "                    next_pattern_parts = []\n",
    "                    if next_toc_entry.item_number:\n",
    "                        next_pattern_parts.append(r'Item\\s*' + re.escape(next_toc_entry.item_number) + r'\\.?')\n",
    "                    if next_toc_entry.part:\n",
    "                        next_pattern_parts.append(r'PART\\s*' + re.escape(next_toc_entry.part.replace(\"PART \", \"\")))\n",
    "                    if next_toc_entry.title:\n",
    "                        next_pattern_parts.append(re.escape(next_toc_entry.title).replace('\\\\ ', '\\\\s*'))\n",
    "\n",
    "                    if next_pattern_parts:\n",
    "                        next_pattern = re.compile(r'(?i)^\\s*(?:' + '|'.join(next_pattern_parts) + r')', re.M)\n",
    "                        next_match = next_pattern.search(content, pos=match.end())\n",
    "                        if next_match:\n",
    "                            next_start_pos = next_match.start()\n",
    "\n",
    "                section_content = content[start_pos:next_start_pos].strip()\n",
    "\n",
    "                combined_sections.append(DocumentSection(\n",
    "                    title=toc_entry.title,\n",
    "                    content=section_content,\n",
    "                    section_type=toc_entry.section_type,\n",
    "                    item_number=toc_entry.item_number,\n",
    "                    part=toc_entry.part,\n",
    "                    start_pos=start_pos,\n",
    "                    end_pos=next_start_pos\n",
    "                ))\n",
    "                current_content_pos = next_start_pos\n",
    "            else:\n",
    "                logger.warning(f\"Could not find content for TOC entry: '{toc_entry.title}'. This section might be merged with previous or skipped.\")\n",
    "\n",
    "        if len(combined_sections) >= 3:\n",
    "            logger.info(f\"Universal detection successful (TOC-based content mapping): Found {len(combined_sections)} sections.\")\n",
    "            return combined_sections\n",
    "        else:\n",
    "            logger.warning(\"TOC-based content mapping yielded few sections. Falling back to page-based detection.\")\n",
    "\n",
    "\n",
    "    # Strategy 3: Page-based fallback (original strategy 2)\n",
    "    logger.warning(\"Trying page-based detection as fallback.\")\n",
    "    sections_strategy2 = detect_sections_strategy_2(content)\n",
    "\n",
    "    if len(sections_strategy2) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections_strategy2)} sections.\")\n",
    "        return sections_strategy2\n",
    "\n",
    "    # Final fallback: return the entire document as a single section\n",
    "    logger.warning(\"All strategies failed, creating single section.\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "\n",
    "# Helper function to extract metadata from filename\n",
    "def extract_metadata_from_filename(file_path: str) -> FilingMetadata:\n",
    "    filename = Path(file_path).name\n",
    "    file_id = filename.replace(\".txt\", \"\")\n",
    "    parts = file_id.split('_')\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        logger.warning(f\"Malformed filename: {filename}. Using default metadata.\")\n",
    "        return FilingMetadata(\n",
    "            ticker=\"UNKNOWN\",\n",
    "            form_type=\"UNKNOWN\",\n",
    "            filing_date=\"1900-01-01\",\n",
    "            fiscal_year=1900,\n",
    "            fiscal_quarter=1,\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "    ticker, form_type, filing_date_str = parts\n",
    "\n",
    "    try:\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Could not parse filing date from {filing_date_str} in {filename}. Using default values.\")\n",
    "        fiscal_year = 1900\n",
    "        fiscal_quarter = 1\n",
    "\n",
    "    # Adjust fiscal year for 10-K filings if the filing date is early in the calendar year\n",
    "    # and typically refers to the previous fiscal year end.\n",
    "    if form_type == '10K' and filing_date.month <= 3: # Assuming fiscal year ends typically in Dec or Jan-Mar for previous year\n",
    "        fiscal_year -= 1 # Often a 10K filed in Jan-Mar of current year is for previous fiscal year\n",
    "\n",
    "    return FilingMetadata(\n",
    "        ticker=ticker,\n",
    "        form_type=form_type,\n",
    "        filing_date=filing_date_str,\n",
    "        fiscal_year=fiscal_year,\n",
    "        fiscal_quarter=fiscal_quarter,\n",
    "        file_path=file_path\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING FUNCTION (Universal)\n",
    "# =============================================================================\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filing_metadata = extract_metadata_from_filename(file_path)\n",
    "        filename = Path(file_path).name # For logging clarity\n",
    "        file_id = filename.replace(\".txt\", \"\") # For chunk_id creation\n",
    "\n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "\n",
    "        # Basic check for empty content after cleaning\n",
    "        if not cleaned_content.strip():\n",
    "            logger.warning(f\"Cleaned content for {filename} is empty. No chunks created.\")\n",
    "            return []\n",
    "\n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "\n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        for section in sections:\n",
    "            # Ensure section.content is not empty before processing\n",
    "            if not section.content.strip():\n",
    "                continue # Skip empty sections\n",
    "\n",
    "            # Extract tables and narrative from this section's content\n",
    "            tables_in_section, narrative_content_in_section = extract_and_process_tables(section.content)\n",
    "\n",
    "            # Create section info string using the original create_section_info\n",
    "            section_info = create_section_info(section, filing_metadata.form_type)\n",
    "\n",
    "            # Process tables found within this section\n",
    "            for table in tables_in_section:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "\n",
    "            # Process narrative content from this section\n",
    "            if narrative_content_in_section.strip():\n",
    "                # Use the existing create_overlapping_chunks for narrative\n",
    "                narrative_sub_chunks = create_overlapping_chunks(\n",
    "                    narrative_content_in_section, target_tokens, overlap_tokens\n",
    "                )\n",
    "\n",
    "                for chunk_data in narrative_sub_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "\n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "\n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100,\n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            current_overlap_tokens = 0 # Renamed variable to avoid conflict with function parameter 'overlap_tokens'\n",
    "\n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            # Ensure we don't go past the start of the chunk\n",
    "            for sent_idx in range(len(current_chunk_sentences) - 1, -1, -1):\n",
    "                sent = current_chunk_sentences[sent_idx]\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if current_overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    current_overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # If after trying to create overlap, we still don't have enough tokens for overlap\n",
    "            # (e.g., first few sentences are very long), just take some minimal content.\n",
    "            if not overlap_sentences and current_chunk_sentences:\n",
    "                # Fallback to last sentence if no other overlap possible and current chunk exists\n",
    "                overlap_sentences = [current_chunk_sentences[-1]]\n",
    "                current_overlap_tokens = len(encoding.encode(overlap_sentences[0]))\n",
    "\n",
    "\n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = current_overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "\n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "\n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "\n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "\n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "\n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\\n\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\\n\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\\n\")\n",
    "print(\"‚úÖ Robust error handling and logging\\n\")\n",
    "print(\"‚úÖ Structured data classes for better organization\\n\")\n",
    "print(\"‚úÖ Quality validation and statistics\\n\")\n",
    "print(\"‚úÖ Separate table and narrative processing\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\\n\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "\n",
    "        print(\"üìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(\"\\nüìù Sample Chunks:\\n\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\\n\")\n",
    "            print(f\"  Section: {chunk.section_info}\\n\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\\n\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\\n\")\n",
    "\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\\n\")\n",
    "        print(\"Please update the file path to match your data structure\\n\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n",
    "\n",
    "def compare_section_strategies(content: str): # Changed content_sample to content to use full content\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\\n\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content) # Changed content_sample to content\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\\n\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\\n\")\n",
    "\n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        # Load full content for comparison, not just a sample\n",
    "        full_content_for_comparison = f.read()\n",
    "    cleaned_content_for_comparison = clean_sec_text(full_content_for_comparison) # Clean it for consistent comparison\n",
    "\n",
    "    sections_1_comp, sections_2_comp = compare_section_strategies(cleaned_content_for_comparison)\n",
    "\n",
    "\n",
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üìä Chunking Quality Analysis\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "\n",
    "    print(f\"Token Distribution:\\n\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\\n\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\\n\")\n",
    "    print(f\"  Min: {min(token_counts)}\\n\")\n",
    "    print(f\"  Max: {max(token_counts)}\\n\")\n",
    "\n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "\n",
    "    print(f\"\\nChunk Types:\\n\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\\n\")\n",
    "\n",
    "    # Section distribution\n",
    "    sections_dist = {} # Renamed to avoid conflict with `sections` list\n",
    "    for chunk in chunks:\n",
    "        sections_dist[chunk.section_info] = sections_dist.get(chunk.section_info, 0) + 1\n",
    "\n",
    "    print(f\"\\nSection Distribution:\\n\")\n",
    "    for section, count in sorted(sections_dist.items()):\n",
    "        print(f\"  {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\\n\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\\n\")\n",
    "\n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections_dist,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)\n",
    "\n",
    "\n",
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\\n\")\n",
    "        return\n",
    "\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "\n",
    "    print(\"üîß Testing Different Chunking Parameters\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\\n\")\n",
    "        # Changed to universal processing function\n",
    "        test_chunks = process_filing_robust_universal(\n",
    "            test_file,\n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "\n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\\n\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\\n\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()\n",
    "\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\\n\")\n",
    "    # Changed to universal processing function\n",
    "    fake_chunks = process_filing_robust_universal(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\\n\")\n",
    "    empty_sections = detect_sections_robust_universal(\"\") # Changed to universal detection\n",
    "    print(f\"  Result: {len(empty_sections)} sections\\n\")\n",
    "\n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\\n\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "\n",
    "    # Changed to universal processing function\n",
    "    bad_chunks = process_filing_robust_universal(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\\n\")\n",
    "\n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "\n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\\n\")\n",
    "    # This call is correct, as create_overlapping_chunks is a helper\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\\n\")\n",
    "\n",
    "test_error_handling()\n",
    "\n",
    "\n",
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\\n\")\n",
    "        return []\n",
    "\n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\\n\")\n",
    "\n",
    "        # Changed to universal processing function\n",
    "        file_chunks = process_filing_robust_universal(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "\n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\\n\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "\n",
    "    print(f\"  Total files processed: {len(all_results)}\\n\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\\n\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Per-file results:\\n\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)\n",
    "\n",
    "\n",
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Assumes 'chunks' variable from test_single_file() is available\n",
    "    if 'chunks' not in globals() or not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\\n\")\n",
    "        return\n",
    "\n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "\n",
    "    print(\"üéØ Key Insights:\\n\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\\n\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\\n\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\\n\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\\n\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\\n\")\n",
    "\n",
    "    print(f\"\\nüìö Section Breakdown:\\n\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\\n\")\n",
    "\n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\\n\")\n",
    "\n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\\n\")\n",
    "\n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\\n\")\n",
    "\n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        # Find the actual chunk object to get its full text\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\\n\")\n",
    "        print(f\"    Section: {sample['section']}\\n\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()\n",
    "\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "\n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Key Improvements:\\n\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\\n\")\n",
    "\n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\\n\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\\n\")\n",
    "\n",
    "    print(f\"\\nüéØ Recommended Next Steps:\\n\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Preprocessing Strategy Testing Complete!\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Next step: Convert this notebook into modular Python files\\n\")\n",
    "    print(\"Then: Implement the embedding pipeline and MCP server!\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "# Test functions adapted to _fixed suffix to avoid NameErrors from notebook re-runs\n",
    "# Ensure these are called after all function definitions.\n",
    "print(\"üöÄ Ready to test universal SEC detection!\\n\")\n",
    "print(\"\\n1. Run test_universal_detection_fixed() to test all files\\n\")\n",
    "print(\"2. Run compare_old_vs_universal_fixed() to see the improvement\\n\")\n",
    "print(\"3. Run quick_pattern_test_fixed() to see what patterns match\\n\")\n",
    "\n",
    "# Define the _fixed test functions so they are available when called below\n",
    "def test_universal_detection_fixed():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\", # This file name is in the future based on current date\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Testing: {test_file}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\\n\")\n",
    "        for i, section in enumerate(sections[:10]):\n",
    "            print(f\"  {i+1}. {section.title}\\n\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\\n\")\n",
    "\n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "\n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Processing Results:\\n\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        if chunks:\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "\n",
    "            print(f\"\\nüìö Section Distribution (sample):\\n\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\\n\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal_fixed():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Running old detection...\\n\")\n",
    "    old_sections = detect_sections_robust_old(content)\n",
    "\n",
    "    print(\"Running universal detection...\\n\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "\n",
    "    print(f\"\\nüìä Comparison Results:\\n\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\\n\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\\n\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Old Sections:\\n\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    print(f\"\\nüìã Universal Sections:\\n\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\\n\")\n",
    "\n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test_fixed():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"üîç QUICK PATTERN TEST\\n\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    patterns = [\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?Item(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Items\"),\n",
    "        (re.compile(r'Item\\s+\\d+[A-C]?\\.\\s*\\|', re.I), \"Pipe-separated Items\"),\n",
    "        (re.compile(r'PART\\s+[IVX]+', re.I), \"Part headers\"),\n",
    "        (re.compile(r'\\[TABLE_START\\](?:.|\\n)*?PART(?:.|\\n)*?\\[TABLE_END\\]', re.I | re.DOTALL), \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "\n",
    "    for compiled_pattern, description in patterns:\n",
    "        matches = compiled_pattern.findall(content)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\\n\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\\n\")\n",
    "\n",
    "# Run the fixed tests\n",
    "results_universal = test_universal_detection_fixed()\n",
    "old_vs_new_sections = compare_old_vs_universal_fixed()\n",
    "quick_pattern_test_fixed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take-home-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
