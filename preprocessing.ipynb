{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6da480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing fixed section detection...\n",
      "Add this to your notebook and replace the detect_sections_robust function\n",
      "üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\n",
      "============================================================\n",
      "Key improvements over original approach:\n",
      "‚úÖ Multi-strategy section detection with fallbacks\n",
      "‚úÖ Sentence-aware chunking with overlap\n",
      "‚úÖ Robust error handling and logging\n",
      "‚úÖ Structured data classes for better organization\n",
      "‚úÖ Quality validation and statistics\n",
      "‚úÖ Separate table and narrative processing\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SEC Filing Preprocessing Strategy - From Scratch\n",
    "# Let's build a robust chunking approach step by step\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. IMPROVED SEC MAPPINGS WITH FALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "# Keep the excellent domain knowledge from the original\n",
    "ITEM_NAME_MAP_10K = {\n",
    "    \"1\": \"Business\", \n",
    "    \"1A\": \"Risk Factors\", \n",
    "    \"1B\": \"Unresolved Staff Comments\", \n",
    "    \"1C\": \"Cybersecurity\",\n",
    "    \"2\": \"Properties\", \n",
    "    \"3\": \"Legal Proceedings\", \n",
    "    \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\",\n",
    "    \"6\": \"Reserved\", \n",
    "    \"7\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"7A\": \"Quantitative and Qualitative Disclosures About Market Risk\", \n",
    "    \"8\": \"Financial Statements and Supplementary Data\",\n",
    "    \"9\": \"Changes in and Disagreements With Accountants on Accounting and Financial Disclosure\", \n",
    "    \"9A\": \"Controls and Procedures\",\n",
    "    \"9B\": \"Other Information\", \n",
    "    \"9C\": \"Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\",\n",
    "    \"10\": \"Directors, Executive Officers and Corporate Governance\", \n",
    "    \"11\": \"Executive Compensation\",\n",
    "    \"12\": \"Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\",\n",
    "    \"13\": \"Certain Relationships and Related Transactions, and Director Independence\", \n",
    "    \"14\": \"Principal Accountant Fees and Services\",\n",
    "    \"15\": \"Exhibits, Financial Statement Schedules\", \n",
    "    \"16\": \"Form 10-K Summary\"\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_I = {\n",
    "    \"1\": \"Financial Statements\",\n",
    "    \"2\": \"Management's Discussion and Analysis of Financial Condition and Results of Operations\",\n",
    "    \"3\": \"Quantitative and Qualitative Disclosures About Market Risk\",\n",
    "    \"4\": \"Controls and Procedures\",\n",
    "}\n",
    "\n",
    "ITEM_NAME_MAP_10Q_PART_II = {\n",
    "    \"1\": \"Legal Proceedings\", \"1A\": \"Risk Factors\",\n",
    "    \"2\": \"Unregistered Sales of Equity Securities and Use of Proceeds\",\n",
    "    \"3\": \"Defaults Upon Senior Securities\", \"4\": \"Mine Safety Disclosures\",\n",
    "    \"5\": \"Other Information\", \"6\": \"Exhibits\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA STRUCTURES FOR BETTER ORGANIZATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FilingMetadata:\n",
    "    \"\"\"Structured metadata for a filing\"\"\"\n",
    "    ticker: str\n",
    "    form_type: str\n",
    "    filing_date: str\n",
    "    fiscal_year: int\n",
    "    fiscal_quarter: int\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"Represents a section of the document\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    section_type: str  # 'item', 'part', 'intro', 'table'\n",
    "    item_number: Optional[str] = None\n",
    "    part: Optional[str] = None\n",
    "    start_pos: int = 0\n",
    "    end_pos: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Final chunk with all metadata\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    token_count: int\n",
    "    chunk_type: str  # 'narrative', 'table', 'mixed'\n",
    "    section_info: str\n",
    "    filing_metadata: FilingMetadata\n",
    "    chunk_index: int\n",
    "    has_overlap: bool = False\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ROBUST TEXT CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def clean_sec_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SEC filing text more robustly\n",
    "    \"\"\"\n",
    "    # Remove common SEC artifacts\n",
    "    text = re.sub(r'UNITED STATES\\s+SECURITIES AND EXCHANGE COMMISSION.*?FORM \\d+[A-Z]*', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Handle page breaks more intelligently\n",
    "    text = text.replace('[PAGE BREAK]', '\\n\\n--- PAGE BREAK ---\\n\\n')\n",
    "    \n",
    "    # Preserve table boundaries but clean them up\n",
    "    text = re.sub(r'\\[TABLE_START\\]', '\\n\\n=== TABLE START ===\\n', text)\n",
    "    text = re.sub(r'\\[TABLE_END\\]', '\\n=== TABLE END ===\\n\\n', text)\n",
    "    \n",
    "    # Clean up excessive whitespace but preserve paragraph structure\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines -> double newline\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs -> single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text, flags=re.MULTILINE)  # Trim lines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MULTI-STRATEGY SECTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "def detect_sections_strategy_1_improved(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Improved Strategy 1: Patterns based on real SEC filing structure\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Much more comprehensive patterns based on your actual files\n",
    "    patterns = [\n",
    "        # PART patterns - handle various formats\n",
    "        r'(?im)^\\s*PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$',\n",
    "        r'(?im)^PART\\s+([IVX]+)(?:\\s*[-‚Äì‚Äî].*?)?$',\n",
    "        \n",
    "        # ITEM patterns - much more flexible\n",
    "        r'(?im)^\\s*ITEM\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        r'(?im)^ITEM\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        r'(?im)Item\\s+(\\d{1,2}[A-C]?)(?:\\.|\\s|[-‚Äì‚Äî])',\n",
    "        \n",
    "        # Number-dot format common in SEC filings\n",
    "        r'(?im)^(\\d{1,2}[A-C]?)\\.\\s+[A-Z][A-Za-z\\s]{10,}',\n",
    "        \n",
    "        # Content-based patterns for known sections\n",
    "        r'(?im)^.{0,50}(BUSINESS)\\s*$',\n",
    "        r'(?im)^.{0,50}(RISK FACTORS)\\s*$',\n",
    "        r'(?im)^.{0,50}(LEGAL PROCEEDINGS)\\s*$',\n",
    "        r'(?im)^.{0,50}(FINANCIAL STATEMENTS)\\s*$',\n",
    "        r'(?im)^.{0,50}(MANAGEMENT.S DISCUSSION)\\s*',\n",
    "        r'(?im)^.{0,50}(PROPERTIES)\\s*$',\n",
    "        r'(?im)^.{0,50}(CONTROLS AND PROCEDURES)\\s*$',\n",
    "    ]\n",
    "    \n",
    "    all_matches = []\n",
    "    \n",
    "    # Process each pattern\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in re.finditer(pattern, content):\n",
    "            # Get the full line containing this match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "            \n",
    "            full_line = content[line_start:line_end].strip()\n",
    "            \n",
    "            # Filter out obvious false positives\n",
    "            if (len(full_line) > 400 or  # Too long to be a header\n",
    "                len(full_line) < 3 or    # Too short\n",
    "                '|' in full_line or      # Likely table content\n",
    "                full_line.count(' ') > 20):  # Too many words\n",
    "                continue\n",
    "            \n",
    "            # Extract section identifier\n",
    "            section_id = match.group(1) if match.groups() else 'unknown'\n",
    "            \n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates - matches within 200 characters of each other\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 200 \n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"üîç Improved detection found {len(unique_matches)} potential sections:\")\n",
    "    for i, match in enumerate(unique_matches[:15]):  # Show more for debugging\n",
    "        print(f\"  {i+1}: {match['full_line'][:80]}...\")\n",
    "    \n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(unique_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = unique_matches[i + 1]['start_pos'] if i + 1 < len(unique_matches) else len(content)\n",
    "        \n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Determine section type and metadata\n",
    "        full_line_upper = match['full_line'].upper()\n",
    "        section_id = match['section_id'].upper() if match['section_id'] != 'unknown' else None\n",
    "        \n",
    "        if 'PART' in full_line_upper and section_id:\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "        elif ('ITEM' in full_line_upper or re.match(r'^\\d+[A-C]?$', str(section_id))) and section_id:\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "        elif any(keyword in full_line_upper for keyword in \n",
    "                ['BUSINESS', 'RISK', 'LEGAL', 'FINANCIAL', 'MANAGEMENT', 'PROPERTIES', 'CONTROLS']):\n",
    "            section_type = 'named_section'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        else:\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['full_line']\n",
    "        \n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Update the main detect_sections_robust function\n",
    "def detect_sections_robust_fixed(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Updated robust detection with the fixed strategy\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Fixed Strategy 1: Improved regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1_fixed(content)\n",
    "    \n",
    "    if len(sections) >= 3:  # Good result\n",
    "        logger.info(f\"Fixed Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"Fixed Strategy 1 found few sections, trying Strategy 2\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "    \n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# Test the fixed detection\n",
    "print(\"üß™ Testing fixed section detection...\")\n",
    "print(\"Add this to your notebook and replace the detect_sections_robust function\")\n",
    "\n",
    "\n",
    "\n",
    "def detect_sections_strategy_2(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Fallback using page breaks and heuristics\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Split by page breaks first\n",
    "    pages = content.split('--- PAGE BREAK ---')\n",
    "    \n",
    "    current_section = \"\"\n",
    "    current_title = \"Document Content\"\n",
    "    \n",
    "    for i, page in enumerate(pages):\n",
    "        page = page.strip()\n",
    "        if not page:\n",
    "            continue\n",
    "            \n",
    "        # Look for section headers in the page\n",
    "        lines = page.split('\\n')\n",
    "        potential_headers = []\n",
    "        \n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines of each page\n",
    "            line = line.strip()\n",
    "            if (len(line) < 100 and  # Headers are usually short\n",
    "                (re.search(r'\\b(ITEM|PART)\\b', line, re.IGNORECASE) or\n",
    "                 re.search(r'\\b(BUSINESS|RISK FACTORS|FINANCIAL STATEMENTS)\\b', line, re.IGNORECASE))):\n",
    "                potential_headers.append((j, line))\n",
    "        \n",
    "        if potential_headers:\n",
    "            # Found a header, start new section\n",
    "            if current_section:\n",
    "                sections.append(DocumentSection(\n",
    "                    title=current_title,\n",
    "                    content=current_section.strip(),\n",
    "                    section_type='content',\n",
    "                    start_pos=0,\n",
    "                    end_pos=len(current_section)\n",
    "                ))\n",
    "            \n",
    "            current_title = potential_headers[0][1]\n",
    "            current_section = page\n",
    "        else:\n",
    "            # Continue current section\n",
    "            current_section += \"\\n\\n\" + page\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section:\n",
    "        sections.append(DocumentSection(\n",
    "            title=current_title,\n",
    "            content=current_section.strip(),\n",
    "            section_type='content',\n",
    "            start_pos=0,\n",
    "            end_pos=len(current_section)\n",
    "        ))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def detect_sections_robust(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Multi-strategy section detection with fallbacks\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting Strategy 1: Regex-based section detection\")\n",
    "    sections = detect_sections_strategy_1(content)\n",
    "    \n",
    "    if len(sections) >= 3:  # Good result\n",
    "        logger.info(f\"Strategy 1 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"Strategy 1 failed, trying Strategy 2: Page-based detection\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "    \n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Strategy 2 successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# =============================================================================\n",
    "# 5. IMPROVED SENTENCE-AWARE CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using multiple heuristics\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (can be improved with spaCy/NLTK)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    \n",
    "    # Clean up sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def create_overlapping_chunks(text: str, target_tokens: int = 500, overlap_tokens: int = 100, \n",
    "                            min_tokens: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create semantically aware chunks with overlap\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "    \n",
    "    current_chunk_sentences = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = len(encoding.encode(sentence))\n",
    "        \n",
    "        # If adding this sentence exceeds target, finalize current chunk\n",
    "        if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "            \n",
    "            # Create overlap: keep last few sentences\n",
    "            overlap_sentences = []\n",
    "            overlap_tokens = 0\n",
    "            \n",
    "            # Add sentences from the end until we reach overlap target\n",
    "            for sent in reversed(current_chunk_sentences):\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                if overlap_tokens + sent_tokens <= overlap_tokens:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Start new chunk with overlap + current sentence\n",
    "            current_chunk_sentences = overlap_sentences + [sentence]\n",
    "            current_tokens = overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    # Add final chunk if it has content\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = ' '.join(current_chunk_sentences)\n",
    "        final_tokens = len(encoding.encode(chunk_text))\n",
    "        \n",
    "        if final_tokens >= min_tokens:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': final_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'has_overlap': len(chunks) > 0\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TABLE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def extract_and_process_tables(content: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Extract tables and return both table chunks and narrative text\n",
    "    \"\"\"\n",
    "    table_pattern = re.compile(r'=== TABLE START ===.*?=== TABLE END ===', re.DOTALL)\n",
    "    tables = []\n",
    "    \n",
    "    # Find all tables\n",
    "    for i, match in enumerate(table_pattern.finditer(content)):\n",
    "        table_content = match.group(0)\n",
    "        # Clean table markers\n",
    "        table_text = table_content.replace('=== TABLE START ===', '').replace('=== TABLE END ===', '').strip()\n",
    "        \n",
    "        if table_text:  # Only add non-empty tables\n",
    "            tables.append({\n",
    "                'text': table_text,\n",
    "                'token_count': len(encoding.encode(table_text)),\n",
    "                'table_index': i,\n",
    "                'chunk_type': 'table'\n",
    "            })\n",
    "    \n",
    "    # Remove tables from content to get narrative text\n",
    "    narrative_content = table_pattern.sub('', content).strip()\n",
    "    \n",
    "    return tables, narrative_content\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MAIN PROCESSING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def process_filing_robust(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Main function that puts it all together\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "        parts = file_id.split('_')\n",
    "        \n",
    "        if len(parts) != 3:\n",
    "            logger.error(f\"Invalid filename format: {filename}\")\n",
    "            return []\n",
    "        \n",
    "        ticker, form_type, filing_date_str = parts\n",
    "        \n",
    "        # Create filing metadata\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "        \n",
    "        # Adjust fiscal year for 10-K filings\n",
    "        if form_type == '10K' and filing_date.month < 4:\n",
    "            fiscal_year -= 1\n",
    "        \n",
    "        filing_metadata = FilingMetadata(\n",
    "            ticker=ticker,\n",
    "            form_type=form_type,\n",
    "            filing_date=filing_date_str,\n",
    "            fiscal_year=fiscal_year,\n",
    "            fiscal_quarter=fiscal_quarter,\n",
    "            file_path=file_path\n",
    "        )\n",
    "        \n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        \n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "        \n",
    "        # Detect sections\n",
    "        sections = detect_sections_robust(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "        \n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "        \n",
    "        for section in sections:\n",
    "            # Extract tables from this section\n",
    "            tables, narrative_content = extract_and_process_tables(section.content)\n",
    "            \n",
    "            # Create section info string\n",
    "            section_info = create_section_info(section, form_type)\n",
    "            \n",
    "            # Process tables\n",
    "            for table in tables:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "            \n",
    "            # Process narrative content\n",
    "            if narrative_content.strip():\n",
    "                narrative_chunks = create_overlapping_chunks(\n",
    "                    narrative_content, target_tokens, overlap_tokens\n",
    "                )\n",
    "                \n",
    "                for chunk_data in narrative_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "        \n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_section_info(section: DocumentSection, form_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Create human-readable section information\n",
    "    \"\"\"\n",
    "    if section.section_type == 'item' and section.item_number:\n",
    "        if form_type == '10K':\n",
    "            item_name = ITEM_NAME_MAP_10K.get(section.item_number, \"Unknown Section\")\n",
    "            return f\"Item {section.item_number} - {item_name}\"\n",
    "        elif form_type == '10Q':\n",
    "            # Determine which part this item belongs to\n",
    "            if section.item_number in ITEM_NAME_MAP_10Q_PART_I:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_I[section.item_number]\n",
    "                return f\"Part I, Item {section.item_number} - {item_name}\"\n",
    "            else:\n",
    "                item_name = ITEM_NAME_MAP_10Q_PART_II.get(section.item_number, \"Unknown Section\")\n",
    "                return f\"Part II, Item {section.item_number} - {item_name}\"\n",
    "    \n",
    "    elif section.section_type == 'part' and section.part:\n",
    "        return section.part\n",
    "    \n",
    "    else:\n",
    "        return section.title or \"Document Content\"\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TESTING AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_chunks(chunks: List[Chunk]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of our chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks created\"}\n",
    "    \n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "    \n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"chunks_with_overlap\": sum(1 for chunk in chunks if chunk.has_overlap),\n",
    "        \"table_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'table'),\n",
    "        \"narrative_chunks\": sum(1 for chunk in chunks if chunk.chunk_type == 'narrative'),\n",
    "        \"unique_sections\": len(set(chunk.section_info for chunk in chunks))\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LET'S TEST THIS!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ SEC Filing Preprocessing Strategy - Ready for Testing!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original approach:\")\n",
    "print(\"‚úÖ Multi-strategy section detection with fallbacks\")\n",
    "print(\"‚úÖ Sentence-aware chunking with overlap\")\n",
    "print(\"‚úÖ Robust error handling and logging\")\n",
    "print(\"‚úÖ Structured data classes for better organization\")\n",
    "print(\"‚úÖ Quality validation and statistics\")\n",
    "print(\"‚úÖ Separate table and narrative processing\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb547fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 152 chunks for AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "==================================================\n",
      "üìä Processing Results:\n",
      "  total_chunks: 152\n",
      "  avg_tokens: 365.07236842105266\n",
      "  min_tokens: 38\n",
      "  max_tokens: 1692\n",
      "  chunks_with_overlap: 85\n",
      "  table_chunks: 66\n",
      "  narrative_chunks: 86\n",
      "  unique_sections: 1\n",
      "\n",
      "üìù Sample Chunks:\n",
      "\n",
      "Chunk 1 (table):\n",
      "  Section: Full Document\n",
      "  Tokens: 58\n",
      "  Text preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way | Cupertino | , | California | 95014 | (Address o...\n",
      "\n",
      "Chunk 2 (table):\n",
      "  Section: Full Document\n",
      "  Tokens: 240\n",
      "  Text preview: Title of each class | Trading symbol(s) | Name of each exchange on which registered | Common Stock, $0.00001 par value per share | AAPL | The Nasdaq Stock Market LLC | 1.000% Notes due 2022 | ‚Äî | The ...\n",
      "\n",
      "Chunk 3 (table):\n",
      "  Section: Full Document\n",
      "  Tokens: 41\n",
      "  Text preview: Large accelerated filer | ‚òí | Accelerated filer | ‚òê | Non-accelerated filer | ‚òê | Smaller reporting company | ‚òê | Emerging growth company | ‚òê...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Test with a single file\n",
    "# =============================================================================\n",
    "def test_single_file():\n",
    "    \"\"\"Test our preprocessing on a single file\"\"\"\n",
    "    # Replace with an actual file path from your processed_filings directory\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"üß™ Testing with: {test_file}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        chunks = process_filing_robust(test_file)\n",
    "        stats = validate_chunks(chunks)\n",
    "        \n",
    "        print(\"üìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(\"\\nüìù Sample Chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "            print(f\"\\nChunk {i+1} ({chunk.chunk_type}):\")\n",
    "            print(f\"  Section: {chunk.section_info}\")\n",
    "            print(f\"  Tokens: {chunk.token_count}\")\n",
    "            print(f\"  Text preview: {chunk.text[:200]}...\")\n",
    "        \n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {test_file}\")\n",
    "        print(\"Please update the file path to match your data structure\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "chunks = test_single_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf43460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comparing Section Detection Strategies\n",
      "==================================================\n",
      "üîç Improved detection found 1 potential sections:\n",
      "  1: PART I...\n",
      "Strategy 1 (Regex): 1 sections\n",
      "  1. Part I...\n",
      "\n",
      "Strategy 2 (Page-based): 1 sections\n",
      "  1. Document Content...\n"
     ]
    }
   ],
   "source": [
    "def compare_section_strategies(content_sample: str):\n",
    "    \"\"\"Compare how different strategies perform\"\"\"\n",
    "    print(\"üîç Comparing Section Detection Strategies\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Strategy 1: Robust regex\n",
    "    sections_1 = detect_sections_strategy_1_improved(content_sample)\n",
    "    print(f\"Strategy 1 (Regex): {len(sections_1)} sections\")\n",
    "    for i, section in enumerate(sections_1[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Strategy 2: Page-based fallback\n",
    "    sections_2 = detect_sections_strategy_2(content_sample)\n",
    "    print(f\"Strategy 2 (Page-based): {len(sections_2)} sections\")\n",
    "    for i, section in enumerate(sections_2[:5]):  # Show first 5\n",
    "        print(f\"  {i+1}. {section.title[:60]}...\")\n",
    "    \n",
    "    return sections_1, sections_2\n",
    "\n",
    "# Test if we have chunks from previous test\n",
    "if chunks:\n",
    "    # Use the first chunk's filing to get the full content\n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        sample_content = f.read()[:10000]  # First 10k characters\n",
    "    \n",
    "    sections_1, sections_2 = compare_section_strategies(sample_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a7bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunking Quality Analysis\n",
      "==================================================\n",
      "Token Distribution:\n",
      "  Mean: 365.1\n",
      "  Median: 435\n",
      "  Min: 38\n",
      "  Max: 1692\n",
      "\n",
      "Chunk Types:\n",
      "  table: 66\n",
      "  narrative: 86\n",
      "\n",
      "Section Distribution:\n",
      "  Full Document: 152 chunks\n",
      "\n",
      "Overlap Analysis:\n",
      "  Chunks with overlap: 85/152 (55.9%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_chunking_quality(chunks: List[Chunk]):\n",
    "    \"\"\"Deep dive into chunk quality\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Chunking Quality Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Token distribution\n",
    "    token_counts = [chunk.token_count for chunk in chunks]\n",
    "    \n",
    "    print(f\"Token Distribution:\")\n",
    "    print(f\"  Mean: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "    print(f\"  Median: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    print(f\"  Min: {min(token_counts)}\")\n",
    "    print(f\"  Max: {max(token_counts)}\")\n",
    "    \n",
    "    # Chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in chunks:\n",
    "        chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunk Types:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "    \n",
    "    # Section distribution\n",
    "    sections = {}\n",
    "    for chunk in chunks:\n",
    "        sections[chunk.section_info] = sections.get(chunk.section_info, 0) + 1\n",
    "    \n",
    "    print(f\"\\nSection Distribution:\")\n",
    "    for section, count in sorted(sections.items()):\n",
    "        print(f\"  {section}: {count} chunks\")\n",
    "    \n",
    "    # Overlap analysis\n",
    "    overlap_count = sum(1 for chunk in chunks if chunk.has_overlap)\n",
    "    print(f\"\\nOverlap Analysis:\")\n",
    "    print(f\"  Chunks with overlap: {overlap_count}/{len(chunks)} ({overlap_count/len(chunks)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'token_stats': {\n",
    "            'mean': sum(token_counts)/len(token_counts),\n",
    "            'median': sorted(token_counts)[len(token_counts)//2],\n",
    "            'min': min(token_counts),\n",
    "            'max': max(token_counts)\n",
    "        },\n",
    "        'chunk_types': chunk_types,\n",
    "        'sections': sections,\n",
    "        'overlap_rate': overlap_count/len(chunks)\n",
    "    }\n",
    "\n",
    "# Analyze our test chunks\n",
    "if chunks:\n",
    "    quality_analysis = analyze_chunking_quality(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fbec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 214 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 152 chunks for AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "INFO:__main__:Created 117 chunks for AAPL_10K_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing Different Chunking Parameters\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Small chunks, low overlap\n",
      "  Total chunks: 214\n",
      "  Avg tokens: 259.3\n",
      "  Overlap rate: 147/214\n",
      "\n",
      "üß™ Testing: Medium chunks, medium overlap\n",
      "  Total chunks: 152\n",
      "  Avg tokens: 365.1\n",
      "  Overlap rate: 85/152\n",
      "\n",
      "üß™ Testing: Large chunks, high overlap\n",
      "  Total chunks: 117\n",
      "  Avg tokens: 474.3\n",
      "  Overlap rate: 50/117\n"
     ]
    }
   ],
   "source": [
    "def test_chunking_parameters():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No test file processed yet\")\n",
    "        return\n",
    "    \n",
    "    test_file = chunks[0].filing_metadata.file_path\n",
    "    \n",
    "    print(\"üîß Testing Different Chunking Parameters\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test different parameter combinations\n",
    "    param_configs = [\n",
    "        {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Small chunks, low overlap\"},\n",
    "        {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Medium chunks, medium overlap\"},\n",
    "        {\"target_tokens\": 800, \"overlap_tokens\": 150, \"name\": \"Large chunks, high overlap\"},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in param_configs:\n",
    "        print(f\"\\nüß™ Testing: {config['name']}\")\n",
    "        test_chunks = process_filing_robust(\n",
    "            test_file, \n",
    "            target_tokens=config['target_tokens'],\n",
    "            overlap_tokens=config['overlap_tokens']\n",
    "        )\n",
    "        \n",
    "        stats = validate_chunks(test_chunks)\n",
    "        results[config['name']] = stats\n",
    "        \n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\")\n",
    "        print(f\"  Overlap rate: {stats['chunks_with_overlap']}/{stats['total_chunks']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different parameters\n",
    "param_results = test_chunking_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f9e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error processing non_existent_file.txt: Unknown datetime string format, unable to parse: file, at position 0\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "ERROR:__main__:Error processing /var/folders/pj/bmp5122d3d77bzq_cvf0wbl40000gn/T/tmp4dqo7j8s_bad_name.txt: Unknown datetime string format, unable to parse: name, at position 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Testing Error Handling\n",
      "==================================================\n",
      "Test 1: Non-existent file\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 2: Empty content\n",
      "  Result: 1 sections\n",
      "\n",
      "Test 3: Malformed filename\n",
      "  Result: 0 chunks (expected 0)\n",
      "\n",
      "Test 4: Very short text\n",
      "  Result: 0 chunks\n"
     ]
    }
   ],
   "source": [
    "def test_error_handling():\n",
    "    \"\"\"Test how our system handles various edge cases\"\"\"\n",
    "    print(\"üõ°Ô∏è Testing Error Handling\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test 1: Non-existent file\n",
    "    print(\"Test 1: Non-existent file\")\n",
    "    fake_chunks = process_filing_robust(\"non_existent_file.txt\")\n",
    "    print(f\"  Result: {len(fake_chunks)} chunks (expected 0)\")\n",
    "    \n",
    "    # Test 2: Empty file\n",
    "    print(\"\\nTest 2: Empty content\")\n",
    "    empty_sections = detect_sections_robust(\"\")\n",
    "    print(f\"  Result: {len(empty_sections)} sections\")\n",
    "    \n",
    "    # Test 3: Malformed filename\n",
    "    print(\"\\nTest 3: Malformed filename\")\n",
    "    # Create a temporary file with bad name\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='_bad_name.txt', delete=False) as f:\n",
    "        f.write(\"Some content\")\n",
    "        temp_file = f.name\n",
    "    \n",
    "    bad_chunks = process_filing_robust(temp_file)\n",
    "    print(f\"  Result: {len(bad_chunks)} chunks (expected 0)\")\n",
    "    \n",
    "    # Clean up\n",
    "    os.unlink(temp_file)\n",
    "    \n",
    "    # Test 4: Very short text\n",
    "    print(\"\\nTest 4: Very short text\")\n",
    "    short_chunks = create_overlapping_chunks(\"Short text.\", target_tokens=500)\n",
    "    print(f\"  Result: {len(short_chunks)} chunks\")\n",
    "\n",
    "test_error_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Created 109 chunks for AMZN_10Q_2022-04-29.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Created 183 chunks for AMZN_10Q_2020-05-01.txt\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "WARNING:__main__:Strategy 1 failed, trying Strategy 2: Page-based detection\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2020-10-30.txt\n",
      "INFO:__main__:Created 106 chunks for AMZN_10Q_2020-10-30.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Batch Processing (max 3 files)\n",
      "==================================================\n",
      "Processing 3 files...\n",
      "  1/3: AMZN_10Q_2022-04-29.txt\n",
      "  2/3: AMZN_10Q_2020-05-01.txt\n",
      "  3/3: AMZN_10Q_2020-10-30.txt\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "  Total files processed: 3\n",
      "  Total chunks created: 398\n",
      "  Average chunks per file: 132.7\n",
      "\n",
      "üìã Per-file results:\n",
      "  AMZN_10Q_2022-04-29.txt: 109 chunks, 1 sections, 51 tables\n",
      "  AMZN_10Q_2020-05-01.txt: 183 chunks, 1 sections, 131 tables\n",
      "  AMZN_10Q_2020-10-30.txt: 106 chunks, 1 sections, 48 tables\n"
     ]
    }
   ],
   "source": [
    "def test_batch_processing(max_files: int = 5):\n",
    "    \"\"\"Test processing multiple files\"\"\"\n",
    "    print(f\"üîÑ Testing Batch Processing (max {max_files} files)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    data_path = \"processed_filings/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Data path not found: {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Process a subset\n",
    "    test_files = all_files[:max_files]\n",
    "    print(f\"Processing {len(test_files)} files...\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"  {i+1}/{len(test_files)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        file_chunks = process_filing_robust(file_path)\n",
    "        stats = validate_chunks(file_chunks)\n",
    "        \n",
    "        all_results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'chunks': len(file_chunks),\n",
    "            'avg_tokens': stats.get('avg_tokens', 0),\n",
    "            'sections': stats.get('unique_sections', 0),\n",
    "            'tables': stats.get('table_chunks', 0)\n",
    "        })\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    total_chunks = sum(r['chunks'] for r in all_results)\n",
    "    avg_chunks_per_file = total_chunks / len(all_results) if all_results else 0\n",
    "    \n",
    "    print(f\"  Total files processed: {len(all_results)}\")\n",
    "    print(f\"  Total chunks created: {total_chunks}\")\n",
    "    print(f\"  Average chunks per file: {avg_chunks_per_file:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìã Per-file results:\")\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['file']}: {result['chunks']} chunks, {result['sections']} sections, {result['tables']} tables\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run batch test\n",
    "batch_results = test_batch_processing(max_files=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0680f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Final Analysis Summary\n",
      "============================================================\n",
      "üéØ Key Insights:\n",
      "  ‚Ä¢ Document: AAPL 10K (FY2020)\n",
      "  ‚Ä¢ Total chunks: 152\n",
      "  ‚Ä¢ Average chunk size: 365 tokens\n",
      "  ‚Ä¢ Size range: 38 - 1692 tokens\n",
      "  ‚Ä¢ Overlap rate: 55.9%\n",
      "\n",
      "üìä Chunk Distribution by Type:\n",
      "  ‚Ä¢ narrative: 86 chunks (56.6%)\n",
      "  ‚Ä¢ table: 66 chunks (43.4%)\n",
      "\n",
      "üìö Section Breakdown:\n",
      "  ‚Ä¢ Full Document: 152 chunks\n",
      "\n",
      "‚úÖ Quality Metrics:\n",
      "  ‚Ä¢ Very small chunks (<50 tokens): 2 (1.3%)\n",
      "  ‚Ä¢ Large chunks (>800 tokens): 3 (2.0%)\n",
      "  ‚Ä¢ Unique sections identified: 1\n",
      "\n",
      "üîç Sample Chunks for Review:\n",
      "\n",
      "  TABLE example (58 tokens):\n",
      "    Section: Full Document\n",
      "    Preview: California | 94-2404110 | (State or other jurisdiction | of incorporation or organization) | (I.R.S. Employer Identification No.) | One Apple Park Way...\n",
      "\n",
      "  NARRATIVE example (420 tokens):\n",
      "    Section: Full Document\n",
      "    Preview: aapl-20200926-K(Mark One)‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended September 26,...\n"
     ]
    }
   ],
   "source": [
    "def create_analysis_summary():\n",
    "    \"\"\"Create a comprehensive summary of our preprocessing\"\"\"\n",
    "    print(\"üìà Final Analysis Summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"No chunks to analyze - run test_single_file() first\")\n",
    "        return\n",
    "    \n",
    "    # Create a mini dataset for analysis\n",
    "    chunk_data = []\n",
    "    for chunk in chunks:\n",
    "        chunk_data.append({\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'tokens': chunk.token_count,\n",
    "            'type': chunk.chunk_type,\n",
    "            'section': chunk.section_info,\n",
    "            'has_overlap': chunk.has_overlap,\n",
    "            'ticker': chunk.filing_metadata.ticker,\n",
    "            'form_type': chunk.filing_metadata.form_type,\n",
    "            'fiscal_year': chunk.filing_metadata.fiscal_year\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(chunk_data)\n",
    "    \n",
    "    print(\"üéØ Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Document: {df['ticker'].iloc[0]} {df['form_type'].iloc[0]} (FY{df['fiscal_year'].iloc[0]})\")\n",
    "    print(f\"  ‚Ä¢ Total chunks: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Average chunk size: {df['tokens'].mean():.0f} tokens\")\n",
    "    print(f\"  ‚Ä¢ Size range: {df['tokens'].min()} - {df['tokens'].max()} tokens\")\n",
    "    print(f\"  ‚Ä¢ Overlap rate: {(df['has_overlap'].sum() / len(df) * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä Chunk Distribution by Type:\")\n",
    "    type_dist = df['type'].value_counts()\n",
    "    for chunk_type, count in type_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {chunk_type}: {count} chunks ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìö Section Breakdown:\")\n",
    "    section_dist = df['section'].value_counts()\n",
    "    for section, count in section_dist.head(8).items():  # Top 8 sections\n",
    "        print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(f\"\\n‚úÖ Quality Metrics:\")\n",
    "    \n",
    "    # Check for very small chunks (potential issues)\n",
    "    small_chunks = df[df['tokens'] < 50]\n",
    "    print(f\"  ‚Ä¢ Very small chunks (<50 tokens): {len(small_chunks)} ({len(small_chunks)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for very large chunks (might need splitting)\n",
    "    large_chunks = df[df['tokens'] > 800]\n",
    "    print(f\"  ‚Ä¢ Large chunks (>800 tokens): {len(large_chunks)} ({len(large_chunks)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check section coverage\n",
    "    unique_sections = df['section'].nunique()\n",
    "    print(f\"  ‚Ä¢ Unique sections identified: {unique_sections}\")\n",
    "    \n",
    "    # Show some example chunks for manual review\n",
    "    print(f\"\\nüîç Sample Chunks for Review:\")\n",
    "    \n",
    "    # Show one of each type\n",
    "    for chunk_type in df['type'].unique():\n",
    "        sample = df[df['type'] == chunk_type].iloc[0]\n",
    "        chunk_obj = next(c for c in chunks if c.chunk_id == sample['chunk_id'])\n",
    "        print(f\"\\n  {chunk_type.upper()} example ({sample['tokens']} tokens):\")\n",
    "        print(f\"    Section: {sample['section']}\")\n",
    "        print(f\"    Preview: {chunk_obj.text[:150]}...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create final summary\n",
    "summary_df = create_analysis_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "012734b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Comparison: New vs Original Approach\n",
      "============================================================\n",
      "üöÄ Key Improvements:\n",
      "  ‚úÖ Multi-strategy section detection (fallbacks for robustness)\n",
      "  ‚úÖ Sentence-aware chunking (preserves semantic boundaries)\n",
      "  ‚úÖ Overlapping chunks (maintains context across boundaries)\n",
      "  ‚úÖ Separate table processing (handles structured data better)\n",
      "  ‚úÖ Comprehensive error handling (graceful degradation)\n",
      "  ‚úÖ Rich metadata structure (better for search/filtering)\n",
      "  ‚úÖ Quality validation (ensures chunk coherence)\n",
      "  ‚úÖ Configurable parameters (tunable for different use cases)\n",
      "\n",
      "‚öñÔ∏è Potential Tradeoffs:\n",
      "  ‚ö†Ô∏è Slightly more complex code (but more maintainable)\n",
      "  ‚ö†Ô∏è More chunks due to overlap (but better retrieval)\n",
      "  ‚ö†Ô∏è Processing takes longer (but more robust results)\n",
      "\n",
      "üéØ Recommended Next Steps:\n",
      "  1. Test on more diverse filings to validate robustness\n",
      "  2. Fine-tune chunking parameters based on embedding performance\n",
      "  3. Add semantic similarity checks between overlapping chunks\n",
      "  4. Implement incremental processing for large datasets\n",
      "  5. Add support for other SEC forms (8-K, DEF 14A, etc.)\n",
      "  6. Create embedding quality metrics and evaluation\n",
      "\n",
      "============================================================\n",
      "üéâ Preprocessing Strategy Testing Complete!\n",
      "============================================================\n",
      "Next step: Convert this notebook into modular Python files\n",
      "Then: Implement the embedding pipeline and MCP server!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def compare_with_original():\n",
    "    \"\"\"Compare our approach with the original chunking strategy\"\"\"\n",
    "    print(\"‚öñÔ∏è Comparison: New vs Original Approach\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    improvements = [\n",
    "        \"‚úÖ Multi-strategy section detection (fallbacks for robustness)\",\n",
    "        \"‚úÖ Sentence-aware chunking (preserves semantic boundaries)\",\n",
    "        \"‚úÖ Overlapping chunks (maintains context across boundaries)\",\n",
    "        \"‚úÖ Separate table processing (handles structured data better)\",\n",
    "        \"‚úÖ Comprehensive error handling (graceful degradation)\",\n",
    "        \"‚úÖ Rich metadata structure (better for search/filtering)\",\n",
    "        \"‚úÖ Quality validation (ensures chunk coherence)\",\n",
    "        \"‚úÖ Configurable parameters (tunable for different use cases)\"\n",
    "    ]\n",
    "    \n",
    "    potential_tradeoffs = [\n",
    "        \"‚ö†Ô∏è Slightly more complex code (but more maintainable)\",\n",
    "        \"‚ö†Ô∏è More chunks due to overlap (but better retrieval)\",\n",
    "        \"‚ö†Ô∏è Processing takes longer (but more robust results)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Key Improvements:\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  {improvement}\")\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Potential Tradeoffs:\")\n",
    "    for tradeoff in potential_tradeoffs:\n",
    "        print(f\"  {tradeoff}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Recommended Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"1. Test on more diverse filings to validate robustness\",\n",
    "        \"2. Fine-tune chunking parameters based on embedding performance\",\n",
    "        \"3. Add semantic similarity checks between overlapping chunks\",\n",
    "        \"4. Implement incremental processing for large datasets\",\n",
    "        \"5. Add support for other SEC forms (8-K, DEF 14A, etc.)\",\n",
    "        \"6. Create embedding quality metrics and evaluation\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "compare_with_original()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Preprocessing Strategy Testing Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next step: Convert this notebook into modular Python files\")\n",
    "print(\"Then: Implement the embedding pipeline and MCP server!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f61c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Universal SEC section detection ready!\n",
      "This should work for all your SEC filings with table-based formatting\n",
      "Replace process_filing_robust with process_filing_robust_universal\n"
     ]
    }
   ],
   "source": [
    "def detect_sections_universal_sec(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal section detection for SEC filings with table-based formatting\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Universal patterns for table-formatted SEC filings\n",
    "    patterns = [\n",
    "        # Table-based ITEM patterns (most common in your files)\n",
    "        r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Table-based PART patterns\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^\\[]+?)\\s*\\[TABLE_END\\]',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        r'(?i)\\[TABLE_START\\]\\s*PART\\s+([IVX]+)\\s*\\[TABLE_END\\]',\n",
    "        \n",
    "        # Standalone ITEM patterns (fallback)\n",
    "        r'(?i)^\\s*Item\\s+(\\d{1,2}[A-C]?)\\.\\s*([^\\n]+)',\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Standalone PART patterns (fallback)\n",
    "        r'(?i)^\\s*PART\\s+([IVX]+)\\s*([^\\n]*)',\n",
    "        r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Number-only patterns in tables\n",
    "        r'(?i)\\[TABLE_START\\]\\s*(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+)',\n",
    "        \n",
    "        # Headers that might appear standalone\n",
    "        r'(?i)^(Item\\s+\\d{1,2}[A-C]?\\.\\s+[^|]+?)$',\n",
    "        r'(?i)^(PART\\s+[IVX]+)(?:\\s*[-‚Äì‚Äî]\\s*(.+))?$',\n",
    "    ]\n",
    "    \n",
    "    all_matches = []\n",
    "    \n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for match in re.finditer(pattern, content, re.MULTILINE):\n",
    "            # Get context around the match\n",
    "            line_start = content.rfind('\\n', 0, match.start()) + 1\n",
    "            line_end = content.find('\\n', match.end())\n",
    "            if line_end == -1:\n",
    "                line_end = len(content)\n",
    "            \n",
    "            full_line = content[line_start:line_end].strip()\n",
    "            \n",
    "            # Skip if this looks like metadata or page headers\n",
    "            if any(skip_word in full_line.lower() for skip_word in \n",
    "                   ['page', 'signature', 'exhibit', 'index', 'table of contents']):\n",
    "                continue\n",
    "                \n",
    "            # Skip very long lines that are probably not headers\n",
    "            if len(full_line) > 500:\n",
    "                continue\n",
    "            \n",
    "            # Extract section information\n",
    "            groups = match.groups()\n",
    "            \n",
    "            if len(groups) >= 2 and groups[1]:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = groups[1].strip()\n",
    "                # Clean up section title\n",
    "                section_title = re.sub(r'\\[TABLE_END\\].*', '', section_title).strip()\n",
    "                section_title = section_title.replace('|', '').strip()\n",
    "            elif len(groups) >= 1:\n",
    "                section_id = groups[0].strip()\n",
    "                section_title = f\"Section {section_id}\"\n",
    "            else:\n",
    "                section_id = 'unknown'\n",
    "                section_title = full_line\n",
    "            \n",
    "            all_matches.append({\n",
    "                'start_pos': line_start,\n",
    "                'end_pos': line_end,\n",
    "                'full_line': full_line,\n",
    "                'section_id': section_id,\n",
    "                'section_title': section_title,\n",
    "                'pattern_idx': pattern_idx,\n",
    "                'match_start': match.start()\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates - matches within 100 characters\n",
    "    unique_matches = []\n",
    "    for match in sorted(all_matches, key=lambda x: x['start_pos']):\n",
    "        is_duplicate = any(\n",
    "            abs(match['start_pos'] - existing['start_pos']) < 100 \n",
    "            for existing in unique_matches\n",
    "        )\n",
    "        if not is_duplicate:\n",
    "            unique_matches.append(match)\n",
    "    \n",
    "    # Remove very similar section IDs (e.g., multiple \"1\" entries)\n",
    "    final_matches = []\n",
    "    seen_section_ids = set()\n",
    "    for match in unique_matches:\n",
    "        section_key = f\"{match['section_id'].upper()}_{match['section_title'][:20]}\"\n",
    "        if section_key not in seen_section_ids:\n",
    "            final_matches.append(match)\n",
    "            seen_section_ids.add(section_key)\n",
    "    \n",
    "    print(f\"üîç Universal SEC detection found {len(final_matches)} unique sections:\")\n",
    "    for i, match in enumerate(final_matches[:15]):\n",
    "        print(f\"  {i+1}: Item/Part {match['section_id']} - {match['section_title'][:60]}...\")\n",
    "    \n",
    "    # Convert to DocumentSection objects\n",
    "    for i, match in enumerate(final_matches):\n",
    "        start_pos = match['start_pos']\n",
    "        end_pos = final_matches[i + 1]['start_pos'] if i + 1 < len(final_matches) else len(content)\n",
    "        \n",
    "        section_content = content[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Determine section type and metadata\n",
    "        section_id = match['section_id'].upper()\n",
    "        \n",
    "        if re.match(r'^[IVX]+$', section_id):\n",
    "            # This is a PART\n",
    "            section_type = 'part'\n",
    "            part = f\"PART {section_id}\"\n",
    "            item_number = None\n",
    "            title = f\"Part {section_id}\"\n",
    "            if match['section_title'] and match['section_title'] != f\"Section {section_id}\":\n",
    "                title = f\"Part {section_id} - {match['section_title']}\"\n",
    "        elif re.match(r'^\\d+[A-C]?$', section_id):\n",
    "            # This is an ITEM\n",
    "            section_type = 'item'\n",
    "            part = None\n",
    "            item_number = section_id\n",
    "            title = f\"Item {section_id}\"\n",
    "            if match['section_title'] and match['section_title'] != f\"Section {section_id}\":\n",
    "                title = f\"Item {section_id} - {match['section_title']}\"\n",
    "        else:\n",
    "            # Unknown section type\n",
    "            section_type = 'content'\n",
    "            part = None\n",
    "            item_number = None\n",
    "            title = match['section_title'] if match['section_title'] else match['full_line']\n",
    "        \n",
    "        sections.append(DocumentSection(\n",
    "            title=title,\n",
    "            content=section_content,\n",
    "            section_type=section_type,\n",
    "            item_number=item_number,\n",
    "            part=part,\n",
    "            start_pos=start_pos,\n",
    "            end_pos=end_pos\n",
    "        ))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def detect_sections_from_toc_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Extract sections from table of contents - works for any SEC filing\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Look for table of contents patterns\n",
    "    toc_patterns = [\n",
    "        r'(?i)INDEX.*?(?=\\[PAGE BREAK\\])',\n",
    "        r'(?i)TABLE OF CONTENTS.*?(?=\\[PAGE BREAK\\])',\n",
    "        r'(?i)FORM 10-[KQ].*?INDEX.*?(?=\\[PAGE BREAK\\])',\n",
    "        r'(?i)\\[TABLE_START\\].*?Page.*?\\[TABLE_END\\].*?(?=\\[PAGE BREAK\\])',\n",
    "    ]\n",
    "    \n",
    "    toc_content = \"\"\n",
    "    for pattern in toc_patterns:\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        if match:\n",
    "            toc_content = match.group(0)\n",
    "            break\n",
    "    \n",
    "    if not toc_content:\n",
    "        print(\"No table of contents found\")\n",
    "        return sections\n",
    "    \n",
    "    print(f\"Found table of contents ({len(toc_content)} chars)\")\n",
    "    \n",
    "    # Extract sections from TOC using multiple patterns\n",
    "    item_patterns = [\n",
    "        # Standard table format: Item 1. | Business | Page\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+',\n",
    "        r'(?i)PART\\s+([IVX]+)\\s*\\|\\s*([^|]+)',\n",
    "        # Alternative formats\n",
    "        r'(?i)Item\\s+(\\d{1,2}[A-C]?)\\.\\s+([^|\\d]+)',\n",
    "        r'(?i)(\\d{1,2}[A-C]?)\\.\\s*\\|\\s*([^|]+?)\\s*\\|\\s*\\d+',\n",
    "    ]\n",
    "    \n",
    "    found_items = []\n",
    "    for pattern in item_patterns:\n",
    "        for match in re.finditer(pattern, toc_content):\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2:\n",
    "                item_id = groups[0].strip()\n",
    "                item_title = groups[1].strip()\n",
    "                # Clean up the title\n",
    "                item_title = re.sub(r'\\s+', ' ', item_title)\n",
    "                found_items.append((item_id, item_title))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item_id, title in found_items:\n",
    "        key = f\"{item_id}_{title[:20]}\"\n",
    "        if key not in seen:\n",
    "            unique_items.append((item_id, title))\n",
    "            seen.add(key)\n",
    "    \n",
    "    print(f\"Extracted {len(unique_items)} sections from table of contents:\")\n",
    "    for item_id, title in unique_items[:10]:\n",
    "        print(f\"  ‚Ä¢ {item_id}: {title[:50]}...\")\n",
    "    \n",
    "    return sections  # For now, just return the found items info\n",
    "\n",
    "def detect_sections_robust_universal(content: str) -> List[DocumentSection]:\n",
    "    \"\"\"\n",
    "    Universal robust section detection for all SEC filings\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting universal SEC section detection\")\n",
    "    \n",
    "    # Strategy 1: Direct pattern matching for table-formatted sections\n",
    "    sections = detect_sections_universal_sec(content)\n",
    "    \n",
    "    if len(sections) >= 3:\n",
    "        logger.info(f\"Universal detection successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    # Strategy 2: Table of contents analysis\n",
    "    logger.warning(\"Direct detection found few sections, analyzing table of contents\")\n",
    "    detect_sections_from_toc_universal(content)  # For debugging info\n",
    "    \n",
    "    # Strategy 3: Page-based fallback\n",
    "    logger.warning(\"Trying page-based detection as fallback\")\n",
    "    sections = detect_sections_strategy_2(content)\n",
    "    \n",
    "    if len(sections) >= 2:\n",
    "        logger.info(f\"Page-based detection successful: Found {len(sections)} sections\")\n",
    "        return sections\n",
    "    \n",
    "    # Final fallback\n",
    "    logger.warning(\"All strategies failed, creating single section\")\n",
    "    return [DocumentSection(\n",
    "        title=\"Full Document\",\n",
    "        content=content,\n",
    "        section_type='document',\n",
    "        start_pos=0,\n",
    "        end_pos=len(content)\n",
    "    )]\n",
    "\n",
    "# Universal processing function\n",
    "def process_filing_robust_universal(file_path: str, target_tokens: int = 500, overlap_tokens: int = 100) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Universal processing function for all SEC filings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract filing metadata\n",
    "        filename = Path(file_path).name\n",
    "        file_id = filename.replace(\".txt\", \"\")\n",
    "        parts = file_id.split('_')\n",
    "        \n",
    "        if len(parts) != 3:\n",
    "            logger.error(f\"Invalid filename format: {filename}\")\n",
    "            return []\n",
    "        \n",
    "        ticker, form_type, filing_date_str = parts\n",
    "        \n",
    "        # Create filing metadata\n",
    "        filing_date = pd.to_datetime(filing_date_str)\n",
    "        fiscal_year = filing_date.year\n",
    "        fiscal_quarter = filing_date.quarter\n",
    "        \n",
    "        if form_type == '10K' and filing_date.month < 4:\n",
    "            fiscal_year -= 1\n",
    "        \n",
    "        filing_metadata = FilingMetadata(\n",
    "            ticker=ticker,\n",
    "            form_type=form_type,\n",
    "            filing_date=filing_date_str,\n",
    "            fiscal_year=fiscal_year,\n",
    "            fiscal_quarter=fiscal_quarter,\n",
    "            file_path=file_path\n",
    "        )\n",
    "        \n",
    "        # Read and clean content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        \n",
    "        cleaned_content = clean_sec_text(raw_content)\n",
    "        \n",
    "        # Use universal section detection\n",
    "        sections = detect_sections_robust_universal(cleaned_content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {filename}\")\n",
    "        \n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        chunk_counter = 0\n",
    "        \n",
    "        for section in sections:\n",
    "            # Extract tables from this section\n",
    "            tables, narrative_content = extract_and_process_tables(section.content)\n",
    "            \n",
    "            # Create section info string\n",
    "            section_info = create_section_info_improved(section, form_type)\n",
    "            \n",
    "            # Process tables\n",
    "            for table in tables:\n",
    "                chunk = Chunk(\n",
    "                    chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                    text=table['text'],\n",
    "                    token_count=table['token_count'],\n",
    "                    chunk_type='table',\n",
    "                    section_info=section_info,\n",
    "                    filing_metadata=filing_metadata,\n",
    "                    chunk_index=chunk_counter,\n",
    "                    has_overlap=False\n",
    "                )\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_counter += 1\n",
    "            \n",
    "            # Process narrative content\n",
    "            if narrative_content.strip():\n",
    "                narrative_chunks = create_overlapping_chunks(\n",
    "                    narrative_content, target_tokens, overlap_tokens\n",
    "                )\n",
    "                \n",
    "                for chunk_data in narrative_chunks:\n",
    "                    chunk = Chunk(\n",
    "                        chunk_id=f\"{file_id}-chunk-{chunk_counter:04d}\",\n",
    "                        text=chunk_data['text'],\n",
    "                        token_count=chunk_data['token_count'],\n",
    "                        chunk_type='narrative',\n",
    "                        section_info=section_info,\n",
    "                        filing_metadata=filing_metadata,\n",
    "                        chunk_index=chunk_counter,\n",
    "                        has_overlap=chunk_data['has_overlap']\n",
    "                    )\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_counter += 1\n",
    "        \n",
    "        logger.info(f\"Created {len(all_chunks)} chunks for {filename}\")\n",
    "        return all_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"üöÄ Universal SEC section detection ready!\")\n",
    "print(\"This should work for all your SEC filings with table-based formatting\")\n",
    "print(\"Replace process_filing_robust with process_filing_robust_universal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e2f1a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ready to test universal SEC detection!\n",
      "\n",
      "1. Run test_universal_detection() to test all files\n",
      "2. Run compare_old_vs_universal() to see the improvement\n",
      "3. Run quick_pattern_test() to see what patterns match\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST UNIVERSAL SEC DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def test_universal_detection():\n",
    "    \"\"\"Test the universal detection on all your file types\"\"\"\n",
    "    \n",
    "    # Test different files to verify universal approach\n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\", \n",
    "        \"processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\",\n",
    "        \"processed_filings/KO/KO_10Q_2020-07-22.txt\"  # If you have this one\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file} - file not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüß™ Testing: {test_file}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Test universal detection\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(sections)} sections:\")\n",
    "        for i, section in enumerate(sections[:10]):  # Show first 10\n",
    "            print(f\"  {i+1}. {section.title}\")\n",
    "            print(f\"     Type: {section.section_type}, Length: {len(section.content):,} chars\")\n",
    "        \n",
    "        # Test full pipeline\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        stats = validate_chunks(chunks) if chunks else {\"error\": \"No chunks created\"}\n",
    "        \n",
    "        results[test_file] = {\n",
    "            'sections': len(sections),\n",
    "            'chunks': len(chunks) if chunks else 0,\n",
    "            'stats': stats\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä Processing Results:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        if chunks:\n",
    "            # Show section distribution\n",
    "            section_counts = {}\n",
    "            for chunk in chunks[:20]:  # Sample first 20\n",
    "                section = chunk.section_info\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "            \n",
    "            print(f\"\\nüìö Section Distribution (sample):\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä UNIVERSAL DETECTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for file_path, result in results.items():\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"{filename:<25} | {result['sections']:>2} sections | {result['chunks']:>3} chunks\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_old_vs_universal():\n",
    "    \"\"\"Compare the old detection vs universal detection\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found for comparison\")\n",
    "        return\n",
    "    \n",
    "    print(\"‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Old detection\n",
    "    print(\"Running old detection...\")\n",
    "    old_sections = detect_sections_robust(content)\n",
    "    \n",
    "    # New universal detection  \n",
    "    print(\"Running universal detection...\")\n",
    "    new_sections = detect_sections_robust_universal(content)\n",
    "    \n",
    "    print(f\"\\nüìä Comparison Results:\")\n",
    "    print(f\"  Old detection: {len(old_sections)} sections\")\n",
    "    print(f\"  Universal detection: {len(new_sections)} sections\")\n",
    "    print(f\"  Improvement: +{len(new_sections) - len(old_sections)} sections\")\n",
    "    \n",
    "    print(f\"\\nüìã Old Sections:\")\n",
    "    for i, section in enumerate(old_sections):\n",
    "        print(f\"  {i+1}. {section.title}\")\n",
    "    \n",
    "    print(f\"\\nüìã Universal Sections:\")\n",
    "    for i, section in enumerate(new_sections):\n",
    "        print(f\"  {i+1}. {section.title}\")\n",
    "    \n",
    "    return old_sections, new_sections\n",
    "\n",
    "def quick_pattern_test():\n",
    "    \"\"\"Quick test to see what patterns match in your content\"\"\"\n",
    "    test_file = \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\"\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Test file not found\")\n",
    "        return\n",
    "    \n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(\"üîç QUICK PATTERN TEST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test key patterns\n",
    "    patterns = [\n",
    "        (r'\\[TABLE_START\\].*?Item.*?\\[TABLE_END\\]', \"Table-wrapped Items\"),\n",
    "        (r'Item\\s+\\d+[A-C]?\\.\\s*\\|', \"Pipe-separated Items\"),\n",
    "        (r'PART\\s+[IVX]+', \"Part headers\"),\n",
    "        (r'\\[TABLE_START\\].*?PART.*?\\[TABLE_END\\]', \"Table-wrapped Parts\"),\n",
    "    ]\n",
    "    \n",
    "    for pattern, description in patterns:\n",
    "        matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)\n",
    "        print(f\"\\n{description}: {len(matches)} matches\")\n",
    "        for i, match in enumerate(matches[:3]):\n",
    "            # Clean up match for display\n",
    "            clean_match = ' '.join(match.split())[:100]\n",
    "            print(f\"  {i+1}: {clean_match}...\")\n",
    "\n",
    "print(\"üöÄ Ready to test universal SEC detection!\")\n",
    "print(\"\\n1. Run test_universal_detection() to test all files\")\n",
    "print(\"2. Run compare_old_vs_universal() to see the improvement\") \n",
    "print(\"3. Run quick_pattern_test() to see what patterns match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ae8d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING FUNCTION FIX - Add this to your notebook\n",
    "# =============================================================================\n",
    "\n",
    "def create_section_info_improved(section_title: str, section_type: str = \"unknown\") -> str:\n",
    "    \"\"\"\n",
    "    Create standardized section info for chunks\n",
    "    \"\"\"\n",
    "    if not section_title or section_title.strip() == \"\":\n",
    "        return \"Full Document\"\n",
    "    \n",
    "    # Clean up the section title\n",
    "    clean_title = section_title.strip()\n",
    "    \n",
    "    # Remove redundant prefixes\n",
    "    clean_title = re.sub(r'^(Item/Part\\s+)', '', clean_title)\n",
    "    clean_title = re.sub(r'^(Part\\s+[IVX]+\\s+-\\s+)', '', clean_title)\n",
    "    \n",
    "    # Standardize format\n",
    "    if clean_title.startswith(\"Item \"):\n",
    "        return clean_title\n",
    "    elif clean_title.startswith(\"Part \"):\n",
    "        return clean_title\n",
    "    elif section_type == \"item\":\n",
    "        return f\"Item {clean_title}\"\n",
    "    elif section_type == \"part\":\n",
    "        return f\"Part {clean_title}\"\n",
    "    else:\n",
    "        return clean_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cdbd803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 19 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing: processed_filings/AAPL/AAPL_10K_2020-10-30.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 19 unique sections:\n",
      "  1: Item/Part I - Item 1.    Business...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 3 - Legal Proceedings...\n",
      "  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "  7: Item/Part 6 - Selected Financial Data...\n",
      "  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part 11 - Executive Compensation...\n",
      "  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "\n",
      "‚úÖ Found 19 sections:\n",
      "  1. Part I - Item 1.    Business\n",
      "     Type: part, Length: 13,274 chars\n",
      "  2. Item 1A - Risk Factors\n",
      "     Type: item, Length: 61,136 chars\n",
      "  3. Item 1B - Unresolved Staff Comments\n",
      "     Type: item, Length: 582 chars\n",
      "  4. Item 3 - Legal Proceedings\n",
      "     Type: item, Length: 898 chars\n",
      "  5. Item 4 - Mine Safety Disclosures\n",
      "     Type: item, Length: 99 chars\n",
      "  6. Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "     Type: part, Length: 4,191 chars\n",
      "  7. Item 6 - Selected Financial Data\n",
      "     Type: item, Length: 1,745 chars\n",
      "  8. Item 7 - Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 33,154 chars\n",
      "  9. Item 7A - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 6,799 chars\n",
      "  10. Item 8 - Financial Statements and Supplementary Data\n",
      "     Type: item, Length: 103,042 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AAPL_10K_2020-10-30.txt\n",
      "ERROR:__main__:Error processing processed_filings/AAPL/AAPL_10K_2020-10-30.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 20 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10K_2023-02-03.txt\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10K_2023-02-03.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 9 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10K_2023-02-03.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 20 unique sections:\n",
      "  1: Item/Part I - [TABLE_START]...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 2 - Properties...\n",
      "  5: Item/Part 3 - Legal Proceedings...\n",
      "  6: Item/Part 4 - Mine Safety Disclosures...\n",
      "  7: Item/Part II - [TABLE_START]...\n",
      "  8: Item/Part 6 - Reserved...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants On Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part III - [TABLE_START]...\n",
      "  15: Item/Part 11 - Executive Compensation...\n",
      "\n",
      "‚úÖ Found 20 sections:\n",
      "  1. Part I - [TABLE_START]\n",
      "     Type: part, Length: 13,293 chars\n",
      "  2. Item 1A - Risk Factors\n",
      "     Type: item, Length: 55,960 chars\n",
      "  3. Item 1B - Unresolved Staff Comments\n",
      "     Type: item, Length: 106 chars\n",
      "  4. Item 2 - Properties\n",
      "     Type: item, Length: 1,437 chars\n",
      "  5. Item 3 - Legal Proceedings\n",
      "     Type: item, Length: 185 chars\n",
      "  6. Item 4 - Mine Safety Disclosures\n",
      "     Type: item, Length: 113 chars\n",
      "  7. Part II - [TABLE_START]\n",
      "     Type: part, Length: 516 chars\n",
      "  8. Item 6 - Reserved\n",
      "     Type: item, Length: 50,497 chars\n",
      "  9. Item 7A - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 6,524 chars\n",
      "  10. Item 8 - Financial Statements and Supplementary Data\n",
      "     Type: item, Length: 123,990 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/AMZN/AMZN_10Q_2024-11-01.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 9 unique sections:\n",
      "  1: Item/Part I - . FINANCIAL INFORMATION...\n",
      "  2: Item/Part 2 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  4: Item/Part 4 - Controls and Procedures...\n",
      "  5: Item/Part II - . OTHER INFORMATION...\n",
      "  6: Item/Part 1A - Risk Factors...\n",
      "  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "  8: Item/Part 3 - Defaults Upon Senior Securities...\n",
      "  9: Item/Part 5 - Other Information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in AMZN_10Q_2024-11-01.txt\n",
      "ERROR:__main__:Error processing processed_filings/AMZN/AMZN_10Q_2024-11-01.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 7 sections\n",
      "INFO:__main__:Attempting universal SEC section detection\n",
      "WARNING:__main__:Direct detection found few sections, analyzing table of contents\n",
      "WARNING:__main__:Trying page-based detection as fallback\n",
      "WARNING:__main__:All strategies failed, creating single section\n",
      "INFO:__main__:Found 1 sections in KO_10Q_2020-07-22.txt\n",
      "ERROR:__main__:Error processing processed_filings/KO/KO_10Q_2020-07-22.txt: 'DocumentSection' object has no attribute 'strip'\n",
      "INFO:__main__:Attempting Strategy 1: Regex-based section detection\n",
      "INFO:__main__:Strategy 1 successful: Found 25 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 9 sections:\n",
      "  1. Part I - . FINANCIAL INFORMATION\n",
      "     Type: part, Length: 67,088 chars\n",
      "  2. Item 2 - Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 45,106 chars\n",
      "  3. Item 3 - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 4,404 chars\n",
      "  4. Item 4 - Controls and Procedures\n",
      "     Type: item, Length: 2,075 chars\n",
      "  5. Part II - . OTHER INFORMATION\n",
      "     Type: part, Length: 189 chars\n",
      "  6. Item 1A - Risk Factors\n",
      "     Type: item, Length: 59,432 chars\n",
      "  7. Item 2 - Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "     Type: item, Length: 102 chars\n",
      "  8. Item 3 - Defaults Upon Senior Securities\n",
      "     Type: item, Length: 152 chars\n",
      "  9. Item 5 - Other Information\n",
      "     Type: item, Length: 5,327 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "üß™ Testing: processed_filings/KO/KO_10Q_2020-07-22.txt\n",
      "================================================================================\n",
      "üîç Universal SEC detection found 7 unique sections:\n",
      "  1: Item/Part I - . Financial Information...\n",
      "  2: Item/Part 2 - Management's Discussion and Analysis of Financial Condition ...\n",
      "  3: Item/Part 3 - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  4: Item/Part 4 - Controls and Procedures...\n",
      "  5: Item/Part II - . Other Information...\n",
      "  6: Item/Part 1A - Risk Factors...\n",
      "  7: Item/Part 2 - Unregistered Sales of Equity Securities and Use of Proceeds...\n",
      "\n",
      "‚úÖ Found 7 sections:\n",
      "  1. Part I - . Financial Information\n",
      "     Type: part, Length: 115,924 chars\n",
      "  2. Item 2 - Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "     Type: item, Length: 87,923 chars\n",
      "  3. Item 3 - Quantitative and Qualitative Disclosures About Market Risk\n",
      "     Type: item, Length: 207 chars\n",
      "  4. Item 4 - Controls and Procedures\n",
      "     Type: item, Length: 1,004 chars\n",
      "  5. Part II - . Other Information\n",
      "     Type: part, Length: 248 chars\n",
      "  6. Item 1A - Risk Factors\n",
      "     Type: item, Length: 11,661 chars\n",
      "  7. Item 2 - Unregistered Sales of Equity Securities and Use of Proceeds\n",
      "     Type: item, Length: 16,047 chars\n",
      "üîç Universal SEC detection found 0 unique sections:\n",
      "No table of contents found\n",
      "\n",
      "üìä Processing Results:\n",
      "  error: No chunks created\n",
      "\n",
      "================================================================================\n",
      "üìä UNIVERSAL DETECTION SUMMARY\n",
      "================================================================================\n",
      "AAPL_10K_2020-10-30.txt   | 19 sections |   0 chunks\n",
      "AMZN_10K_2023-02-03.txt   | 20 sections |   0 chunks\n",
      "AMZN_10Q_2024-11-01.txt   |  9 sections |   0 chunks\n",
      "KO_10Q_2020-07-22.txt     |  7 sections |   0 chunks\n",
      "‚öñÔ∏è OLD vs UNIVERSAL Detection Comparison\n",
      "============================================================\n",
      "Running old detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting universal SEC section detection\n",
      "INFO:__main__:Universal detection successful: Found 19 sections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running universal detection...\n",
      "üîç Universal SEC detection found 19 unique sections:\n",
      "  1: Item/Part I - Item 1.    Business...\n",
      "  2: Item/Part 1A - Risk Factors...\n",
      "  3: Item/Part 1B - Unresolved Staff Comments...\n",
      "  4: Item/Part 3 - Legal Proceedings...\n",
      "  5: Item/Part 4 - Mine Safety Disclosures...\n",
      "  6: Item/Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related St...\n",
      "  7: Item/Part 6 - Selected Financial Data...\n",
      "  8: Item/Part 7 - Management‚Äôs Discussion and Analysis of Financial Condition ...\n",
      "  9: Item/Part 7A - Quantitative and Qualitative Disclosures About Market Risk...\n",
      "  10: Item/Part 8 - Financial Statements and Supplementary Data...\n",
      "  11: Item/Part 9 - Changes in and Disagreements with Accountants on Accounting ...\n",
      "  12: Item/Part 9A - Controls and Procedures...\n",
      "  13: Item/Part 9B - Other Information...\n",
      "  14: Item/Part 11 - Executive Compensation...\n",
      "  15: Item/Part 12 - Security Ownership of Certain Beneficial Owners and Manageme...\n",
      "\n",
      "üìä Comparison Results:\n",
      "  Old detection: 25 sections\n",
      "  Universal detection: 19 sections\n",
      "  Improvement: +-6 sections\n",
      "\n",
      "üìã Old Sections:\n",
      "  1. PART I\n",
      "  2. Item 1.    Business\n",
      "  3. Item 1A.    Risk Factors\n",
      "  4. Item 1B.    Unresolved Staff Comments\n",
      "  5. Item 2.    Properties\n",
      "  6. Item 3.    Legal Proceedings\n",
      "  7. Item 4.    Mine Safety Disclosures\n",
      "  8. PART II\n",
      "  9. Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "  10. Item 6.    Selected Financial Data\n",
      "  11. Item 7.    Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "  12. Item 7A.    Quantitative and Qualitative Disclosures About Market Risk\n",
      "  13. Item 8.    Financial Statements and Supplementary Data\n",
      "  14. Item 9.    Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "  15. Item 9A.    Controls and Procedures\n",
      "  16. Item 9B.    Other Information\n",
      "  17. PART III\n",
      "  18. Item 10.    Directors, Executive Officers and Corporate Governance\n",
      "  19. Item 11.    Executive Compensation\n",
      "  20. Item 12.    Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "  21. Item 13.    Certain Relationships and Related Transactions, and Director Independence\n",
      "  22. Item 14.    Principal Accountant Fees and Services\n",
      "  23. PART IV\n",
      "  24. Item 15.    Exhibit and Financial Statement Schedules\n",
      "  25. Item 16.    Form 10-K Summary\n",
      "\n",
      "üìã Universal Sections:\n",
      "  1. Part I - Item 1.    Business\n",
      "  2. Item 1A - Risk Factors\n",
      "  3. Item 1B - Unresolved Staff Comments\n",
      "  4. Item 3 - Legal Proceedings\n",
      "  5. Item 4 - Mine Safety Disclosures\n",
      "  6. Part II - Item 5.    Market for Registrant‚Äôs Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "  7. Item 6 - Selected Financial Data\n",
      "  8. Item 7 - Management‚Äôs Discussion and Analysis of Financial Condition and Results of Operations\n",
      "  9. Item 7A - Quantitative and Qualitative Disclosures About Market Risk\n",
      "  10. Item 8 - Financial Statements and Supplementary Data\n",
      "  11. Item 9 - Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "  12. Item 9A - Controls and Procedures\n",
      "  13. Item 9B - Other Information\n",
      "  14. Item 11 - Executive Compensation\n",
      "  15. Item 12 - Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "  16. Item 13 - Certain Relationships and Related Transactions, and Director Independence\n",
      "  17. Item 14 - Principal Accountant Fees and Services\n",
      "  18. Section PART IV\n",
      "  19. Item 16 - Form 10-K Summary\n",
      "üîç QUICK PATTERN TEST\n",
      "==================================================\n",
      "\n",
      "Table-wrapped Items: 12 matches\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "  3: [TABLE_START] 2020 | Change | 2019 | Change | 2018 | Net sales by category: | iPhone | (1) | $ | 137...\n",
      "\n",
      "Pipe-separated Items: 19 matches\n",
      "  1: Item 1. |...\n",
      "  2: Item 1A. |...\n",
      "  3: Item 1B. |...\n",
      "\n",
      "Part headers: 33 matches\n",
      "  1: Part III...\n",
      "  2: Part I...\n",
      "  3: Part II...\n",
      "\n",
      "Table-wrapped Parts: 15 matches\n",
      "  1: [TABLE_START] California | 94-2404110 | (State or other jurisdiction | of incorporation or organizat...\n",
      "  2: [TABLE_START] Periods | Total Number | of Shares Purchased | Average Price | Paid Per Share | Total ...\n",
      "  3: [TABLE_START] September 2015 | September 2016 | September 2017 | September 2018 | September 2019 | S...\n"
     ]
    }
   ],
   "source": [
    "# Test the universal detection\n",
    "results = test_universal_detection()\n",
    "\n",
    "# Compare old vs new\n",
    "old_sections, new_sections = compare_old_vs_universal()\n",
    "\n",
    "# See what patterns actually match\n",
    "quick_pattern_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7738a7e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChunkWithMetadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     29\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m clean_title\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_filing_robust_universal\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[43mChunkWithMetadata\u001b[49m]:\n\u001b[32m     32\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    Updated processing function with the missing function included\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'ChunkWithMetadata' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MISSING FUNCTION FIX - Add this to your notebook\n",
    "# =============================================================================\n",
    "\n",
    "def create_section_info_improved(section_title: str, section_type: str = \"unknown\") -> str:\n",
    "    \"\"\"\n",
    "    Create standardized section info for chunks\n",
    "    \"\"\"\n",
    "    if not section_title or section_title.strip() == \"\":\n",
    "        return \"Full Document\"\n",
    "    \n",
    "    # Clean up the section title\n",
    "    clean_title = section_title.strip()\n",
    "    \n",
    "    # Remove redundant prefixes\n",
    "    clean_title = re.sub(r'^(Item/Part\\s+)', '', clean_title)\n",
    "    clean_title = re.sub(r'^(Part\\s+[IVX]+\\s+-\\s+)', '', clean_title)\n",
    "    \n",
    "    # Standardize format\n",
    "    if clean_title.startswith(\"Item \"):\n",
    "        return clean_title\n",
    "    elif clean_title.startswith(\"Part \"):\n",
    "        return clean_title\n",
    "    elif section_type == \"item\":\n",
    "        return f\"Item {clean_title}\"\n",
    "    elif section_type == \"part\":\n",
    "        return f\"Part {clean_title}\"\n",
    "    else:\n",
    "        return clean_title\n",
    "\n",
    "def process_filing_robust_universal(file_path: str) -> List[ChunkWithMetadata]:\n",
    "    \"\"\"\n",
    "    Updated processing function with the missing function included\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing {file_path}\")\n",
    "        \n",
    "        # Read file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_metadata_from_filename(file_path)\n",
    "        \n",
    "        # Detect sections using universal approach\n",
    "        sections = detect_sections_robust_universal(content)\n",
    "        logger.info(f\"Found {len(sections)} sections in {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Process each section\n",
    "        all_chunks = []\n",
    "        \n",
    "        for section in sections:\n",
    "            # Create chunks for this section\n",
    "            section_chunks = create_chunks_from_section(\n",
    "                section.content, \n",
    "                chunk_size=512, \n",
    "                overlap=0.2\n",
    "            )\n",
    "            \n",
    "            # Add metadata to each chunk\n",
    "            for chunk in section_chunks:\n",
    "                chunk_with_meta = ChunkWithMetadata(\n",
    "                    content=chunk.content,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        'section_info': create_section_info_improved(section.title, section.section_type),\n",
    "                        'section_type': section.section_type,\n",
    "                        'start_pos': chunk.start_pos,\n",
    "                        'end_pos': chunk.end_pos,\n",
    "                        'chunk_index': len(all_chunks),\n",
    "                        'token_count': chunk.token_count,\n",
    "                        'content_type': chunk.content_type\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(chunk_with_meta)\n",
    "        \n",
    "        return all_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Test the fix\n",
    "def test_fixed_processing():\n",
    "    \"\"\"Test that the missing function fix works\"\"\"\n",
    "    \n",
    "    test_files = [\n",
    "        \"processed_filings/AAPL/AAPL_10K_2020-10-30.txt\",\n",
    "        \"processed_filings/AMZN/AMZN_10K_2023-02-03.txt\"\n",
    "    ]\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüß™ Testing FIXED processing: {test_file}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Process with fixed function\n",
    "        chunks = process_filing_robust_universal(test_file)\n",
    "        \n",
    "        if chunks:\n",
    "            print(f\"‚úÖ SUCCESS: Created {len(chunks)} chunks!\")\n",
    "            \n",
    "            # Show chunk distribution by section\n",
    "            section_counts = {}\n",
    "            for chunk in chunks:\n",
    "                section = chunk.metadata.get('section_info', 'Unknown')\n",
    "                section_counts[section] = section_counts.get(section, 0) + 1\n",
    "            \n",
    "            print(f\"\\nüìö Chunk Distribution by Section:\")\n",
    "            for section, count in sorted(section_counts.items()):\n",
    "                print(f\"  ‚Ä¢ {section}: {count} chunks\")\n",
    "            \n",
    "            # Show sample chunks\n",
    "            print(f\"\\nüìÑ Sample Chunks:\")\n",
    "            for i, chunk in enumerate(chunks[:3]):\n",
    "                section = chunk.metadata.get('section_info', 'Unknown')\n",
    "                content_preview = ' '.join(chunk.content.split()[:15])\n",
    "                print(f\"  {i+1}. [{section}] {content_preview}...\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå FAILED: No chunks created\")\n",
    "    \n",
    "    return chunks if 'chunks' in locals() else []\n",
    "\n",
    "print(\"üöÄ Missing function fix added!\")\n",
    "print(\"\\nRun test_fixed_processing() to verify the fix works!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take-home-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
